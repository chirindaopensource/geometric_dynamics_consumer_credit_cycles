{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH_9y0DFuCco"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `README.md`\n",
        "\n",
        "# Geometric Dynamics of Consumer Credit Cycles: A Complete Implementation\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2510.15892-b31b1b.svg)](https://arxiv.org/abs/2510.15892)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/geometric_dynamics_consumer_credit_cycles)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Econometrics%20%7C%20ML%20%7C%20Finance-00529B)](https://github.com/chirindaopensource/geometric_dynamics_consumer_credit_cycles)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-FRED-lightgrey)](https://fred.stlouisfed.org/)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Geometric%20Algebra%20%7C%20Linear%20Attention-orange)](https://github.com/chirindaopensource/geometric_dynamics_consumer_credit_cycles)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Regime%20Detection%20%7C%20Interpretability-red)](https://github.com/chirindaopensource/geometric_dynamics_consumer_credit_cycles)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)\n",
        "[![Matplotlib](https://img.shields.io/badge/matplotlib-%2311557c.svg?style=flat&logo=matplotlib&logoColor=white)](https://matplotlib.org/)\n",
        "[![Seaborn](https://img.shields.io/badge/seaborn-%233776ab.svg?style=flat&logo=seaborn&logoColor=white)](https://seaborn.pydata.org/)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-gray?style=flat)](https://pyyaml.org/)\n",
        "[![tqdm](https://img.shields.io/badge/tqdm-ff69b4?style=flat)](https://github.com/tqdm/tqdm)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "---\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/geometric_dynamics_consumer_credit_cycles`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Geometric Dynamics of Consumer Credit Cycles: A Multivector-based Linear-Attention Framework for Explanatory Economic Analysis\"** by:\n",
        "\n",
        "*   Agus Sudjianto\n",
        "*   Sandi Setiawan\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and preprocessing to model training, post-hoc attribution analysis, and the generation of all final diagnostic reports.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `execute_geometric_credit_cycle_research`](#key-callable-execute_geometric_credit_cycle_research)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Geometric Dynamics of Consumer Credit Cycles.\" The core of this repository is the iPython Notebook `geometric_dynamics_consumer_credit_cycles_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation of all analytical tables and figures.\n",
        "\n",
        "The paper introduces a novel framework using Geometric Algebra and Linear Attention to move beyond traditional correlation-based analysis of economic cycles. This codebase operationalizes the framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Process raw quarterly macroeconomic data through a causally pure pipeline, including growth transformations and rolling-window standardization.\n",
        "-   Construct multivector embeddings that explicitly model the rotational (feedback) dynamics between variables.\n",
        "-   Train the Linear Attention model using the paper's specified chronological, single-step update algorithm.\n",
        "-   Generate a full suite of diagnostic and interpretability outputs, including temporal attribution, geometric component attribution, and PCA-based regime trajectory plots.\n",
        "-   Conduct systematic robustness analysis through automated hyperparameter sweeps.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in Geometric (Clifford) Algebra, deep learning, and econometric time series analysis.\n",
        "\n",
        "**1. Geometric Algebra (GA) Embedding:**\n",
        "The core innovation is representing the economic state not as a simple vector, but as a **multivector** in a Clifford Algebra. The geometric product of two vectors `a` and `b` decomposes their relationship into a scalar (projective) part and a bivector (rotational) part:\n",
        "$$\n",
        "ab = a \\cdot b + a \\wedge b\n",
        "$$\n",
        "This project implements the multivector embedding from Equation (3) of the paper, which includes scalar, vector, and bivector components. The bivector terms, such as `(x_{u,t} - x_{c,t})(e_u \\wedge e_c)`, are designed to activate when variables diverge, capturing the \"tension\" that drives feedback spirals.\n",
        "\n",
        "**2. Linear Attention Mechanism:**\n",
        "The model uses Linear Attention to identify relevant historical precedents. The attended context vector `O_t` is computed as a weighted average of past information, where the weights are determined by the geometric similarity between the current state (query `Q_t`) and historical states (keys `K_τ`). The key equations implemented are (8), (9), and (10):\n",
        "$$\n",
        "S_t = \\sum_{\\tau \\in \\mathcal{W}_t} K_\\tau V_\\tau^\\top, \\quad Z_t = \\sum_{\\tau \\in \\mathcal{W}_t} K_\\tau\n",
        "$$\n",
        "$$\n",
        "O_t = \\frac{Q_t^\\top S_t}{Q_t^\\top Z_t + \\varepsilon}\n",
        "$$\n",
        "The similarity `Q_t^T K_τ` is a multivector-aware dot product, allowing the model to match not just variable levels but the underlying geometric interaction patterns.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`geometric_dynamics_consumer_credit_cycles_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 23 distinct, modular tasks, each with its own orchestrator function, ensuring clarity, testability, and rigor.\n",
        "-   **Configuration-Driven Design:** All study parameters are managed in an external `config.yaml` file, allowing for easy customization and replication without code changes.\n",
        "-   **Causally Pure Data Pipeline:** Implements professional-grade time series preprocessing, including causally correct rolling-window operations and transformations, with a `valid_mask` system to prevent any look-ahead bias.\n",
        "-   **High-Fidelity Model Implementation:** Includes a complete, from-scratch implementation of the multivector embedding, the custom shifted Leaky ReLU, and the chronological `batch_size=1` training loop as specified in the paper.\n",
        "-   **Comprehensive Interpretability Suite:** Provides functions to generate all key analytical outputs from the paper, including temporal attention heatmaps, geometric occlusion attribution, component magnitude plots, and PCA trajectory analysis.\n",
        "-   **Automated Robustness Analysis:** Includes a top-level function to automatically conduct hyperparameter sweeps, running the entire pipeline for each configuration and compiling the results.\n",
        "-   **Automated Reporting and Archival:** Concludes by automatically generating all publication-ready plots, summary tables, and a complete, timestamped archive of all data, parameters, results, and environment metadata for perfect reproducibility.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation (Tasks 1-2):** Ingests and rigorously validates the `config.yaml` and the raw `pd.DataFrame` against a strict schema.\n",
        "2.  **Data Preprocessing (Tasks 3-6):** Cleanses the data, applies growth transformations, performs rolling-window standardization, and constructs the final `(T, 11)` multivector embedding matrix.\n",
        "3.  **Model Setup (Tasks 7-8):** Initializes all learnable parameters with best-practice schemes (Kaiming/Xavier) and defines the custom activation function.\n",
        "4.  **Training (Tasks 9-16):** Implements the full forward pass (QKV projections, attention statistics, context vector, MLP head), computes the prediction and regularization losses, and executes the chronological, per-time-step training loop to produce the final trained parameters.\n",
        "5.  **Post-Hoc Analysis (Tasks 17-20):** Uses the trained model to compute temporal attributions, geometric attributions, component magnitudes, and the PCA trajectory of the system's state.\n",
        "6.  **Master Orchestration (Tasks 21-23):** Provides top-level functions to run the entire pipeline, conduct robustness sweeps, and generate all final deliverables.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `geometric_dynamics_consumer_credit_cycles_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `execute_geometric_credit_cycle_research`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`execute_geometric_credit_cycle_research`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, from data validation to the final report generation and archival.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `torch`, `matplotlib`, `seaborn`, `pyyaml`, `tqdm`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/geometric_dynamics_consumer_credit_cycles.git\n",
        "    cd geometric_dynamics_consumer_credit_cycles\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy torch matplotlib seaborn pyyaml tqdm\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires a `pandas.DataFrame` with a specific schema, as generated in the \"Usage\" example. All other parameters are controlled by the `config.yaml` file.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `geometric_dynamics_consumer_credit_cycles_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `execute_geometric_credit_cycle_research` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # --- 1. Generate/Load Inputs ---\n",
        "    # A synthetic data generator is included in the notebook for demonstration.\n",
        "    # In a real use case, you would load your data here.\n",
        "    # consolidated_df_raw = pd.read_csv(...)\n",
        "    \n",
        "    # Load the model configuration from the YAML file.\n",
        "    with open('config.yaml', 'r') as f:\n",
        "        model_config = yaml.safe_load(f)\n",
        "        \n",
        "    # Define the hyperparameter grid for robustness analysis.\n",
        "    hyperparameter_grid = {\n",
        "        'hidden_dimension_dh': [32, 64],\n",
        "        'learning_rate_eta': [1e-4, 5e-5]\n",
        "    }\n",
        "    \n",
        "    # --- 2. Execute Pipeline ---\n",
        "    # Define the top-level directory for all outputs.\n",
        "    RESULTS_DIRECTORY = \"research_output\"\n",
        "\n",
        "    # Execute the entire research study.\n",
        "    final_results = execute_geometric_credit_cycle_research(\n",
        "        consolidated_df_raw=consolidated_df_raw, # Assumes this df is generated/loaded\n",
        "        model_config=model_config,\n",
        "        hyperparameter_grid=hyperparameter_grid,\n",
        "        save_dir=RESULTS_DIRECTORY,\n",
        "        base_random_seed=42,\n",
        "        run_robustness_analysis=True,\n",
        "        show_plots=True\n",
        "    )\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline creates a `save_dir` with a highly structured set of outputs. A unique timestamped subdirectory is created for the primary run (e.g., `analysis_run_20251027_103000/`), containing:\n",
        "-   `historical_fit.png` and `diagnostic_dashboard.png`: Publication-quality plots.\n",
        "-   `regime_summary_table.csv`: The data-driven summary of economic regimes.\n",
        "-   `full_results.pkl`: A complete archive of the primary run's results.\n",
        "-   `trained_parameters.pth`: The final PyTorch model parameters.\n",
        "-   `environment.json`: A record of the computational environment.\n",
        "\n",
        "If robustness analysis is run, a top-level file `robustness_analysis_full_results.pkl` is also saved, containing the results from every run in the hyperparameter sweep.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "geometric_dynamics_consumer_credit_cycles/\n",
        "│\n",
        "├── geometric_dynamics_consumer_credit_cycles_draft.ipynb # Main implementation notebook\n",
        "├── config.yaml                                           # Master configuration file\n",
        "├── requirements.txt                                      # Python package dependencies\n",
        "│\n",
        "├── research_output/                                      # Example output directory\n",
        "│   ├── analysis_run_20251027_103000/\n",
        "│   │   ├── historical_fit.png\n",
        "│   │   ├── diagnostic_dashboard.png\n",
        "│   │   ├── regime_summary_table.csv\n",
        "│   │   ├── full_results.pkl\n",
        "│   │   ├── trained_parameters.pth\n",
        "│   │   └── environment.json\n",
        "│   │\n",
        "│   └── robustness_analysis_full_results.pkl\n",
        "│\n",
        "├── LICENSE                                               # MIT Project License File\n",
        "└── README.md                                             # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can easily modify all study parameters, including lookback horizons, model dimensions, activation function parameters, and regularization strengths, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Alternative Geometries:** Exploring other geometric algebras (e.g., Projective Geometric Algebra) or differential geometry frameworks (e.g., Riemannian manifolds) to model economic state spaces.\n",
        "-   **GPU Acceleration:** While the current implementation is efficient, the chronological training loop could be further optimized or parallelized for GPUs for very large datasets or extensive hyperparameter searches.\n",
        "-   **Alternative Attention Mechanisms:** Integrating other efficient attention mechanisms (e.g., Performers, Transformers-are-RNNs) and comparing their diagnostic outputs.\n",
        "-   **Formal Backtesting:** Extending the framework to a formal out-of-sample forecasting or trading strategy backtest to quantify the economic value of the geometric signals.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{sudjianto2025geometric,\n",
        "  title   = {Geometric Dynamics of Consumer Credit Cycles: A Multivector-based Linear-Attention Framework for Explanatory Economic Analysis},\n",
        "  author  = {Sudjianto, Agus and Setiawan, Sandi},\n",
        "  journal = {arXiv preprint arXiv:2510.15892},\n",
        "  year    = {2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Professional-Grade Implementation of the Geometric Dynamics Framework.\n",
        "GitHub repository: https://github.com/chirindaopensource/geometric_dynamics_consumer_credit_cycles\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Agus Sudjianto and Sandi Setiawan** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **PyTorch, Pandas, NumPy, Matplotlib, Seaborn, and Jupyter**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `geometric_dynamics_consumer_credit_cycles_draft.ipynb` notebook and follows best practices for research software documentation.*\n"
      ],
      "metadata": {
        "id": "tQAC_ynbc5Xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Geometric Dynamics of Consumer Credit Cycles: A Multivector-based Linear-Attention Framework for Explanatory Economic Analysis*\"\n",
        "\n",
        "Authors: Agus Sudjianto, Sandi Setiawan\n",
        "\n",
        "E-Journal Submission Date: 8 September 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2510.15892\n",
        "\n",
        "Abstract:\n",
        "\n",
        "This study introduces geometric algebra to decompose credit system relationships into their projective (correlation-like) and rotational (feedback-spiral) components. We represent economic states as multi-vectors in Clifford algebra, where bivector elements capture the rotational coupling between unemployment, consumption, savings, and credit utilization. This mathematical framework reveals interaction patterns invisible to conventional analysis: when unemployment and credit contraction enter simultaneous feedback loops, their geometric relationship shifts from simple correlation to dangerous rotational dynamics that characterize systemic crises."
      ],
      "metadata": {
        "id": "6pnK4kmDuPU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "\n",
        "### **Summary: \"Geometric Dynamics of Consumer Credit Cycles\"**\n",
        "\n",
        "#### **The Core Economic Problem and Motivation**\n",
        "\n",
        "The paper starts with a compelling and well-known puzzle. The 1990-91 recession and the 2008 Great Financial Crisis both showed a high correlation (ρ ≈ 0.7-0.8) between rising unemployment and credit defaults. Yet, they were fundamentally different events.\n",
        "\n",
        "*   **1990-91 Recession:** A sequential, almost *additive* process. Unemployment rose, and *then*, with a predictable lag, defaults increased as households exhausted their savings. The system was stressed but not spiraling.\n",
        "*   **2008 Financial Crisis:** A simultaneous, *multiplicative* feedback loop. Rising unemployment caused defaults, which caused banks to tighten credit, which slowed the economy, which caused more unemployment. The variables were amplifying each other in a dangerous, self-reinforcing spiral.\n",
        "\n",
        "Traditional models like Vector Autoregressions (VARs) would see the high correlation in both periods and struggle to capture this mechanistic difference without pre-specifying different \"regimes.\" The authors posit that this difference is inherently geometric, and they propose a mathematical framework to capture it directly.\n",
        "\n",
        "--\n",
        "\n",
        "#### **The Methodological Framework: A Two-Part Innovation**\n",
        "\n",
        "The technical core of the paper combines two powerful ideas: Geometric Algebra for representation and Linear Attention for dynamic analysis.\n",
        "\n",
        "**Part A: Geometric Algebra for Economic Representation**\n",
        "\n",
        "This is the most novel contribution. Instead of representing economic variables as a simple vector of scalars, they embed them in a **Clifford Algebra**, also known as Geometric Algebra (GA). The key insight comes from the **geometric product** of two vectors (say, unemployment `u` and credit `c`):\n",
        "\n",
        "`uc = u·c + u∧c`\n",
        "\n",
        "Let's dissect this from a financial mathematics perspective:\n",
        "\n",
        "1.  **The Inner Product (`u·c`):** This is a scalar. It captures the **projective** relationship between the variables. Think of it as a sophisticated version of correlation—it measures how much one variable moves in the direction of the other. This represents the simple, linear, lead-lag relationships seen in normal cycles.\n",
        "\n",
        "2.  **The Outer Product (`u∧c`):** This is not a scalar; it's a new mathematical object called a **bivector**. A bivector represents an oriented plane. In this economic context, its magnitude captures the strength of the **rotational** coupling or feedback loop between `u` and `c`. A large bivector magnitude means the variables are not just correlated but are mutually reinforcing each other in a spiral.\n",
        "\n",
        "So, an economic state at time `t` is represented as a **multivector**, `Mt`, which is a composite object containing a scalar part (baseline level), a vector part (individual variable movements), and a bivector part (pairwise feedback loops).\n",
        "\n",
        "*   **Vector-dominated regime:** Normal business cycle. Additive stress.\n",
        "*   **Bivector-dominated regime:** Systemic crisis. Multiplicative, spiraling feedback.\n",
        "\n",
        "**Part B: Linear Attention for Dynamic Pattern Recognition**\n",
        "\n",
        "Having a rich representation is not enough; the model must adapt over time. The authors cleverly employ a **linear attention mechanism**, a concept from deep learning, for this purpose.\n",
        "\n",
        "From a computer science perspective, attention is typically used to find relevant parts of an input sequence. Here, it's used to find relevant *historical precedents*.\n",
        "\n",
        "1.  **Query, Key, Value:** At each time `t`, the current economic state (the multivector `Mt`) is projected to form a **Query**. The historical states (`Mτ` for `τ < t`) are projected to form **Keys** and **Values**.\n",
        "2.  **Geometric Similarity:** The model calculates the similarity between the current Query and all historical Keys. Crucially, because the Query and Keys are multivectors, this is a **geometric similarity**. The model isn't just asking, \"When was unemployment at this level before?\" It's asking, \"When did the *entire geometric configuration* of feedback loops and projections look like it does today?\"\n",
        "3.  **Time-Varying Parameters:** The attention weights determine how much to draw from each historical period's Value. This creates a context vector that is essentially a weighted average of past economic states, where the weights are determined by geometric analogy. This allows the model's effective parameters to change continuously and endogenously, without needing pre-defined regimes. It learns which historical patterns are most relevant *right now*.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Empirical Application and Key Findings**\n",
        "\n",
        "The authors apply this framework to quarterly U.S. data from 1980-2024, using variables like unemployment, savings, consumption, and credit.\n",
        "\n",
        "*   **Crisis Signatures:** The model successfully identifies distinct \"geometric signatures\" for different periods. The 2008 crisis is marked by a massive spike in the magnitude of the **unemployment-credit** and **unemployment-consumption bivectors**, confirming the feedback loop hypothesis. The 1990-91 recession shows much weaker bivector activity. The 2020 COVID crisis shows a unique signature dominated by **savings-related bivectors**, reflecting the unusual impact of fiscal stimulus and lockdowns.\n",
        "*   **Economic Hysteresis:** The trajectory of the economic state in a phase-space plot (Figure 2) shows that the path out of a crisis is different from the path in. This provides formal evidence for hysteresis, or \"economic scarring,\" where the structure of the economy is permanently altered by a crisis.\n",
        "*   **Interpretability:** The attention weights are highly interpretable. During stable times, the model places high weight on the recent past. As a crisis approaches, the attention spreads out, searching deeper into history for relevant analogies.\n",
        "*   **Contemporary Analysis (2022-2024):** The model suggests that recent economic stress is geometrically more similar to the manageable downturns of 1990-91 or 2001-02 than to the systemic crisis of 2008. The dynamics are primarily vector-dominated (additive), not bivector-dominated (multiplicative). This is a powerful, model-driven insight into current risk.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Theoretical Guarantees**\n",
        "\n",
        "The paper includes a section on theoretical foundations, which is essential for taking this from a clever heuristic to a rigorous framework. They establish several key properties:\n",
        "\n",
        "1.  **Geometric Invariance:** The model's core findings (the attention scores) are invariant to rotations, meaning the identified economic relationships are fundamental and not an artifact of the chosen coordinate system or data scaling.\n",
        "2.  **Stability and Boundedness:** They prove the model is Lipschitz continuous, meaning small changes in input data will not cause wild swings in predictions. This is critical for any model intended for policy use.\n",
        "3.  **Exact Impulse Response:** They derive an analytical formula for how a shock to a variable at a past time `τ` propagates to the current prediction. This connects the complex ML model back to the language of traditional econometrics and allows for precise scenario analysis.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Contributions and Implications**\n",
        "\n",
        "This paper makes three significant contributions:\n",
        "\n",
        "1.  **To Econometrics:** It offers a principled way to move beyond linear, static-parameter models. It provides a data-driven method for modeling regime changes and complex interactions that is more flexible than traditional switching models and more interpretable than \"black-box\" neural networks.\n",
        "2.  **To Machine Learning:** It presents a compelling, non-physical application of Geometric Algebra. More importantly, it showcases how an architectural choice (Linear Attention) can be motivated by the need for *interpretability* (reasoning by historical analogy) rather than just computational efficiency.\n",
        "3.  **To Finance and Policy:** It provides a new, powerful diagnostic tool. An \"early warning system\" could be built by monitoring the ratio of bivector to vector magnitudes. A rising ratio would signal a shift from normal cyclical stress to dangerous, self-reinforcing feedback dynamics, potentially justifying more aggressive policy intervention.\n",
        "\n",
        "In conclusion, this is a first-rate piece of research. It elegantly combines deep mathematical concepts with a state-of-the-art machine learning architecture to shed new light on a classic and critical economic problem. It lays the groundwork for a new class of explanatory, dynamic models for macroeconomic and financial analysis."
      ],
      "metadata": {
        "id": "FR32QvJRw3QS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "pB281A7GjbbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  Geometric Dynamics of Consumer Credit Cycles: A Multivector-based\n",
        "#  Linear-Attention Framework for Explanatory Economic Analysis\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Geometric Dynamics of Consumer Credit\n",
        "#  Cycles\" by Sudjianto and Setiawan (2025). It delivers a robust system for\n",
        "#  the explanatory analysis of economic regimes, capable of distinguishing\n",
        "#  between stable, projective dynamics and unstable, rotational (feedback-spiral)\n",
        "#  dynamics that characterize systemic crises.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Geometric Algebra (Clifford Algebra) for state representation, decomposing\n",
        "#    variable interactions into projective (correlation-like) and rotational\n",
        "#    (feedback-loop) components.\n",
        "#  • Multivector embeddings that explicitly model pairwise interactions (bivectors)\n",
        "#    between key macroeconomic variables (unemployment, savings, consumption, credit).\n",
        "#  • A Linear Attention mechanism that identifies historical precedents based on\n",
        "#    geometric similarity across scalar, vector, and bivector components.\n",
        "#  • A time-varying coefficient interpretation where model parameters evolve based\n",
        "#    on data-driven analogies to past economic regimes.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • A fully modular and reproducible pipeline from raw data ingestion to final\n",
        "#    analytical deliverables.\n",
        "#  • Causally pure time series preprocessing, including rolling-window standardization\n",
        "#    and growth rate transformations.\n",
        "#  • A custom, numerically stable shifted Leaky ReLU activation function to ensure\n",
        "#    positivity in the attention mechanism.\n",
        "#  • An efficient, vectorized implementation of the chronological, single-step\n",
        "#    training algorithm specified in the paper.\n",
        "#  • A suite of post-hoc interpretability tools, including temporal attribution,\n",
        "#    geometric occlusion analysis, and PCA-based trajectory visualization.\n",
        "#  • A complete framework for robustness analysis via hyperparameter sweeps.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Sudjianto, A., & Setiawan, S. (2025). Geometric Dynamics of Consumer Credit\n",
        "#  Cycles: A Multivector-based Linear-Attention Framework for Explanatory\n",
        "#  Economic Analysis. arXiv preprint arXiv:2510.15892.\n",
        "#  https://arxiv.org/abs/2510.15892\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "# Consolidated Imports for the Geometric Credit Cycle Analysis Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Core Scientific Computing ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- PyTorch for Deep Learning ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Standard Python Libraries ---\n",
        "import warnings\n",
        "import copy\n",
        "import json\n",
        "import pickle\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Set, Tuple, Union\n",
        "\n",
        "# --- Visualization ---\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Utilities ---\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "TRcttdjQje-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "SgHvlLmPjgAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Functional-Methodological Summary of All Orchestrator Callables**\n",
        "\n",
        "Here is a callable-by-callable breakdown of the entire pipeline.\n",
        "\n",
        "#### **Task 1: `validate_raw_data`**\n",
        "*   **Inputs:** `consolidated_df_raw` (a `pd.DataFrame`).\n",
        "*   **Processes:**\n",
        "    1.  Verifies the DataFrame's index is a continuous, monotonic, quarter-end `pd.DatetimeIndex`.\n",
        "    2.  Confirms the presence and `float64` dtype of the five required columns (`UNRATE`, `PCE`, `PSAVERT`, `REVOLSL`, `CORCACBS`).\n",
        "    3.  Checks that all data points are finite and fall within economically plausible ranges (e.g., `PCE > 0`, `0 <= UNRATE <= 100`).\n",
        "*   **Outputs:** A boolean `True` if all checks pass; otherwise, it raises a specific `ValueError` or `TypeError`.\n",
        "*   **Data Transformation:** This is a pure validation function; it does not transform data.\n",
        "*   **Role in Research Pipeline:** This function implements the foundational data integrity checks described implicitly in **Section 4.2 (Data Construction and Variable Selection)**. It ensures that the raw data fed into the pipeline is structurally sound and semantically valid before any transformations are applied, preventing downstream errors.\n",
        "\n",
        "#### **Task 2: `validate_model_config`**\n",
        "*   **Inputs:** `model_config` (a `dict`).\n",
        "*   **Processes:**\n",
        "    1.  Recursively traverses the nested dictionary structure.\n",
        "    2.  For each key, it verifies its presence, data type, and value against a schema derived from the paper's specifications (e.g., in **Sections 4.2, 4.3, 4.4**).\n",
        "    3.  Performs critical consistency checks, such as ensuring the `component_order` and `pi_order` lists are exact and internally consistent.\n",
        "*   **Outputs:** A boolean `True` if the configuration is valid; otherwise, it raises `KeyError`, `TypeError`, or `ValueError`.\n",
        "*   **Data Transformation:** This is a pure validation function.\n",
        "*   **Role in Research Pipeline:** This function ensures that all hyperparameters and structural definitions used in the model are a perfect match for those specified throughout the paper, particularly in **Section 4 (Empirical Motivation and Analytical Protocol)**. It is the primary guardrail for ensuring a high-fidelity reproduction.\n",
        "\n",
        "#### **Task 3: `cleanse_and_prepare_data`**\n",
        "*   **Inputs:** `consolidated_df_raw` (`pd.DataFrame`), `model_config` (`dict`).\n",
        "*   **Processes:**\n",
        "    1.  Sorts the data chronologically and removes duplicate entries.\n",
        "    2.  Removes any rows with `NaN` or `inf` values.\n",
        "    3.  Removes any rows where `PCE` or `REVOLSL` are not strictly positive.\n",
        "    4.  Verifies that the resulting DataFrame has a continuous, gap-free quarterly index.\n",
        "    5.  Creates a boolean `valid_mask` based on `warmup_min_observations`, correctly implementing the rule `t < max(standardization_window, lookback_horizon_L)`.\n",
        "*   **Outputs:** A tuple containing `consolidated_df_clean` (`pd.DataFrame`), `valid_mask` (`np.ndarray`), and a `cleaning_report` (`dict`).\n",
        "*   **Data Transformation:** It transforms the raw DataFrame into a cleansed, structurally sound DataFrame and generates the initial validity mask.\n",
        "*   **Role in Research Pipeline:** This function implements the data cleaning and warm-up period definition described in **Section 4.2 (Data Preprocessing)**. It prepares the dataset for causal time series operations by ensuring data integrity and explicitly defining which time steps have sufficient history.\n",
        "\n",
        "#### **Task 4: `apply_growth_transformations`**\n",
        "*   **Inputs:** `consolidated_df_clean` (`pd.DataFrame`), `valid_mask` (`np.ndarray`).\n",
        "*   **Processes:**\n",
        "    1.  Calculates the log-difference for the `PCE` and `REVOLSL` series.\n",
        "    2.  Extracts the `UNRATE` and `PSAVERT` series as levels.\n",
        "    3.  Assembles these four series into a new DataFrame with columns `['u', 's', 'r', 'v']`.\n",
        "    4.  Updates the `valid_mask` to mark the first time step as invalid due to the differencing operation.\n",
        "*   **Outputs:** A tuple containing `features_df` (`pd.DataFrame`) and the `updated_valid_mask` (`np.ndarray`).\n",
        "*   **Data Transformation:** It transforms the level series for consumption and credit into growth rates, implementing the variable definitions used in the model's vector space, as mentioned in **Section 3.2 (Multivector Representation)** where `r_t` and `v_t` are consumption and credit growth. The specific transformation is the log difference:\n",
        "    $$ r_t = \\Delta \\log(\\text{PCE}_t) = \\log(\\text{PCE}_t) - \\log(\\text{PCE}_{t-1}) $$\n",
        "    $$ v_t = \\Delta \\log(\\text{REVOLSL}_t) = \\log(\\text{REVOLSL}_t) - \\log(\\text{REVOLSL}_{t-1}) $$\n",
        "*   **Role in Research Pipeline:** This function creates the four fundamental economic variables that form the basis of the model's 4D vector space.\n",
        "\n",
        "#### **Task 5: `apply_rolling_standardization`**\n",
        "*   **Inputs:** `features_df` (`pd.DataFrame`), `valid_mask` (`np.ndarray`), `model_config` (`dict`).\n",
        "*   **Processes:**\n",
        "    1.  Calculates the 8-quarter rolling mean (`μ_{z,t}`) and rolling standard deviation (`σ_{z,t}`) for each of the four feature columns.\n",
        "    2.  Applies a numerical stability floor to the standard deviation: `\\tilde{\\sigma}_{z,t} = max(\\sigma_{z,t}, \\varepsilon_{\\text{std}})`.\n",
        "    3.  Applies the standardization formula to each feature.\n",
        "*   **Outputs:** A tuple containing `standardized_features_df` (`pd.DataFrame`) and the `valid_mask`.\n",
        "*   **Data Transformation:** It transforms the pre-processed features into standardized features with rolling statistics. This implements the process described in **Section 4.2 (Data Preprocessing)**. The core equation is:\n",
        "    $$ x_{z,t} = \\frac{z_t - \\mu_{z,t}}{\\tilde{\\sigma}_{z,t}} $$\n",
        "*   **Role in Research Pipeline:** This is the final feature engineering step, ensuring all input variables are normalized. This is crucial for the stability of the neural network training and for making the magnitudes of the different geometric components comparable.\n",
        "\n",
        "#### **Task 6: `construct_multivector_embedding`**\n",
        "*   **Inputs:** `standardized_features_df` (`pd.DataFrame`), `model_config` (`dict`).\n",
        "*   **Processes:**\n",
        "    1.  Initializes a `(T, 11)` zero matrix.\n",
        "    2.  Populates column 0 with the scalar component (1.0).\n",
        "    3.  Populates columns 1-4 with the four standardized features (the vector components).\n",
        "    4.  For each of the 6 bivector pairs `(i, j)` in `pi_order`, it computes the difference `x_{t,i} - x_{t,j}` and populates the corresponding columns (5-10).\n",
        "*   **Outputs:** The `multivector_matrix` (`np.ndarray`).\n",
        "*   **Data Transformation:** It transforms the `(T, 4)` feature matrix into the `(T, 11)` multivector embedding matrix.\n",
        "*   **Role in Research Pipeline:** This is the heart of the paper's theoretical contribution. It implements the multivector representation from **Section 3.2, Equation (3)**:\n",
        "    $$ M_t = \\alpha_0 + \\sum_{i=1}^4 \\alpha_i e_i + \\sum_{(i,j) \\in \\Pi} \\gamma_{ij} (x_{t,i} - x_{t,j}) (e_i \\wedge e_j) $$\n",
        "    (where the implementation sets `α` and `γ` coefficients to 1). This function constructs the mathematical object that allows the model to distinguish between projective and rotational dynamics.\n",
        "\n",
        "#### **Task 7: `initialize_model_and_optimizer`**\n",
        "*   **Inputs:** `model_config` (`dict`), `random_seed` (`int`).\n",
        "*   **Processes:**\n",
        "    1.  Sets the global `torch` random seed for reproducibility.\n",
        "    2.  Initializes the projection matrices (`W_Q`, `W_K`, `W_V`) and MLP head parameters (`W1`, `b1`, `W2`, `b2`) with appropriate dimensions and initialization schemes (Kaiming/Xavier).\n",
        "    3.  Wraps all tensors in `torch.nn.Parameter`.\n",
        "    4.  Instantiates the `torch.optim.Adam` optimizer with all parameters and hyperparameters from the config.\n",
        "*   **Outputs:** A tuple containing a `dict` of `parameters` and the `optimizer` object.\n",
        "*   **Data Transformation:** This function creates the learnable state of the model from scratch.\n",
        "*   **Role in Research Pipeline:** This implements the setup for the machine learning model described in **Section 3.3** and the training setup from **Section 4.4**. It creates the learnable linear maps `W_Q, W_K, W_V` from **Equation (6)** and the MLP head from **Equation (12)**.\n",
        "\n",
        "#### **Task 8: `shifted_leaky_relu`**\n",
        "*   **Inputs:** An input tensor `x`, `model_config` (`dict`).\n",
        "*   **Processes:** Applies the piecewise activation function element-wise.\n",
        "*   **Outputs:** A new tensor of the same shape as `x`.\n",
        "*   **Data Transformation:** A non-linear element-wise transformation.\n",
        "*   **Role in Research Pipeline:** This function implements the feature map `φ(·)` from **Section 3.3, Equation (7)**:\n",
        "    $$ \\phi(x) = \\begin{cases} x + 1, & x > 0 \\\\ \\alpha x + 1, & x \\le 0 \\end{cases} $$\n",
        "    Its purpose is to ensure the positivity of the query and key vectors, which is critical for the stability of the rational attention denominator.\n",
        "\n",
        "#### **Task 9: `compute_qkv_projections`**\n",
        "*   **Inputs:** `multivector_matrix` (`np.ndarray`), `parameters` (`dict`), `model_config` (`dict`).\n",
        "*   **Processes:**\n",
        "    1.  Performs the matrix multiplication `M * W_Q^T` and applies the `shifted_leaky_relu`.\n",
        "    2.  Performs the matrix multiplication `M * W_K^T` and applies the `shifted_leaky_relu`.\n",
        "    3.  Performs the linear matrix multiplication `M * W_V^T`.\n",
        "*   **Outputs:** A dictionary containing the `queries`, `keys`, and `values` tensors, each of shape `(T, d_h)`.\n",
        "*   **Data Transformation:** It projects the `(T, 11)` multivector embedding into the `(T, 32)` query, key, and value spaces.\n",
        "*   **Role in Research Pipeline:** This function implements the projections from **Section 3.3, Equation (6)**:\n",
        "    $$ Q_t = \\phi(W_Q M_t), \\quad K_t = \\phi(W_K M_t), \\quad V_t = W_V M_t $$\n",
        "\n",
        "#### **Task 10: `compute_attention_statistics`**\n",
        "*   **Inputs:** `qkv_projections` (`dict`), `valid_mask` (`torch.Tensor`), `model_config` (`dict`).\n",
        "*   **Processes:**\n",
        "    1.  Creates causally pure (lagged) versions of the `keys` and `values` tensors.\n",
        "    2.  Computes the sequence of outer products `K_{τ-1} V_{τ-1}^T`.\n",
        "    3.  Applies an efficient rolling sum of size `L` to the lagged outer products and lagged keys.\n",
        "*   **Outputs:** A dictionary containing the `S_statistics` tensor `(T, d_h, d_h)` and `Z_statistics` tensor `(T, d_h)`.\n",
        "*   **Data Transformation:** It aggregates the historical key and value information into the two sufficient statistics needed for linear attention.\n",
        "*   **Role in Research Pipeline:** This function implements the calculation of the sufficient statistics from **Section 3.3.1, Equations (8) and (9)**:\n",
        "    $$ S_t = \\sum_{\\tau \\in \\mathcal{W}_t} K_\\tau V_\\tau^\\top, \\quad Z_t = \\sum_{\\tau \\in \\mathcal{W}_t} K_\\tau $$\n",
        "    (where the implementation correctly uses the causal window `W_t = {t-L, ..., t-1}`).\n",
        "\n",
        "#### **Task 11: `compute_attended_context`**\n",
        "*   **Inputs:** `qkv_projections` (`dict`), `attention_statistics` (`dict`), `valid_mask` (`torch.Tensor`), `model_config` (`dict`).\n",
        "*   **Processes:**\n",
        "    1.  Computes the numerator `Q_t^T S_t` for all `t` using batch matrix multiplication.\n",
        "    2.  Computes the denominator `Q_t^T Z_t + ε` for all `t` using batch dot products.\n",
        "    3.  Performs the element-wise division to get the final context vector `O_t`.\n",
        "    4.  Zeros out the results for invalid time steps.\n",
        "*   **Outputs:** The `attended_context` tensor `(T, d_h)`.\n",
        "*   **Data Transformation:** It transforms the queries and historical statistics into the final context vector that summarizes the relevant past.\n",
        "*   **Role in Research Pipeline:** This function implements the final step of the linear attention calculation from **Section 3.3.1, Equation (10)**:\n",
        "    $$ O_t = \\frac{Q_t^\\top S_t}{Q_t^\\top Z_t + \\varepsilon} $$\n",
        "\n",
        "#### **Task 12: `mlp_forward_pass`**\n",
        "*   **Inputs:** `attended_context` (`torch.Tensor`), `parameters` (`dict`).\n",
        "*   **Processes:**\n",
        "    1.  Performs the first linear transformation and applies the ReLU activation.\n",
        "    2.  Performs the second linear transformation to produce a scalar output.\n",
        "*   **Outputs:** A 1D `predictions` tensor `(T,)`.\n",
        "*   **Data Transformation:** It maps the `(T, d_h)` context vectors to `(T,)` scalar predictions.\n",
        "*   **Role in Research Pipeline:** This function implements the MLP prediction head from **Section 3.4, Equation (12)**:\n",
        "    $$ \\hat{y}_t = f_{\\text{MLP}}(O_t) = W_2 \\sigma(W_1 O_t + b_1) + b_2 $$\n",
        "\n",
        "#### **Task 13: `compute_prediction_loss`**\n",
        "*   **Inputs:** `predictions` (`torch.Tensor`), `ground_truth` (`torch.Tensor`), `valid_mask` (`torch.Tensor`).\n",
        "*   **Processes:**\n",
        "    1.  Uses the `valid_mask` to filter the predictions and ground truth tensors.\n",
        "    2.  Computes the Mean Squared Error between the valid subsets.\n",
        "*   **Outputs:** A scalar `torch.Tensor` representing the loss.\n",
        "*   **Data Transformation:** It transforms the time series of predictions and targets into a single scalar objective value.\n",
        "*   **Role in Research Pipeline:** This implements the primary objective function for training, as specified in **Section 4.5 (Training)** of the `model_config`. The core equation is:\n",
        "    $$ \\mathcal{L}_{\\text{pred}} = \\frac{1}{|\\mathcal{T}_{\\text{valid}}|} \\sum_{t \\in \\mathcal{T}_{\\text{valid}}} (\\hat{y}_t - y_t)^2 $$\n",
        "\n",
        "#### **Task 14: `compute_regularization_loss`**\n",
        "*   **Inputs:** `parameters` (`dict`), `model_config` (`dict`).\n",
        "*   **Processes:**\n",
        "    1.  Computes the squared Frobenius norm for `W_Q`, `W_K`, and `W_V`.\n",
        "    2.  Applies the respective hyperparameters `λ_QK` and `λ_V` and sums the terms.\n",
        "*   **Outputs:** A scalar `torch.Tensor` representing the regularization penalty.\n",
        "*   **Data Transformation:** It transforms the parameter matrices into a single scalar penalty value.\n",
        "*   **Role in Research Pipeline:** This implements the structured L2 regularization from **Section 4.4, Equation (24)** (equivalent to Equation 35):\n",
        "    $$ \\mathcal{L}_{\\text{reg}} = \\lambda_{QK} (\\|W_Q\\|_F^2 + \\|W_K\\|_F^2) + \\lambda_V \\|W_V\\|_F^2 $$\n",
        "\n",
        "#### **Task 15: `perform_training_step`**\n",
        "*   **Inputs:** `prediction_loss` (`torch.Tensor`), `regularization_loss` (`torch.Tensor`), `optimizer` (`torch.optim.Optimizer`).\n",
        "*   **Processes:**\n",
        "    1.  Sums the two loss components to get the total loss.\n",
        "    2.  Calls `optimizer.zero_grad()`.\n",
        "    3.  Calls `total_loss.backward()`.\n",
        "    4.  Calls `optimizer.step()`.\n",
        "*   **Outputs:** A scalar `float` of the total loss for logging.\n",
        "*   **Data Transformation:** This function has the side effect of modifying the model's parameters in place.\n",
        "*   **Role in Research Pipeline:** This function encapsulates a single step of the gradient descent algorithm used to train the model.\n",
        "\n",
        "#### **Task 16: `train_model`**\n",
        "*   **Inputs:** All pre-processed data, initial parameters, optimizer, and config.\n",
        "*   **Processes:**\n",
        "    1.  Loops over the specified number of epochs.\n",
        "    2.  Within each epoch, loops chronologically through each time step `t`.\n",
        "    3.  For each valid `t`, it performs a full forward pass, computes the loss, and calls `perform_training_step` to update the parameters.\n",
        "    4.  Manages the causal history of `K` and `V` projections.\n",
        "*   **Outputs:** A tuple of the final `trained_parameters` (`dict`) and the `training_history` (`list`).\n",
        "*   **Data Transformation:** This is the main stateful function that transforms the initialized parameters into trained parameters through iterative optimization.\n",
        "*   **Role in Research Pipeline:** This function orchestrates the entire training process described in **Section 4.5**.\n",
        "\n",
        "#### **Task 17: `compute_temporal_attribution`**\n",
        "*   **Inputs:** `multivector_matrix`, trained `parameters`, `valid_mask`, `model_config`.\n",
        "*   **Processes:**\n",
        "    1.  Computes the final `Q` and `K` tensors using the trained parameters.\n",
        "    2.  For each time `T`, computes the raw dot-product scores `a_τ = Q_T^T K_τ` for all `τ` in the lookback window.\n",
        "    3.  Normalizes these scores using the Softmax function for stability.\n",
        "*   **Outputs:** A `(T, L)` NumPy array of attention weights.\n",
        "*   **Data Transformation:** It transforms the trained model's internal states (`Q`, `K`) into an interpretable matrix of attention weights.\n",
        "*   **Role in Research Pipeline:** This function implements the temporal attribution analysis from **Section 3.6**, based on the attention weights from **Equation (16)** (though using a more robust softmax normalization):\n",
        "    $$ w_{\\tau,T} = \\frac{Q_T^\\top K_\\tau}{\\sum_{j=T-L}^{T-1} Q_T^\\top K_j} $$\n",
        "\n",
        "#### **Task 18: `compute_geometric_attribution`**\n",
        "*   **Inputs:** `multivector_matrix`, trained `parameters`, `valid_mask`, `model_config`.\n",
        "*   **Processes:**\n",
        "    1.  Computes a baseline prediction `ŷ`.\n",
        "    2.  Iteratively creates occluded versions of the `multivector_matrix` by zeroing out the scalar, vector, and bivector components in turn.\n",
        "    3.  Runs a full forward pass for each occluded matrix to get `ŷ_occluded`.\n",
        "    4.  Calculates the attribution as the difference `ŷ - ŷ_occluded`.\n",
        "*   **Outputs:** A dictionary of attribution scores for each component.\n",
        "*   **Data Transformation:** An analysis function that transforms the model and data into a set of attribution time series.\n",
        "*   **Role in Research Pipeline:** This function implements the geometric attribution via controlled occlusion from **Section 3.6, Equation (17)**:\n",
        "    $$ \\Delta^B \\hat{y}_T = \\hat{y}_T - \\hat{y}_T|_{M(B)=0} $$\n",
        "    (where the implementation correctly occludes the source `M` instead of the intermediate `Q`).\n",
        "\n",
        "#### **Task 19: `compute_component_magnitudes`**\n",
        "*   **Inputs:** `multivector_matrix`, trained `parameters`, `valid_mask`, `model_config`, `date_index`.\n",
        "*   **Processes:**\n",
        "    1.  Computes a baseline context vector `O`.\n",
        "    2.  Iteratively creates occluded versions of the `multivector_matrix`.\n",
        "    3.  Runs a forward pass up to the context vector for each to get `O_occluded`.\n",
        "    4.  Calculates the L2 norm of the difference vector `||O - O_occluded||_2`.\n",
        "*   **Outputs:** A `pd.DataFrame` of the component magnitudes over time.\n",
        "*   **Data Transformation:** An analysis function that transforms the model's internal state changes into a time series of diagnostic metrics.\n",
        "*   **Role in Research Pipeline:** This function provides the quantitative data needed for the regime diagnostics and the component evolution heatmap shown in **Figure 3** and discussed in **Section 6.3**.\n",
        "\n",
        "#### **Task 20: `perform_pca_on_context`**\n",
        "*   **Inputs:** `attended_context` tensor, `ground_truth`, `valid_mask`, `date_index`.\n",
        "*   **Processes:**\n",
        "    1.  Filters the context vectors for valid time steps.\n",
        "    2.  Centers the data.\n",
        "    3.  Performs PCA via SVD to find the first two principal components.\n",
        "    4.  Projects the centered data onto these two components.\n",
        "*   **Outputs:** A `pd.DataFrame` containing the PC coordinates and corresponding charge-off rates, and the explained variance ratio.\n",
        "*   **Data Transformation:** A dimensionality reduction technique.\n",
        "*   **Role in Research Pipeline:** This function implements the PCA needed to generate the trajectory analysis plot shown in **Figure 2** and discussed in **Section 6.2**.\n",
        "\n",
        "#### **Task 21: `run_full_analysis_pipeline`**\n",
        "*   **Inputs:** `consolidated_df_raw`, `model_config`, `random_seed`, `**kwargs`.\n",
        "*   **Processes:** Sequentially calls the orchestrators for Tasks 1-7, 16, and 17-20.\n",
        "*   **Outputs:** A comprehensive dictionary containing all data, model, and analysis artifacts.\n",
        "*   **Data Transformation:** Orchestrates the entire end-to-end transformation from raw data to final results.\n",
        "*   **Role in Research Pipeline:** This is the master orchestrator for a single, complete run of the entire research methodology.\n",
        "\n",
        "#### **Task 22: `conduct_robustness_analysis`**\n",
        "*   **Inputs:** `consolidated_df_raw`, `model_config`, `hyperparameter_grid`, `base_random_seed`.\n",
        "*   **Processes:**\n",
        "    1.  Generates a grid of hyperparameter combinations.\n",
        "    2.  Loops through the grid, calling `run_full_analysis_pipeline` for each combination with the appropriate overrides.\n",
        "*   **Outputs:** A list of the comprehensive results dictionaries, one for each run.\n",
        "*   **Data Transformation:** Orchestrates multiple runs of the entire pipeline.\n",
        "*   **Role in Research Pipeline:** This function implements the robustness analysis framework described in the plan for **Task 22**.\n",
        "\n",
        "#### **Task 23: `generate_final_deliverables`**\n",
        "*   **Inputs:** A `results` dictionary from a pipeline run, `save_dir`.\n",
        "*   **Processes:**\n",
        "    1.  Calls the various plotting functions (`plot_historical_fit`, `plot_diagnostic_visuals`).\n",
        "    2.  Calls the table generation function (`_create_regime_summary_table`).\n",
        "    3.  Calls the archiving function (`archive_analysis_results`).\n",
        "*   **Outputs:** None. This function has the side effect of creating files (plots, CSVs, pickles).\n",
        "*   **Data Transformation:** A reporting function that transforms the results dictionary into human-readable and archivable formats.\n",
        "*   **Role in Research Pipeline:** This function implements the final reporting and archival steps described in **Task 23**.\n",
        "\n",
        "#### **Final Master Orchestrator: `execute_geometric_credit_cycle_research`**\n",
        "*   **Inputs:** All raw inputs for the entire project.\n",
        "*   **Processes:** Calls `run_full_analysis_pipeline`, `generate_final_deliverables`, and optionally `conduct_robustness_analysis`.\n",
        "*   **Outputs:** A dictionary containing the primary results and the robustness analysis results.\n",
        "*   **Data Transformation:** The highest-level orchestrator that manages the entire research workflow.\n",
        "*   **Role in Research Pipeline:** This is the single, top-level entry point for executing the entire study.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### **Usage Example**\n",
        "\n",
        "### **High-Fidelity Example: Executing the Dynamic Diagnostic Engine**\n",
        "\n",
        "This example demonstrates the end-to-end execution of the research pipeline. It is structured to be a practical guide for a quantitative analyst or researcher tasked with running this model. We will proceed in three distinct, granular stages:\n",
        "\n",
        "1.  **Input Preparation:** We will define and load all necessary inputs for the master orchestrator function. This includes generating a high-fidelity synthetic dataset, defining the hyperparameter grid for robustness checks, and loading the model configuration from a YAML file.\n",
        "2.  **Pipeline Execution:** We will make a single, well-defined call to the master orchestrator, `execute_geometric_credit_cycle_research`, to run the entire analysis.\n",
        "3.  **Output Deconstruction:** We will dissect the comprehensive results dictionary returned by the pipeline, explaining what each key artifact represents and how it would be used in a research report.\n",
        "\n",
        "#### **Stage 1: Input Preparation**\n",
        "\n",
        "A robust pipeline requires well-defined inputs. Here, we prepare the three primary inputs: the raw data, the model configuration, and the hyperparameter grid for sensitivity analysis.\n",
        "\n",
        "**1.A. Synthetic Data Generation (`consolidated_df_raw`)**\n",
        "\n",
        "For this demonstration to be self-contained and reproducible, we will generate a synthetic `pandas.DataFrame`. This DataFrame is meticulously crafted to mimic the structure, naming conventions, and dynamic properties of the real-world economic data specified in the paper. It includes a plausible business cycle, allowing us to observe how the model's diagnostics would behave during a crisis.\n",
        "\n",
        "*   **`consolidated_df_raw`**: This is a `pandas.DataFrame` that serves as the sole data input for the entire pipeline. Its structure must be exact.\n",
        "    *   **Index**: A `pandas.DatetimeIndex` with a quarterly frequency, using quarter-end dates (e.g., 'YYYY-MM-DD'). This is the temporal backbone of the analysis.\n",
        "    *   **Columns**: Five specific columns, each representing a raw, seasonally adjusted economic time series.\n",
        "\n",
        "```python\n",
        "# Import necessary libraries for data generation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Define the temporal structure of the data ---\n",
        "# We generate 25 years of quarterly data (100 observations) to provide a\n",
        "# sufficient sample for training and observing cyclical dynamics.\n",
        "# The index uses quarter-end dates, as specified.\n",
        "date_rng = pd.date_range(start='1999-03-31', end='2023-12-31', freq='Q')\n",
        "n_obs = len(date_rng)\n",
        "\n",
        "# --- Generate plausible, dynamic time series for each variable ---\n",
        "# The goal is to create data with a clear \"normal\" period, a \"crisis\" event,\n",
        "# and a \"recovery\" to test the model's diagnostic capabilities.\n",
        "np.random.seed(123) # Set a seed for perfect reproducibility.\n",
        "\n",
        "# UNRATE: Civilian Unemployment Rate (%).\n",
        "# Represents: Labor market stress.\n",
        "# Behavior: Starts low, spikes sharply during a simulated crisis (quarters 35-50),\n",
        "# and then exhibits a slow, persistent recovery.\n",
        "unrate_trend = np.linspace(5.0, 4.0, n_obs)\n",
        "crisis_shock = np.zeros(n_obs)\n",
        "crisis_shock[35:50] = np.sin(np.linspace(0, np.pi, 15)) * 5.5 # Sharp spike to ~9.5%\n",
        "unrate = unrate_trend + crisis_shock + np.random.normal(0, 0.15, n_obs)\n",
        "\n",
        "# PCE: Personal Consumption Expenditures (Billions of $).\n",
        "# Represents: Aggregate demand and consumer confidence.\n",
        "# Behavior: Exhibits a strong secular uptrend but experiences a sharp V-shaped\n",
        "# dip during the crisis, reflecting a collapse and subsequent rebound in spending.\n",
        "pce_trend = 12000 * np.exp(np.linspace(0, 0.025 * 25, n_obs)) # ~2.5% real annual growth\n",
        "pce_crisis_dip = np.zeros(n_obs)\n",
        "pce_crisis_dip[37:47] = -800 * np.sin(np.linspace(0, np.pi, 10)) # A sharp dip\n",
        "pce = pce_trend + pce_crisis_dip + np.random.normal(0, 80, n_obs)\n",
        "\n",
        "# PSAVERT: Personal Saving Rate (%).\n",
        "# Represents: Household precautionary behavior.\n",
        "# Behavior: Counter-cyclical. It is relatively low during normal times but\n",
        "# spikes dramatically during the crisis as households cut spending and hoard cash.\n",
        "psavert_base = np.linspace(7.0, 5.0, n_obs)\n",
        "psavert_crisis_spike = np.zeros(n_obs)\n",
        "psavert_crisis_spike[36:51] = np.sin(np.linspace(0, np.pi, 15)) * 8.0 # Precautionary savings spike\n",
        "psavert = psavert_base + psavert_crisis_spike + np.random.normal(0, 0.3, n_obs)\n",
        "\n",
        "# REVOLSL: Revolving Consumer Credit Outstanding (Billions of $).\n",
        "# Represents: Household leverage and access to credit.\n",
        "# Behavior: Grows steadily pre-crisis, then stagnates and declines post-crisis,\n",
        "# representing a period of household deleveraging.\n",
        "revols_trend = 900 * np.exp(np.linspace(0, 0.03 * 25, n_obs))\n",
        "revols_crisis_deleveraging = np.zeros(n_obs)\n",
        "revols_crisis_deleveraging[40:60] = -150 # Post-crisis deleveraging\n",
        "revols = revols_trend + revols_crisis_deleveraging + np.random.normal(0, 25, n_obs)\n",
        "\n",
        "# CORCACBS: Charge-Off Rate on Consumer Loans (%).\n",
        "# Represents: The target variable; realized credit losses in the banking system.\n",
        "# Behavior: This is a lagging indicator. It remains low and then spikes\n",
        "# significantly *after* the peak in unemployment, reflecting the time it takes\n",
        "# for defaults to materialize.\n",
        "corcacbs_base = np.linspace(1.8, 2.2, n_obs)\n",
        "corcacbs_crisis_spike = np.zeros(n_obs)\n",
        "# The spike lags the unemployment peak by several quarters.\n",
        "corcacbs_crisis_spike[42:57] = np.sin(np.linspace(0, np.pi, 15)) * 5.0 # Spike to ~7%\n",
        "corcacbs = corcacbs_base + corcacbs_crisis_spike + np.random.normal(0, 0.2, n_obs)\n",
        "\n",
        "# --- Assemble the final DataFrame ---\n",
        "consolidated_df_raw = pd.DataFrame({\n",
        "    'UNRATE': unrate,\n",
        "    'PCE': pce,\n",
        "    'PSAVERT': psavert,\n",
        "    'REVOLSL': revols,\n",
        "    'CORCACBS': corcacbs\n",
        "}, index=date_rng)\n",
        "\n",
        "# Ensure all dtypes are float64, as required by the validation functions.\n",
        "consolidated_df_raw = consolidated_df_raw.astype('float64')\n",
        "\n",
        "print(\"--- Synthetic Raw Data (`consolidated_df_raw`) ---\")\n",
        "print(f\"Shape: {consolidated_df_raw.shape}\")\n",
        "print(\"Data Head:\")\n",
        "print(consolidated_df_raw.head())\n",
        "print(\"\\nData Tail:\")\n",
        "print(consolidated_df_raw.tail())\n",
        "```\n",
        "\n",
        "**1.B. Loading the Model Configuration (`model_config`)**\n",
        "\n",
        "We assume the `config.yaml` file, exists in the user's home directory. We will load this file into a Python dictionary. This separation of configuration from code is a critical best practice.\n",
        "\n",
        "*   **`model_config`**: This is a nested Python `dict` that contains every hyperparameter and structural definition for the entire pipeline. It is the control panel for the model.\n",
        "\n",
        "```python\n",
        "# Import necessary libraries for file handling and YAML parsing\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Define the path to the configuration file ---\n",
        "# We assume the 'config.yaml' file is located in the user's working directory.\n",
        "config_path = 'config.yaml'\n",
        "\n",
        "# --- Load the YAML file into a Python dictionary ---\n",
        "try:\n",
        "    with open(config_path, 'r') as f:\n",
        "        model_config = yaml.safe_load(f)\n",
        "    print(f\"\\n--- Model Configuration (`model_config`) ---\")\n",
        "    print(f\"Successfully loaded configuration from: {config_path}\")\n",
        "    # Print a sample parameter to verify successful loading.\n",
        "    print(f\"Example Parameter (learning_rate_eta): {model_config['training']['learning_rate_eta']}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Configuration file not found at {config_path}.\")\n",
        "    # In a real script, you would exit or raise an exception here.\n",
        "    model_config = {} # Assign empty dict to allow script to continue for demonstration\n",
        "```\n",
        "\n",
        "**1.C. Defining the Hyperparameter Grid (`hyperparameter_grid`)**\n",
        "\n",
        "This dictionary defines the scope of the robustness analysis. The keys are the names of parameters in `model_config`, and the values are lists of settings to test.\n",
        "\n",
        "*   **`hyperparameter_grid`**: A `dict` where keys are `str` and values are `list`. It specifies which experiments to run during the robustness check.\n",
        "\n",
        "```python\n",
        "# --- Define the hyperparameter grid for the robustness analysis ---\n",
        "# This grid specifies which parameters to vary and which values to test.\n",
        "# The `conduct_robustness_analysis` function will run the full pipeline for\n",
        "# every unique combination of these parameters.\n",
        "hyperparameter_grid = {\n",
        "    # Test two different sizes for the hidden dimension of the attention mechanism.\n",
        "    'hidden_dimension_dh': [32, 64],\n",
        "    # Test two different regularization strengths for the Q and K matrices.\n",
        "    'regularization_lambda_qk': [1e-3, 5e-4],\n",
        "    # Test two different learning rates for the optimizer.\n",
        "    'learning_rate_eta': [1e-4, 5e-5]\n",
        "}\n",
        "\n",
        "print(f\"\\n--- Hyperparameter Grid (`hyperparameter_grid`) ---\")\n",
        "print(\"The following grid will be tested for robustness:\")\n",
        "for key, values in hyperparameter_grid.items():\n",
        "    print(f\"- {key}: {values}\")\n",
        "# Total number of runs will be 2 * 2 * 2 = 8.\n",
        "```\n",
        "\n",
        "#### **Stage 2: Pipeline Execution**\n",
        "\n",
        "With all inputs prepared, we make a single call to the master orchestrator. This function encapsulates the entire research process.\n",
        "\n",
        "*   **`save_directory`**: A `str` specifying the root folder where all outputs (plots, tables, archives) will be saved.\n",
        "*   **`base_random_seed`**: An `int` to ensure the entire experiment, including the random initializations in each run of the sweep, is fully reproducible.\n",
        "\n",
        "```python\n",
        "# --- Define final parameters for the master orchestrator ---\n",
        "save_directory = 'research_output_final'\n",
        "base_random_seed = 42\n",
        "\n",
        "# --- Execute the Full End-to-End Pipeline ---\n",
        "# This single function call will:\n",
        "# 1. Run the primary analysis with the base configuration from the YAML file.\n",
        "# 2. Generate and save all plots, tables, and archives for the primary run.\n",
        "# 3. Run the robustness analysis for all 8 hyperparameter combinations.\n",
        "# 4. Archive the full results of the robustness analysis.\n",
        "# We set `show_plots=False` for automated execution; plots are saved to files.\n",
        "if model_config: # Only run if the config was loaded successfully\n",
        "    final_results_package = execute_geometric_credit_cycle_research(\n",
        "        consolidated_df_raw=consolidated_df_raw,\n",
        "        model_config=model_config,\n",
        "        hyperparameter_grid=hyperparameter_grid,\n",
        "        save_dir=save_directory,\n",
        "        base_random_seed=base_random_seed,\n",
        "        run_robustness_analysis=True,\n",
        "        show_plots=False\n",
        "    )\n",
        "    print(\"\\nMaster orchestrator has finished execution.\")\n",
        "else:\n",
        "    print(\"\\nSkipping execution due to missing configuration file.\")\n",
        "\n",
        "```\n",
        "\n",
        "#### **Stage 3: Output Deconstruction**\n",
        "\n",
        "The `final_results_package` is a dictionary containing the complete output of the entire workflow.\n",
        "\n",
        "*   **`final_results_package['primary_run_results']`**: This is the comprehensive dictionary for the main analysis run. It contains everything needed to reproduce and analyze the baseline findings, including:\n",
        "    *   `...['model_artifacts']['trained_parameters']`: The final trained PyTorch model parameters.\n",
        "    *   `...['interpretability_results']['pca_trajectory_df']`: The pandas DataFrame ready for plotting the PCA trajectory.\n",
        "    *   `...['interpretability_results']['component_magnitudes_df']`: The DataFrame for plotting the evolution of geometric component influences.\n",
        "\n",
        "*   **`final_results_package['robustness_analysis_results']`**: This is a list containing 8 dictionaries, one for each run in the hyperparameter sweep. Each dictionary has the same structure as `primary_run_results` but is augmented with a `'hyperparameters'` key. This allows for detailed comparison of how changing hyperparameters affects model performance, training dynamics, and, most importantly, the qualitative conclusions drawn from the diagnostic analyses. For example, one could iterate through this list and check if the 2008 crisis remains \"Bivector-Dominant\" across all runs, thus verifying the robustness of the paper's central claim."
      ],
      "metadata": {
        "id": "1NwFAf8WSHXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 — Validate consolidated_df_raw structure, semantics, and data integrity\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate consolidated_df_raw structure, semantics, and data integrity\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Helper function for Index Validation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_dataframe_index(\n",
        "    df: pd.DataFrame\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates the structure and temporal alignment of a DataFrame's index.\n",
        "\n",
        "    This function performs a series of checks to ensure the index is a\n",
        "    well-formed, quarterly, and monotonically increasing DatetimeIndex, aligned\n",
        "    to quarter-end dates, as required for the economic time series model.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame):\n",
        "            The DataFrame whose index needs to be validated.\n",
        "\n",
        "    Returns:\n",
        "        bool:\n",
        "            Returns True if all index validation checks pass.\n",
        "\n",
        "    Raises:\n",
        "        ValueError:\n",
        "            If the DataFrame is empty.\n",
        "            If the index is not a pandas DatetimeIndex.\n",
        "            If the index frequency is not quarterly.\n",
        "            If the index is not monotonically increasing.\n",
        "            If any index timestamp is not a quarter-end date.\n",
        "    \"\"\"\n",
        "    # Check 1: Ensure the DataFrame is not empty.\n",
        "    # An empty DataFrame cannot be processed and indicates a data loading issue.\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Input DataFrame 'consolidated_df_raw' cannot be empty.\")\n",
        "\n",
        "    # Store the index for convenient access.\n",
        "    idx = df.index\n",
        "\n",
        "    # Check 2: Verify the index is a pandas DatetimeIndex.\n",
        "    # The model relies on time-based operations that require a DatetimeIndex.\n",
        "    if not isinstance(idx, pd.DatetimeIndex):\n",
        "        raise ValueError(\n",
        "            \"DataFrame index must be a pandas.DatetimeIndex, but found \"\n",
        "            f\"type '{type(idx).__name__}'.\"\n",
        "        )\n",
        "\n",
        "    # Check 3: Verify the frequency is quarterly.\n",
        "    # The model's logic is designed for quarterly data. We handle cases where\n",
        "    # the frequency attribute is not explicitly set by attempting to infer it.\n",
        "    freq = idx.freqstr or pd.infer_freq(idx)\n",
        "    if freq not in ('Q-DEC', 'Q-MAR', 'Q-JUN', 'Q-SEP', 'Q'):\n",
        "        raise ValueError(\n",
        "            f\"DataFrame index frequency must be quarterly. Inferred frequency is '{freq}'.\"\n",
        "        )\n",
        "\n",
        "    # Check 4: Verify the index is strictly monotonically increasing.\n",
        "    # This ensures proper chronological order and no duplicate timestamps, which is\n",
        "    # critical for causal modeling and rolling window operations.\n",
        "    if not idx.is_monotonic_increasing:\n",
        "        raise ValueError(\"DataFrame index must be monotonically increasing.\")\n",
        "\n",
        "    # Check 5: Verify all timestamps are aligned to quarter-end dates.\n",
        "    # This enforces a consistent temporal representation across all data series.\n",
        "    non_quarter_end_dates = [date for date in idx if not date.is_quarter_end]\n",
        "    if non_quarter_end_dates:\n",
        "        raise ValueError(\n",
        "            \"All index timestamps must be quarter-end dates. Found non-compliant \"\n",
        "            f\"date(s): {non_quarter_end_dates[:5]}\"\n",
        "        )\n",
        "\n",
        "    # If all checks pass, return True.\n",
        "    return True\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 2 & 3: Helper function for Column and Semantic Validation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_dataframe_columns_and_semantics(\n",
        "    df: pd.DataFrame\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates column presence, dtypes, and semantic integrity of the data.\n",
        "\n",
        "    This function checks for the required columns, verifies their data types,\n",
        "    and enforces domain-specific semantic constraints (e.g., positivity, valid\n",
        "    percentage ranges) on the economic variables.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame):\n",
        "            The DataFrame whose columns and data need validation.\n",
        "\n",
        "    Returns:\n",
        "        bool:\n",
        "            Returns True if all column and semantic checks pass.\n",
        "\n",
        "    Raises:\n",
        "        ValueError:\n",
        "            If the required columns are not present.\n",
        "            If any column contains NaN or infinite values.\n",
        "            If any data point violates its specified semantic bounds.\n",
        "        TypeError:\n",
        "            If any column has a non-numeric data type.\n",
        "    \"\"\"\n",
        "    # Define the exact set of required columns for the model.\n",
        "    required_columns: Set[str] = {\n",
        "        'UNRATE', 'PCE', 'PSAVERT', 'REVOLSL', 'CORCACBS'\n",
        "    }\n",
        "\n",
        "    # Check 1: Verify presence of exactly the required columns.\n",
        "    # Using sets ensures that no required columns are missing and no extra columns are present.\n",
        "    if set(df.columns) != required_columns:\n",
        "        missing = required_columns - set(df.columns)\n",
        "        extra = set(df.columns) - required_columns\n",
        "        error_msg = \"DataFrame column mismatch.\"\n",
        "        if missing:\n",
        "            error_msg += f\" Missing columns: {sorted(list(missing))}.\"\n",
        "        if extra:\n",
        "            error_msg += f\" Extra columns: {sorted(list(extra))}.\"\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # Check 2: Verify all columns have a numeric data type (specifically float64).\n",
        "    # Non-numeric data would cause failures in downstream mathematical operations.\n",
        "    non_float_cols = [\n",
        "        col for col in required_columns if df[col].dtype != 'float64'\n",
        "    ]\n",
        "    if non_float_cols:\n",
        "        raise TypeError(\n",
        "            f\"All columns must have dtype 'float64'. Found non-compliant \"\n",
        "            f\"columns: {non_float_cols} with dtypes \"\n",
        "            f\"{df[non_float_cols].dtypes.to_dict()}\"\n",
        "        )\n",
        "\n",
        "    # Check 3: Ensure there are no NaN or infinite values.\n",
        "    # These values are invalid for the model's transformations (e.g., log) and must be handled.\n",
        "    if df.isna().any().any():\n",
        "        nan_counts = df.isna().sum()\n",
        "        nan_cols = nan_counts[nan_counts > 0].to_dict()\n",
        "        raise ValueError(f\"DataFrame contains NaN values. Columns with NaNs: {nan_cols}\")\n",
        "    if np.isinf(df).any().any():\n",
        "        inf_counts = np.isinf(df).sum()\n",
        "        inf_cols = inf_counts[inf_counts > 0].to_dict()\n",
        "        raise ValueError(f\"DataFrame contains infinite values. Columns with Infs: {inf_cols}\")\n",
        "\n",
        "    # Check 4: Perform semantic validation based on economic logic.\n",
        "    # Each variable must lie within a plausible range.\n",
        "    semantic_checks = {\n",
        "        'UNRATE': (df['UNRATE'] < 0) | (df['UNRATE'] > 100),\n",
        "        'PSAVERT': (df['PSAVERT'] < -20) | (df['PSAVERT'] > 50),\n",
        "        'PCE': df['PCE'] <= 0,\n",
        "        'REVOLSL': df['REVOLSL'] <= 0,\n",
        "        'CORCACBS': df['CORCACBS'] < 0,\n",
        "    }\n",
        "\n",
        "    # Iterate through checks and raise an error on the first violation found.\n",
        "    for col, violation_mask in semantic_checks.items():\n",
        "        if violation_mask.any():\n",
        "            first_violation_idx = violation_mask.idxmax()\n",
        "            first_violation_val = df.loc[first_violation_idx, col]\n",
        "            raise ValueError(\n",
        "                f\"Semantic validation failed for column '{col}'. Found invalid \"\n",
        "                f\"value {first_violation_val} at index {first_violation_idx}.\"\n",
        "            )\n",
        "\n",
        "    # If all checks pass, return True.\n",
        "    return True\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_raw_data(\n",
        "    consolidated_df_raw: pd.DataFrame\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the raw input DataFrame.\n",
        "\n",
        "    This function serves as the main entry point for Task 1, sequentially\n",
        "    executing all validation steps for the DataFrame's index, columns, and\n",
        "    semantic content. It ensures the input data is structurally sound and\n",
        "    semantically valid before it enters the modeling pipeline.\n",
        "\n",
        "    Args:\n",
        "        consolidated_df_raw (pd.DataFrame):\n",
        "            The raw, unprocessed quarterly economic data. It must have a\n",
        "            DatetimeIndex and columns for UNRATE, PCE, PSAVERT, REVOLSL,\n",
        "            and CORCACBS.\n",
        "\n",
        "    Returns:\n",
        "        bool:\n",
        "            Returns True if the DataFrame passes all validation checks.\n",
        "\n",
        "    Raises:\n",
        "        ValueError, TypeError:\n",
        "            Propagates exceptions from helper validation functions if any\n",
        "            check fails, providing a detailed error message.\n",
        "    \"\"\"\n",
        "    # Sequentially call the validation helper functions.\n",
        "    # Each function will raise an exception if a check fails, halting the process.\n",
        "\n",
        "    # Step 1: Validate the index structure and temporal properties.\n",
        "    _validate_dataframe_index(consolidated_df_raw)\n",
        "\n",
        "    # Step 2 & 3: Validate column presence, dtypes, and semantic data integrity.\n",
        "    _validate_dataframe_columns_and_semantics(consolidated_df_raw)\n",
        "\n",
        "    # If all validations pass without raising an exception, the data is valid.\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "QLUmIvmjji3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 — Validate model_config completeness and internal consistency\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate model_config completeness and internal consistency\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Helper function for 'data_processing' Section Validation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_data_processing_config(config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Validates the 'data_processing' section of the model configuration.\n",
        "\n",
        "    This function ensures that all required keys are present, have the correct\n",
        "    data types, and adhere to the specific values and constraints outlined in\n",
        "    the research protocol. It validates parameters related to data frequency,\n",
        "    transformations, and windowing operations.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]):\n",
        "            The 'data_processing' sub-dictionary from the main model_config.\n",
        "\n",
        "    Returns:\n",
        "        bool:\n",
        "            Returns True if the configuration section is valid.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If a required key is missing from the configuration.\n",
        "        TypeError: If a parameter has an incorrect data type.\n",
        "        ValueError: If a parameter's value is outside its allowed range or set.\n",
        "    \"\"\"\n",
        "    # Define a schema for required keys, their types, and expected values/ranges.\n",
        "    # This schema-driven approach makes validation systematic and maintainable.\n",
        "    schema = {\n",
        "        'frequency': (str, {'Quarterly'}),\n",
        "        'index_alignment': (str, {'QuarterEnd'}),\n",
        "        'lookback_horizon_L': (int, 8),\n",
        "        'standardization_window': (int, 8),\n",
        "        'standardization_epsilon': (float, (0, float('inf'))),\n",
        "        'mask_warmup_in_loss': (bool, {True}),\n",
        "        'warmup_min_observations': (int, 8),\n",
        "        'monthly_to_quarterly_aggregation': (dict, {\n",
        "            'UNRATE': 'QuarterlyAverage',\n",
        "            'PSAVERT': 'QuarterlyAverage',\n",
        "            'PCE': 'QuarterlyAverage',\n",
        "            'REVOLSL': 'QuarterEnd'\n",
        "        }),\n",
        "        'transformations': (dict, {\n",
        "            'UNRATE': 'level',\n",
        "            'PSAVERT': 'level',\n",
        "            'PCE': 'log_diff',\n",
        "            'REVOLSL': 'log_diff'\n",
        "        }),\n",
        "        'causal_indexing': (bool, {True})\n",
        "    }\n",
        "\n",
        "    # Iterate through the schema to validate each parameter.\n",
        "    for key, (expected_type, expected_value) in schema.items():\n",
        "        # Check 1: Key presence.\n",
        "        if key not in config:\n",
        "            raise KeyError(f\"Missing required key in 'data_processing': '{key}'.\")\n",
        "\n",
        "        value = config[key]\n",
        "\n",
        "        # Check 2: Type correctness.\n",
        "        if not isinstance(value, expected_type):\n",
        "            raise TypeError(\n",
        "                f\"Invalid type for 'data_processing.{key}'. Expected \"\n",
        "                f\"{expected_type.__name__}, but got {type(value).__name__}.\"\n",
        "            )\n",
        "\n",
        "        # Check 3: Value correctness (exact match, set membership, or range).\n",
        "        if isinstance(expected_value, set):\n",
        "            if value not in expected_value:\n",
        "                raise ValueError(\n",
        "                    f\"Invalid value for 'data_processing.{key}'. Expected one of \"\n",
        "                    f\"{expected_value}, but got '{value}'.\"\n",
        "                )\n",
        "        elif isinstance(expected_value, tuple): # Range check\n",
        "             if not (expected_value[0] < value < expected_value[1]):\n",
        "                 raise ValueError(\n",
        "                    f\"Value for 'data_processing.{key}' is out of range. \"\n",
        "                    f\"Expected value in ({expected_value[0]}, {expected_value[1]}), but got {value}.\"\n",
        "                 )\n",
        "        elif isinstance(expected_value, dict):\n",
        "            if value != expected_value:\n",
        "                 raise ValueError(\n",
        "                    f\"Invalid dictionary content for 'data_processing.{key}'. \"\n",
        "                    f\"Mismatch found. Expected {expected_value}, got {value}.\"\n",
        "                 )\n",
        "        elif value != expected_value:\n",
        "            raise ValueError(\n",
        "                f\"Invalid value for 'data_processing.{key}'. Expected \"\n",
        "                f\"{expected_value}, but got {value}.\"\n",
        "            )\n",
        "\n",
        "    # Post-validation consistency check.\n",
        "    if config['lookback_horizon_L'] != config['standardization_window']:\n",
        "        raise ValueError(\n",
        "            \"Consistency check failed: 'lookback_horizon_L' must be equal to \"\n",
        "            \"'standardization_window'.\"\n",
        "        )\n",
        "\n",
        "    return True\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Helper function for 'architecture' Section Validation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_architecture_config(config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Validates the 'architecture' section of the model configuration.\n",
        "\n",
        "    This function verifies parameters defining the geometric algebra embedding\n",
        "    and the linear attention mechanism. It pays special attention to the exact\n",
        "    ordering and content of semantic lists like 'component_order' and 'pi_order'.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]):\n",
        "            The 'architecture' sub-dictionary from the main model_config.\n",
        "\n",
        "    Returns:\n",
        "        bool:\n",
        "            Returns True if the configuration section is valid.\n",
        "\n",
        "    Raises:\n",
        "        KeyError, TypeError, ValueError for schema violations.\n",
        "    \"\"\"\n",
        "    # Define canonical reference structures for critical ordered lists.\n",
        "    # This ensures absolute fidelity to the model's geometric basis.\n",
        "    canonical_component_order = [\n",
        "        'scalar', 'e1(u)', 'e2(s)', 'e3(r)', 'e4(v)', 'e1^e2(u^s)',\n",
        "        'e1^e3(u^r)', 'e1^e4(u^v)', 'e2^e3(s^r)', 'e2^e4(s^v)', 'e3^e4(r^v)'\n",
        "    ]\n",
        "    canonical_pi_order = [\n",
        "        (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)\n",
        "    ]\n",
        "\n",
        "    # Define the validation schema.\n",
        "    schema = {\n",
        "        'multivector_dimension_dm': (int, 16),\n",
        "        'active_multivector_components': (int, 11),\n",
        "        'grades_active': (list, [0, 1, 2]),\n",
        "        'grades_inactive': (list, [3, 4]),\n",
        "        'component_order': (list, canonical_component_order),\n",
        "        'pi_order': (list, canonical_pi_order),\n",
        "        'hidden_dimension_dh': (int, (0, float('inf'))),\n",
        "        'leaky_relu_alpha': (float, (0, 1)),\n",
        "        'attention_stability_epsilon': (float, (0, float('inf'))),\n",
        "        'activation_shift_enabled': (bool, {True})\n",
        "    }\n",
        "\n",
        "    # Perform schema validation.\n",
        "    for key, (expected_type, expected_value) in schema.items():\n",
        "        if key not in config:\n",
        "            raise KeyError(f\"Missing required key in 'architecture': '{key}'.\")\n",
        "        value = config[key]\n",
        "        if not isinstance(value, expected_type):\n",
        "            raise TypeError(\n",
        "                f\"Invalid type for 'architecture.{key}'. Expected \"\n",
        "                f\"{expected_type.__name__}, but got {type(value).__name__}.\"\n",
        "            )\n",
        "        if isinstance(expected_value, set):\n",
        "            if value not in expected_value:\n",
        "                raise ValueError(\n",
        "                    f\"Invalid value for 'architecture.{key}'. Expected one of \"\n",
        "                    f\"{expected_value}, but got '{value}'.\"\n",
        "                )\n",
        "        elif isinstance(expected_value, tuple): # Range check\n",
        "             if not (expected_value[0] < value < expected_value[1]):\n",
        "                 raise ValueError(\n",
        "                    f\"Value for 'architecture.{key}' is out of range. \"\n",
        "                    f\"Expected value in ({expected_value[0]}, {expected_value[1]}), but got {value}.\"\n",
        "                 )\n",
        "        elif value != expected_value:\n",
        "            # This handles exact matches for numbers, lists, and tuples.\n",
        "            raise ValueError(\n",
        "                f\"Invalid value for 'architecture.{key}'. Expected \"\n",
        "                f\"{expected_value}, but got {value}.\"\n",
        "            )\n",
        "\n",
        "    # Perform post-validation consistency checks.\n",
        "    if len(config['component_order']) != config['active_multivector_components']:\n",
        "        raise ValueError(\n",
        "            \"Consistency check failed: length of 'component_order' must equal \"\n",
        "            \"'active_multivector_components'.\"\n",
        "        )\n",
        "\n",
        "    num_bivectors = sum(1 for item in config['component_order'] if '^' in item)\n",
        "    if len(config['pi_order']) != num_bivectors:\n",
        "        raise ValueError(\n",
        "            \"Consistency check failed: length of 'pi_order' must equal the \"\n",
        "            \"number of bivector components in 'component_order'.\"\n",
        "        )\n",
        "\n",
        "    return True\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Helper function for 'prediction_head' & 'training' Validation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_training_and_head_config(\n",
        "    head_config: Dict[str, Any],\n",
        "    train_config: Dict[str, Any]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates the 'prediction_head' and 'training' sections of the config.\n",
        "\n",
        "    This function validates hyperparameters for the MLP head, the optimizer,\n",
        "    loss function, regularization, and training loop control flow, including\n",
        "    the nested 'early_stopping' configuration.\n",
        "\n",
        "    Args:\n",
        "        head_config (Dict[str, Any]):\n",
        "            The 'prediction_head' sub-dictionary.\n",
        "        train_config (Dict[str, Any]):\n",
        "            The 'training' sub-dictionary.\n",
        "\n",
        "    Returns:\n",
        "        bool:\n",
        "            Returns True if both configuration sections are valid.\n",
        "\n",
        "    Raises:\n",
        "        KeyError, TypeError, ValueError for schema violations.\n",
        "    \"\"\"\n",
        "    # Schema for 'prediction_head'.\n",
        "    head_schema = {\n",
        "        'head_type': (str, {'MLP'}),\n",
        "        'mlp_hidden_dimension': (int, (0, float('inf'))),\n",
        "        'mlp_activation_function': (str, {'ReLU'}),\n",
        "        'use_bias': (bool, {True, False}),\n",
        "        'dropout': (float, 0.0),\n",
        "        'layer_norm': (bool, {False})\n",
        "    }\n",
        "\n",
        "    # Schema for 'training'.\n",
        "    train_schema = {\n",
        "        'optimizer': (str, {'Adam'}),\n",
        "        'learning_rate_eta': (float, (0, float('inf'))),\n",
        "        'adam_betas': (tuple, ((0, 1), (0, 1))), # Special handling below\n",
        "        'weight_decay': (float, 0.0),\n",
        "        'loss_function': (str, {'MeanSquaredError'}),\n",
        "        'regularization_lambda_qk': (float, (0, float('inf'))),\n",
        "        'regularization_lambda_v': (float, (0, float('inf'))),\n",
        "        'num_epochs': (int, (0, float('inf'))),\n",
        "        'batch_size': (int, 1),\n",
        "        'sequence_batching_policy': (str, {'Chronological'}),\n",
        "        'compute_SZ_per_time_step': (bool, {True}),\n",
        "        'mask_warmup_in_loss': (bool, {True}),\n",
        "        'early_stopping': (dict, None) # Special handling below\n",
        "    }\n",
        "\n",
        "    # Validate 'prediction_head'\n",
        "    for key, (expected_type, expected_value) in head_schema.items():\n",
        "        if key not in head_config:\n",
        "            raise KeyError(f\"Missing required key in 'prediction_head': '{key}'.\")\n",
        "        value = head_config[key]\n",
        "        if not isinstance(value, expected_type):\n",
        "            raise TypeError(\n",
        "                f\"Invalid type for 'prediction_head.{key}'. Expected \"\n",
        "                f\"{expected_type.__name__}, but got {type(value).__name__}.\"\n",
        "            )\n",
        "        if isinstance(expected_value, set):\n",
        "            if value not in expected_value:\n",
        "                raise ValueError(\n",
        "                    f\"Invalid value for 'prediction_head.{key}'. Expected one of \"\n",
        "                    f\"{expected_value}, but got '{value}'.\"\n",
        "                )\n",
        "        elif isinstance(expected_value, tuple): # Range check\n",
        "             if not (expected_value[0] < value < expected_value[1]):\n",
        "                 raise ValueError(\n",
        "                    f\"Value for 'prediction_head.{key}' is out of range. \"\n",
        "                    f\"Expected value in ({expected_value[0]}, {expected_value[1]}), but got {value}.\"\n",
        "                 )\n",
        "        elif value != expected_value:\n",
        "            raise ValueError(\n",
        "                f\"Invalid value for 'prediction_head.{key}'. Expected \"\n",
        "                f\"{expected_value}, but got {value}.\"\n",
        "            )\n",
        "\n",
        "    # Validate 'training'\n",
        "    for key, (expected_type, expected_value) in train_schema.items():\n",
        "        if key not in train_config:\n",
        "            raise KeyError(f\"Missing required key in 'training': '{key}'.\")\n",
        "        value = train_config[key]\n",
        "        if not isinstance(value, expected_type):\n",
        "            raise TypeError(\n",
        "                f\"Invalid type for 'training.{key}'. Expected \"\n",
        "                f\"{expected_type.__name__}, but got {type(value).__name__}.\"\n",
        "            )\n",
        "        # Special handling for complex types\n",
        "        if key == 'adam_betas':\n",
        "            if len(value) != 2 or not all(isinstance(v, float) and 0 < v < 1 for v in value):\n",
        "                raise ValueError(\n",
        "                    \"Invalid value for 'training.adam_betas'. Expected a tuple of \"\n",
        "                    \"two floats between 0 and 1.\"\n",
        "                )\n",
        "        elif key == 'early_stopping':\n",
        "            if not all(k in value for k in ['enabled', 'patience', 'monitor']):\n",
        "                raise KeyError(\"'early_stopping' dict is missing required keys.\")\n",
        "            if not isinstance(value['enabled'], bool) or not isinstance(value['patience'], int) or not isinstance(value['monitor'], str):\n",
        "                 raise TypeError(\"Invalid type in 'early_stopping' values.\")\n",
        "        elif isinstance(expected_value, set):\n",
        "            if value not in expected_value:\n",
        "                raise ValueError(\n",
        "                    f\"Invalid value for 'training.{key}'. Expected one of \"\n",
        "                    f\"{expected_value}, but got '{value}'.\"\n",
        "                )\n",
        "        elif isinstance(expected_value, tuple): # Range check\n",
        "             if not (expected_value[0] < value < expected_value[1]):\n",
        "                 raise ValueError(\n",
        "                    f\"Value for 'training.{key}' is out of range. \"\n",
        "                    f\"Expected value in ({expected_value[0]}, {expected_value[1]}), but got {value}.\"\n",
        "                 )\n",
        "        elif value != expected_value:\n",
        "            raise ValueError(\n",
        "                f\"Invalid value for 'training.{key}'. Expected \"\n",
        "                f\"{expected_value}, but got {value}.\"\n",
        "            )\n",
        "\n",
        "    return True\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_model_config(\n",
        "    model_config: Dict[str, Any]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the entire model configuration dictionary.\n",
        "\n",
        "    This function provides a single entry point for validating the model's\n",
        "    hyperparameters and structural definitions. It sequentially validates each\n",
        "    major section of the configuration ('data_processing', 'architecture',\n",
        "    'prediction_head', 'training') against a rigorous schema derived from the\n",
        "    research paper's specifications.\n",
        "\n",
        "    Args:\n",
        "        model_config (Dict[str, Any]):\n",
        "            The complete model configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        bool:\n",
        "            Returns True if the entire configuration is valid.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If a required top-level or nested key is missing.\n",
        "        TypeError: If any parameter has an incorrect data type.\n",
        "        ValueError: If any parameter's value is invalid or if consistency\n",
        "                    checks between parameters fail.\n",
        "    \"\"\"\n",
        "    # Define the required top-level sections of the configuration.\n",
        "    required_top_level_keys = {\n",
        "        'data_processing', 'architecture', 'prediction_head', 'training'\n",
        "    }\n",
        "\n",
        "    # Check for presence of all top-level keys.\n",
        "    if not required_top_level_keys.issubset(model_config.keys()):\n",
        "        missing_keys = required_top_level_keys - set(model_config.keys())\n",
        "        raise KeyError(f\"Missing required top-level keys in model_config: {missing_keys}\")\n",
        "\n",
        "    # Sequentially validate each major section of the configuration.\n",
        "    # Each helper function will raise a detailed exception upon failure.\n",
        "\n",
        "    # Step 1: Validate the 'data_processing' section.\n",
        "    _validate_data_processing_config(model_config['data_processing'])\n",
        "\n",
        "    # Step 2: Validate the 'architecture' section.\n",
        "    _validate_architecture_config(model_config['architecture'])\n",
        "\n",
        "    # Step 3: Validate the 'prediction_head' and 'training' sections.\n",
        "    _validate_training_and_head_config(\n",
        "        model_config['prediction_head'],\n",
        "        model_config['training']\n",
        "    )\n",
        "\n",
        "    # If all validations complete without error, the configuration is valid.\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "i_5DZaMYkJg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 — Cleanse and finalize consolidated_df_raw for modeling\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Cleanse and finalize consolidated_df_raw for modeling\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 1 & 2: Helper for Structural and Positivity Cleansing\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _cleanse_dataframe_structure_and_values(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Enforces structural integrity and semantic constraints on the DataFrame.\n",
        "\n",
        "    This function performs a multi-stage cleansing process:\n",
        "    1. Sorts the DataFrame by its DatetimeIndex.\n",
        "    2. Removes any rows with duplicate timestamps.\n",
        "    3. Removes any rows containing NaN or infinite values in key columns.\n",
        "    4. Enforces positivity constraints on series requiring log transformation.\n",
        "    5. Verifies that the final cleansed data has no temporal gaps.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame):\n",
        "            The raw input DataFrame, assumed to have passed initial validation.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            - A new DataFrame, cleansed and structurally sound.\n",
        "            - A dictionary reporting the number of rows removed at each stage.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the cleansing process results in a time series with\n",
        "                    gaps in its quarterly frequency.\n",
        "    \"\"\"\n",
        "    # Create a deep copy to avoid any side effects on the original DataFrame.\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Initialize a report to track the cleansing process for auditability.\n",
        "    report = {\n",
        "        'initial_rows': len(df_clean),\n",
        "        'rows_dropped_duplicates': 0,\n",
        "        'rows_dropped_nan_inf': 0,\n",
        "        'rows_dropped_non_positive': 0,\n",
        "    }\n",
        "\n",
        "    # --- Step 1: Enforce structural integrity ---\n",
        "\n",
        "    # Sort by index to ensure chronological order.\n",
        "    df_clean.sort_index(inplace=True)\n",
        "\n",
        "    # Remove duplicate index entries, keeping the first occurrence.\n",
        "    duplicate_mask = df_clean.index.duplicated(keep='first')\n",
        "    num_duplicates = duplicate_mask.sum()\n",
        "    if num_duplicates > 0:\n",
        "        df_clean = df_clean[~duplicate_mask]\n",
        "        report['rows_dropped_duplicates'] = num_duplicates\n",
        "\n",
        "    # Remove rows with any NaN or infinite values in the essential columns.\n",
        "    initial_rows_before_nan = len(df_clean)\n",
        "    essential_cols = ['UNRATE', 'PCE', 'PSAVERT', 'REVOLSL', 'CORCACBS']\n",
        "    df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df_clean.dropna(subset=essential_cols, inplace=True)\n",
        "    report['rows_dropped_nan_inf'] = initial_rows_before_nan - len(df_clean)\n",
        "\n",
        "    # --- Step 2: Enforce positivity constraints ---\n",
        "\n",
        "    # Identify rows where log-transformable series are not strictly positive.\n",
        "    initial_rows_before_pos = len(df_clean)\n",
        "    positivity_mask = (df_clean['PCE'] > 0) & (df_clean['REVOLSL'] > 0)\n",
        "    df_clean = df_clean[positivity_mask]\n",
        "    report['rows_dropped_non_positive'] = initial_rows_before_pos - len(df_clean)\n",
        "\n",
        "    # --- Final Integrity Check: Temporal Continuity ---\n",
        "\n",
        "    # Verify that the cleansed data forms a continuous quarterly sequence.\n",
        "    if not df_clean.empty:\n",
        "        expected_freq = df_clean.index.freqstr or pd.infer_freq(df_clean.index)\n",
        "        expected_index = pd.date_range(\n",
        "            start=df_clean.index.min(),\n",
        "            end=df_clean.index.max(),\n",
        "            freq=expected_freq\n",
        "        )\n",
        "        if not df_clean.index.equals(expected_index):\n",
        "            missing_dates = expected_index.difference(df_clean.index).tolist()\n",
        "            raise ValueError(\n",
        "                \"Cleansing process created temporal gaps in the time series. \"\n",
        "                f\"Missing quarters detected: {missing_dates[:5]}\"\n",
        "            )\n",
        "\n",
        "    report['final_rows'] = len(df_clean)\n",
        "    return df_clean, report\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Helper function to Create the Validity Mask\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _create_validity_mask(\n",
        "    df_clean: pd.DataFrame,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Creates a boolean mask for valid model samples.\n",
        "\n",
        "    This function generates a boolean mask that identifies time steps with\n",
        "    sufficient historical data for all rolling window and lookback operations.\n",
        "    It corrects a previous off-by-one error to align perfectly with the\n",
        "    paper's specification.\n",
        "\n",
        "    The rule, derived from the paper's config, is to exclude time steps `t`\n",
        "    where `t < warmup_min_observations`. For a `warmup_min_observations` of 8,\n",
        "    this means indices 0 through 7 are marked as invalid (False).\n",
        "\n",
        "    Args:\n",
        "        df_clean (pd.DataFrame):\n",
        "            The cleansed DataFrame with a continuous, quarterly DatetimeIndex.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The model configuration dictionary, used to retrieve the\n",
        "            'warmup_min_observations' parameter.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray:\n",
        "            A 1D boolean numpy array of the same length as `df_clean`, where\n",
        "            `True` indicates a valid observation for modeling.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df_clean, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df_clean' must be a pandas DataFrame.\")\n",
        "    if not isinstance(model_config, dict):\n",
        "        raise TypeError(\"Input 'model_config' must be a dictionary.\")\n",
        "\n",
        "    # Retrieve the required number of historical observations for a sample to be valid.\n",
        "    # This value (W) defines the length of the initial warm-up period.\n",
        "    try:\n",
        "        warmup_period = model_config['data_processing']['warmup_min_observations']\n",
        "    except KeyError as e:\n",
        "        raise KeyError(f\"Could not find required key in model_config: {e}\")\n",
        "\n",
        "    # Get the total number of observations in the cleansed dataset.\n",
        "    n_obs = len(df_clean)\n",
        "\n",
        "    # Handle the edge case where the dataset is shorter than the warm-up period.\n",
        "    # In this scenario, no observations are valid.\n",
        "    if n_obs < warmup_period:\n",
        "        return np.zeros(n_obs, dtype=bool)\n",
        "\n",
        "    # Initialize the mask to all True, assuming all samples are valid initially.\n",
        "    valid_mask = np.ones(n_obs, dtype=bool)\n",
        "\n",
        "    # Apply the correct masking logic.\n",
        "    # The paper's rule `t < W` means indices from 0 up to (but not including) W\n",
        "    # are invalid. In Python slicing, this corresponds to `[:W]`.\n",
        "    # For W=8, this correctly masks indices 0, 1, 2, 3, 4, 5, 6, 7.\n",
        "    valid_mask[:warmup_period] = False\n",
        "\n",
        "    return valid_mask\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_and_prepare_data(\n",
        "    consolidated_df_raw: pd.DataFrame,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, np.ndarray, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the cleansing of raw data and creation of a validity mask.\n",
        "\n",
        "    This function executes the full data cleansing pipeline for Task 3. It\n",
        "    takes the raw DataFrame, enforces structural and semantic integrity,\n",
        "    and then generates a boolean mask that identifies which time steps have\n",
        "    sufficient historical data for use in rolling-window calculations and\n",
        "    model training.\n",
        "\n",
        "    Args:\n",
        "        consolidated_df_raw (pd.DataFrame):\n",
        "            The raw, unprocessed quarterly economic data.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, np.ndarray, Dict[str, Any]]:\n",
        "            - `consolidated_df_clean` (pd.DataFrame): The cleansed DataFrame,\n",
        "              ready for feature transformation. It is guaranteed to have a\n",
        "              continuous quarterly index.\n",
        "            - `valid_mask` (np.ndarray): A boolean array aligned with the\n",
        "              cleansed DataFrame, indicating which rows are valid for modeling.\n",
        "            - `cleaning_report` (Dict[str, Any]): A dictionary detailing the\n",
        "              number of rows removed at each stage of the cleansing process.\n",
        "    \"\"\"\n",
        "    # Step 1 & 2: Enforce structural integrity (sorting, duplicates, NaN/inf)\n",
        "    # and positivity constraints. This returns a clean DataFrame and a report.\n",
        "    df_clean, cleaning_report = _cleanse_dataframe_structure_and_values(\n",
        "        consolidated_df_raw\n",
        "    )\n",
        "\n",
        "    # Step 3: Create the validity mask based on the warm-up period defined\n",
        "    # in the model configuration. This mask identifies samples with enough\n",
        "    # historical data for rolling window features.\n",
        "    valid_mask = _create_validity_mask(df_clean, model_config)\n",
        "\n",
        "    # Add final mask information to the report for a complete audit trail.\n",
        "    cleaning_report['valid_observations'] = int(valid_mask.sum())\n",
        "    cleaning_report['warmup_observations_masked'] = len(valid_mask) - int(valid_mask.sum())\n",
        "\n",
        "    return df_clean, valid_mask, cleaning_report\n"
      ],
      "metadata": {
        "id": "15KiFIDrlKXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 — Apply growth transformations (log differences) to PCE and REVOLSL\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Apply growth transformations (log differences) to PCE and REVOLSL\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 1 & 2: Helper function to calculate log differences\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _calculate_log_difference(\n",
        "    series: pd.Series\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Calculates the first difference of the natural logarithm of a series.\n",
        "\n",
        "    This function implements the transformation r_t = Δ log(X_t) = log(X_t) - log(X_{t-1}),\n",
        "    which is used to convert level series like PCE and REVOLSL into growth rates.\n",
        "    The first element of the resulting series will be NaN, as the lagged value\n",
        "    is undefined.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series):\n",
        "            The input time series of positive values (e.g., PCE or REVOLSL).\n",
        "            It is assumed that the series has already been validated to be > 0.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series:\n",
        "            A new series of the same length and index representing the log\n",
        "            growth rate. The first value is NaN.\n",
        "    \"\"\"\n",
        "    # Ensure the input is a pandas Series.\n",
        "    if not isinstance(series, pd.Series):\n",
        "        raise TypeError(\"Input 'series' must be a pandas.Series.\")\n",
        "\n",
        "    # Calculate the log difference: log(X_t) - log(X_{t-1}).\n",
        "    # np.log is applied element-wise, then .diff() computes the difference.\n",
        "    # This is a highly efficient and numerically stable method in pandas.\n",
        "    log_diff_series = np.log(series).diff(periods=1)\n",
        "\n",
        "    return log_diff_series\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def apply_growth_transformations(\n",
        "    consolidated_df_clean: pd.DataFrame,\n",
        "    valid_mask: np.ndarray\n",
        ") -> Tuple[pd.DataFrame, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Orchestrates the transformation of level series into model input features.\n",
        "\n",
        "    This function takes the cleansed data, applies the specified growth\n",
        "    transformations (log differences) to the appropriate columns, prepares the\n",
        "    level series, and assembles the final pre-standardized feature DataFrame.\n",
        "    It also updates the validity mask to account for the loss of the first\n",
        "    observation due to differencing.\n",
        "\n",
        "    The final feature DataFrame will have columns ['u', 's', 'r', 'v'] corresponding\n",
        "    to unemployment, savings, consumption growth, and credit growth, respectively.\n",
        "\n",
        "    Args:\n",
        "        consolidated_df_clean (pd.DataFrame):\n",
        "            The cleansed DataFrame from the previous task, with a continuous\n",
        "            quarterly index and no missing/invalid values.\n",
        "        valid_mask (np.ndarray):\n",
        "            The boolean validity mask corresponding to `consolidated_df_clean`.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, np.ndarray]:\n",
        "            - `features_df` (pd.DataFrame): A DataFrame containing the four\n",
        "              pre-standardized model features with columns ['u', 's', 'r', 'v'].\n",
        "              The first row will have NaN for 'r' and 'v'.\n",
        "            - `updated_valid_mask` (np.ndarray): The validity mask, updated to\n",
        "              mark the first observation as False.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify that the input DataFrame is not empty.\n",
        "    if consolidated_df_clean.empty:\n",
        "        raise ValueError(\"Input 'consolidated_df_clean' cannot be empty.\")\n",
        "\n",
        "    # Verify alignment between the DataFrame and the mask.\n",
        "    if len(consolidated_df_clean) != len(valid_mask):\n",
        "        raise ValueError(\n",
        "            \"Length of 'consolidated_df_clean' and 'valid_mask' must be identical. \"\n",
        "            f\"Got {len(consolidated_df_clean)} and {len(valid_mask)}.\"\n",
        "        )\n",
        "\n",
        "    # Create a deep copy of the mask to avoid modifying the original array.\n",
        "    updated_valid_mask = valid_mask.copy()\n",
        "\n",
        "    # --- Step 1: Compute consumption growth r_t ---\n",
        "    # Equation: r_t = Δ log(PCE_t)\n",
        "    r_t = _calculate_log_difference(consolidated_df_clean['PCE'])\n",
        "    r_t.name = 'r'\n",
        "\n",
        "    # --- Step 2: Compute revolving credit growth v_t ---\n",
        "    # Equation: v_t = Δ log(REVOLSL_t)\n",
        "    v_t = _calculate_log_difference(consolidated_df_clean['REVOLSL'])\n",
        "    v_t.name = 'v'\n",
        "\n",
        "    # --- Step 3: Prepare level series for unemployment and savings ---\n",
        "    # u_t is the unemployment rate level.\n",
        "    u_t = consolidated_df_clean['UNRATE'].copy()\n",
        "    u_t.name = 'u'\n",
        "\n",
        "    # s_t is the personal savings rate level.\n",
        "    s_t = consolidated_df_clean['PSAVERT'].copy()\n",
        "    s_t.name = 's'\n",
        "\n",
        "    # --- Assemble the final feature DataFrame ---\n",
        "    # Concatenate the four series into a single DataFrame.\n",
        "    # The column order ['u', 's', 'r', 'v'] is critical as it maps to the\n",
        "    # geometric algebra basis vectors [e1, e2, e3, e4].\n",
        "    features_df = pd.concat([u_t, s_t, r_t, v_t], axis=1)\n",
        "\n",
        "    # --- Update Validity Mask ---\n",
        "    # The first observation is now invalid for all modeling purposes because\n",
        "    # the growth rates (r_t, v_t) are undefined (NaN).\n",
        "    if not features_df.empty:\n",
        "        updated_valid_mask[0] = False\n",
        "\n",
        "    return features_df, updated_valid_mask\n"
      ],
      "metadata": {
        "id": "ewk327__lylh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 — Apply rolling 8-quarter standardization with numerical stability\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Apply rolling 8-quarter standardization with numerical stability\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Helper to compute rolling statistics with stability floor\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_rolling_statistics(\n",
        "    features_df: pd.DataFrame,\n",
        "    window_size: int,\n",
        "    epsilon: float\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Computes rolling-window means and stabilized standard deviations.\n",
        "\n",
        "    This function calculates statistics over a specified rolling window, ensuring\n",
        "    causality by only using full windows. It also applies a stability floor to\n",
        "    the standard deviation to prevent division by zero in subsequent steps.\n",
        "\n",
        "    Args:\n",
        "        features_df (pd.DataFrame):\n",
        "            DataFrame of pre-standardized features ('u', 's', 'r', 'v').\n",
        "        window_size (int):\n",
        "            The size of the rolling window (e.g., 8 quarters).\n",
        "        epsilon (float):\n",
        "            A small constant to be used as the minimum value for the\n",
        "            standard deviation, ensuring numerical stability.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "            - `rolling_means` (pd.DataFrame): DataFrame of rolling means.\n",
        "            - `rolling_stds_stabilized` (pd.DataFrame): DataFrame of rolling\n",
        "              standard deviations with the stability floor applied.\n",
        "    \"\"\"\n",
        "    # Define the rolling window object.\n",
        "    # `min_periods=window_size` is a critical parameter. It ensures that\n",
        "    # statistics are only computed for full windows, automatically producing\n",
        "    # NaNs for the initial warm-up period and preventing any look-ahead bias.\n",
        "    rolling_window = features_df.rolling(\n",
        "        window=window_size,\n",
        "        min_periods=window_size\n",
        "    )\n",
        "\n",
        "    # Equation: μ_{z,t} = (1/W) * Σ_{τ=t-W+1 to t} z_τ\n",
        "    # Compute the rolling mean for each feature column.\n",
        "    rolling_means = rolling_window.mean()\n",
        "\n",
        "    # Equation: σ_{z,t} = sqrt((1/(W-1)) * Σ_{τ=t-W+1 to t} (z_τ - μ_{z,t})^2)\n",
        "    # Compute the rolling standard deviation (pandas uses ddof=1 by default).\n",
        "    rolling_stds = rolling_window.std()\n",
        "\n",
        "    # Equation: \\tilde{σ}_{z,t} = max(σ_{z,t}, ε_std)\n",
        "    # Apply the stability floor to prevent division by zero or near-zero.\n",
        "    # The .clip() method is a vectorized way to enforce a minimum value.\n",
        "    rolling_stds_stabilized = rolling_stds.clip(lower=epsilon)\n",
        "\n",
        "    return rolling_means, rolling_stds_stabilized\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def apply_rolling_standardization(\n",
        "    features_df: pd.DataFrame,\n",
        "    valid_mask: np.ndarray,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Orchestrates the rolling-window standardization of model features.\n",
        "\n",
        "    This function applies a causal, rolling-window standardization to the\n",
        "    input features. It uses a numerically stable approach by flooring the\n",
        "    standard deviation. The final output is a DataFrame of standardized\n",
        "    features, which will have NaNs during the initial warm-up period. A\n",
        "    consistency check is performed to ensure these NaNs align with the\n",
        "    provided validity mask.\n",
        "\n",
        "    Args:\n",
        "        features_df (pd.DataFrame):\n",
        "            The DataFrame of pre-standardized features ('u', 's', 'r', 'v')\n",
        "            from the previous task.\n",
        "        valid_mask (np.ndarray):\n",
        "            The boolean validity mask indicating which time steps have\n",
        "            sufficient history.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration dictionary, used to retrieve\n",
        "            standardization parameters.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, np.ndarray]:\n",
        "            - `standardized_features_df` (pd.DataFrame): The DataFrame of\n",
        "              standardized features, with columns ['u_std', 's_std', 'r_std', 'v_std'].\n",
        "            - `valid_mask` (np.ndarray): The original validity mask, passed\n",
        "              through unchanged but used for final validation.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the standardization process produces unexpected NaNs\n",
        "                    in rows that the validity mask marks as valid.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(features_df, pd.DataFrame) or features_df.empty:\n",
        "        raise ValueError(\"Input 'features_df' must be a non-empty pandas DataFrame.\")\n",
        "    if len(features_df) != len(valid_mask):\n",
        "        raise ValueError(\n",
        "            \"Length of 'features_df' and 'valid_mask' must be identical.\"\n",
        "        )\n",
        "\n",
        "    # Retrieve standardization parameters from the validated config.\n",
        "    config_dp = model_config['data_processing']\n",
        "    window = config_dp['standardization_window']\n",
        "    epsilon = config_dp['standardization_epsilon']\n",
        "\n",
        "    # --- Step 1: Compute rolling statistics ---\n",
        "    # This helper function computes the rolling mean and stabilized std. dev.\n",
        "    # The initial rows will correctly be NaN due to the full-window requirement.\n",
        "    rolling_means, rolling_stds_stabilized = _compute_rolling_statistics(\n",
        "        features_df=features_df,\n",
        "        window_size=window,\n",
        "        epsilon=epsilon\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Compute standardized features ---\n",
        "    # Equation: x_{z,t} = (z_t - μ_{z,t}) / \\tilde{σ}_{z,t}\n",
        "    # This operation is fully vectorized and index-aligned in pandas.\n",
        "    standardized_features_df = (features_df - rolling_means) / rolling_stds_stabilized\n",
        "\n",
        "    # Rename columns for clarity in subsequent modeling steps.\n",
        "    standardized_features_df.columns = [f\"{col}_std\" for col in features_df.columns]\n",
        "\n",
        "    # --- Step 3: Consolidate and perform final validity check ---\n",
        "    # This is a critical consistency check. We verify that for every time step\n",
        "    # marked as `True` in the validity mask, our standardization process has\n",
        "    # produced a full row of valid, non-NaN numbers.\n",
        "    valid_subset = standardized_features_df[valid_mask]\n",
        "    if valid_subset.isna().any().any():\n",
        "        raise ValueError(\n",
        "            \"Standardization resulted in unexpected NaN values for time steps \"\n",
        "            \"marked as valid by the input mask. Check window size alignment.\"\n",
        "        )\n",
        "\n",
        "    # The valid_mask does not need to be modified because its definition based on\n",
        "    # 'warmup_min_observations' is consistent with the standardization window.\n",
        "    return standardized_features_df, valid_mask\n"
      ],
      "metadata": {
        "id": "FKbtECAVmXFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 — Construct the Geometric Algebra (GA) multivector embedding M_t\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Construct the Geometric Algebra (GA) multivector embedding M_t\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_multivector_embedding(\n",
        "    standardized_features_df: pd.DataFrame,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs the dense geometric algebra multivector embedding M_t.\n",
        "\n",
        "    This function transforms the 4D standardized feature vectors into 11D\n",
        "    multivector representations for each time step. The multivector M_t\n",
        "    encodes the economic state by including scalar, vector, and bivector\n",
        "    components, capturing both the state of individual variables and their\n",
        "    pairwise interaction dynamics.\n",
        "\n",
        "    The embedding follows the structure defined in the paper:\n",
        "    M_t = α_0 + Σ α_i e_i + Σ γ_{ij} (x_{t,i} - x_{t,j}) (e_i ∧ e_j)\n",
        "    where α_0, α_i, and γ_{ij} are set to 1.\n",
        "\n",
        "    Args:\n",
        "        standardized_features_df (pd.DataFrame):\n",
        "            The DataFrame of standardized features with columns\n",
        "            ['u_std', 's_std', 'r_std', 'v_std'].\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration dictionary, containing the\n",
        "            'architecture' definition with 'component_order' and 'pi_order'.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray:\n",
        "            A NumPy array of shape (T, 11), where T is the number of time\n",
        "            steps. Each row represents the multivector M_t, with components\n",
        "            ordered according to 'component_order'. Initial rows corresponding\n",
        "            to the standardization warm-up period will contain NaNs.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(standardized_features_df, pd.DataFrame) or standardized_features_df.empty:\n",
        "        raise ValueError(\"Input 'standardized_features_df' must be a non-empty pandas DataFrame.\")\n",
        "\n",
        "    # Extract necessary configuration from the validated config dictionary.\n",
        "    arch_config = model_config['architecture']\n",
        "    component_order = arch_config['component_order']\n",
        "    pi_order = arch_config['pi_order']\n",
        "    active_components = arch_config['active_multivector_components']\n",
        "\n",
        "    # Verify the number of active components matches the component order list length.\n",
        "    if len(component_order) != active_components:\n",
        "        raise ValueError(\n",
        "            \"Configuration mismatch: 'active_multivector_components' must equal \"\n",
        "            f\"the length of 'component_order'. Got {active_components} and {len(component_order)}.\"\n",
        "        )\n",
        "\n",
        "    # Define the mapping from feature names to their 1-based index in the GA basis.\n",
        "    feature_map = {'u_std': 1, 's_std': 2, 'r_std': 3, 'v_std': 4}\n",
        "\n",
        "    # Verify that the input DataFrame has the expected columns.\n",
        "    expected_cols = [f\"{col}_std\" for col in ['u', 's', 'r', 'v']]\n",
        "    if not all(col in standardized_features_df.columns for col in expected_cols):\n",
        "        raise ValueError(f\"Input DataFrame must contain columns: {expected_cols}\")\n",
        "\n",
        "    # Get the number of time steps.\n",
        "    num_timesteps = len(standardized_features_df)\n",
        "\n",
        "    # Initialize the multivector matrix M with zeros.\n",
        "    # Shape is (T, 11), where 11 is the number of active components.\n",
        "    multivector_matrix = np.zeros((num_timesteps, active_components))\n",
        "\n",
        "    # --- Step 1: Populate Scalar Component (Grade 0) ---\n",
        "    # The scalar component is a constant baseline, set to 1.\n",
        "    # This corresponds to α_0 = 1.\n",
        "    scalar_idx = component_order.index('scalar')\n",
        "    multivector_matrix[:, scalar_idx] = 1.0\n",
        "\n",
        "    # --- Step 2: Populate Vector Components (Grade 1) ---\n",
        "    # These are the standardized features themselves.\n",
        "    # This corresponds to the term Σ α_i * e_i with α_i = 1.\n",
        "    vector_features = {\n",
        "        'e1(u)': 'u_std',\n",
        "        'e2(s)': 's_std',\n",
        "        'e3(r)': 'r_std',\n",
        "        'e4(v)': 'v_std'\n",
        "    }\n",
        "    for component_name, feature_col in vector_features.items():\n",
        "        component_idx = component_order.index(component_name)\n",
        "        multivector_matrix[:, component_idx] = standardized_features_df[feature_col].values\n",
        "\n",
        "    # --- Step 3: Populate Bivector Components (Grade 2) ---\n",
        "    # These capture the interaction dynamics as differences between pairs of features.\n",
        "    # This corresponds to the term Σ γ_{ij} * (x_{t,i} - x_{t,j}) * (e_i ∧ e_j) with γ_{ij} = 1.\n",
        "\n",
        "    # Create a reverse map from 1-based index to feature column name.\n",
        "    idx_to_col = {v: k for k, v in feature_map.items()}\n",
        "\n",
        "    # Iterate through the canonical pi_order to ensure correct bivector ordering.\n",
        "    for i, (idx1, idx2) in enumerate(pi_order):\n",
        "        # Determine the column names for the feature pair.\n",
        "        col1 = idx_to_col[idx1]\n",
        "        col2 = idx_to_col[idx2]\n",
        "\n",
        "        # The bivector component is the difference between the two standardized features.\n",
        "        bivector_values = (\n",
        "            standardized_features_df[col1].values -\n",
        "            standardized_features_df[col2].values\n",
        "        )\n",
        "\n",
        "        # Find the correct column index in the output matrix for this bivector.\n",
        "        # We start searching from the known start of bivector components for efficiency.\n",
        "        bivector_start_idx = len(vector_features) + 1 # 1 (scalar) + 4 (vectors)\n",
        "\n",
        "        # Construct the expected component name string to find its index.\n",
        "        # e.g., for (1,2) -> 'e1^e2'\n",
        "        base_name_part1 = component_order[idx1].split('(')[0] # 'e1'\n",
        "        base_name_part2 = component_order[idx2].split('(')[0] # 'e2'\n",
        "        expected_bivector_name_fragment = f\"{base_name_part1}^{base_name_part2}\"\n",
        "\n",
        "        # Find the component in the order list that matches this fragment.\n",
        "        target_idx = -1\n",
        "        for j in range(bivector_start_idx, len(component_order)):\n",
        "            if expected_bivector_name_fragment in component_order[j]:\n",
        "                target_idx = j\n",
        "                break\n",
        "\n",
        "        if target_idx == -1:\n",
        "            raise ValueError(f\"Could not find a matching bivector for pair ({idx1}, {idx2}) in component_order.\")\n",
        "\n",
        "        # Assign the computed difference to the correct column.\n",
        "        multivector_matrix[:, target_idx] = bivector_values\n",
        "\n",
        "    return multivector_matrix\n"
      ],
      "metadata": {
        "id": "a_m5J7AXm9g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7 — Initialize learnable projection matrices W_Q, W_K, W_V and head parameters\n",
        "\n",
        "# ================================================================================\n",
        "# Task 7: Initialize learnable projection matrices W_Q, W_K, W_V & head parameters\n",
        "# ================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 1 & 2: Helper to initialize all model parameters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _initialize_all_parameters(\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, nn.Parameter]:\n",
        "    \"\"\"\n",
        "    Initializes all learnable model parameters using best-practice schemes.\n",
        "\n",
        "    This function creates and initializes the tensors for the attention mechanism's\n",
        "    projection matrices (W_Q, W_K, W_V) and the MLP prediction head (W1, b1,\n",
        "    W2, b2). It uses Kaiming (He) initialization for layers preceding a ReLU\n",
        "    or Leaky ReLU activation and Xavier (Glorot) initialization for linear layers.\n",
        "    Biases are initialized to zero.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]):\n",
        "            The validated model configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, nn.Parameter]:\n",
        "            A dictionary containing all initialized model parameters, wrapped\n",
        "            as `torch.nn.Parameter` to enable gradient tracking.\n",
        "    \"\"\"\n",
        "    # Extract dimensions from the configuration for clarity.\n",
        "    arch_config = config['architecture']\n",
        "    head_config = config['prediction_head']\n",
        "\n",
        "    d_m = arch_config['active_multivector_components'] # e.g., 11\n",
        "    d_h = arch_config['hidden_dimension_dh']           # e.g., 32\n",
        "    h = head_config['mlp_hidden_dimension']            # e.g., 16\n",
        "\n",
        "    # --- Initialize Attention Projection Matrices ---\n",
        "\n",
        "    # W_Q and W_K precede the shifted Leaky ReLU activation. Kaiming (He)\n",
        "    # initialization is appropriate for ReLU-family activations.\n",
        "    # Shape: (hidden_dim, multivector_dim) -> (32, 11)\n",
        "    W_Q_tensor = torch.empty(d_h, d_m)\n",
        "    nn.init.kaiming_uniform_(W_Q_tensor, a=arch_config['leaky_relu_alpha'])\n",
        "\n",
        "    W_K_tensor = torch.empty(d_h, d_m)\n",
        "    nn.init.kaiming_uniform_(W_K_tensor, a=arch_config['leaky_relu_alpha'])\n",
        "\n",
        "    # W_V is a linear projection without a subsequent non-linearity.\n",
        "    # Xavier (Glorot) initialization is the standard choice here.\n",
        "    # Shape: (hidden_dim, multivector_dim) -> (32, 11)\n",
        "    W_V_tensor = torch.empty(d_h, d_m)\n",
        "    nn.init.xavier_uniform_(W_V_tensor)\n",
        "\n",
        "    # --- Initialize MLP Prediction Head Parameters ---\n",
        "\n",
        "    # W1 is the weight matrix for the first MLP layer, preceding a ReLU.\n",
        "    # Kaiming initialization is appropriate.\n",
        "    # Shape: (mlp_hidden_dim, attention_hidden_dim) -> (16, 32)\n",
        "    W1_tensor = torch.empty(h, d_h)\n",
        "    nn.init.kaiming_uniform_(W1_tensor, nonlinearity='relu')\n",
        "\n",
        "    # b1 is the bias for the first MLP layer. Initialize to zero.\n",
        "    # Shape: (mlp_hidden_dim,) -> (16,)\n",
        "    b1_tensor = torch.empty(h)\n",
        "    nn.init.zeros_(b1_tensor)\n",
        "\n",
        "    # W2 is the weight matrix for the final linear output layer.\n",
        "    # Xavier initialization is appropriate.\n",
        "    # Shape: (output_dim, mlp_hidden_dim) -> (1, 16)\n",
        "    W2_tensor = torch.empty(1, h)\n",
        "    nn.init.xavier_uniform_(W2_tensor)\n",
        "\n",
        "    # b2 is the final output bias. Initialize to zero.\n",
        "    # Shape: (output_dim,) -> (1,)\n",
        "    b2_tensor = torch.empty(1)\n",
        "    nn.init.zeros_(b2_tensor)\n",
        "\n",
        "    # --- Package all tensors as nn.Parameter in a dictionary ---\n",
        "    # Wrapping with nn.Parameter registers them for gradient computation.\n",
        "    parameters = {\n",
        "        'W_Q': nn.Parameter(W_Q_tensor),\n",
        "        'W_K': nn.Parameter(W_K_tensor),\n",
        "        'W_V': nn.Parameter(W_V_tensor),\n",
        "        'W1': nn.Parameter(W1_tensor),\n",
        "        'b1': nn.Parameter(b1_tensor),\n",
        "        'W2': nn.Parameter(W2_tensor),\n",
        "        'b2': nn.Parameter(b2_tensor),\n",
        "    }\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def initialize_model_and_optimizer(\n",
        "    model_config: Dict[str, Any],\n",
        "    random_seed: int\n",
        ") -> Tuple[Dict[str, nn.Parameter], torch.optim.Optimizer]:\n",
        "    \"\"\"\n",
        "    Orchestrates the initialization of all model parameters and the optimizer.\n",
        "\n",
        "    This function provides a reproducible entry point for setting up the model's\n",
        "    learnable components. It first sets a global random seed, then initializes\n",
        "    all parameters according to best practices, and finally configures the\n",
        "    Adam optimizer with hyperparameters specified in the configuration.\n",
        "\n",
        "    Args:\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration dictionary.\n",
        "        random_seed (int):\n",
        "            An integer to seed the random number generator for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, nn.Parameter], torch.optim.Optimizer]:\n",
        "            - A dictionary containing all named, initialized model parameters.\n",
        "            - The configured Adam optimizer instance ready for training.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(random_seed, int):\n",
        "        raise TypeError(\"random_seed must be an integer.\")\n",
        "\n",
        "    # Step 0: Set the random seed for reproducibility.\n",
        "    # This ensures that the random weight initialization is identical every time.\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    # Step 1 & 2: Initialize all learnable parameters for the attention\n",
        "    # mechanism and the MLP head using a dedicated helper function.\n",
        "    parameters = _initialize_all_parameters(model_config)\n",
        "\n",
        "    # Step 3: Configure and instantiate the Adam optimizer.\n",
        "    train_config = model_config['training']\n",
        "\n",
        "    # The optimizer is configured with all model parameters.\n",
        "    # The learning rate, betas, and weight decay are sourced from the config.\n",
        "    # The config validation in Task 2 ensures weight_decay is 0.0, preventing\n",
        "    # conflict with the explicit L2 regularization loss term.\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=parameters.values(),\n",
        "        lr=train_config['learning_rate_eta'],\n",
        "        betas=train_config['adam_betas'],\n",
        "        weight_decay=train_config['weight_decay']\n",
        "    )\n",
        "\n",
        "    return parameters, optimizer\n"
      ],
      "metadata": {
        "id": "94alqKxdoEb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 — Implement the shifted Leaky ReLU activation φ\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Implement the shifted Leaky ReLU activation φ\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, All Steps: Implement the Shifted Leaky ReLU Activation Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def shifted_leaky_relu(\n",
        "    x: torch.Tensor,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Implements the shifted Leaky ReLU activation function, φ.\n",
        "\n",
        "    This function applies the custom activation defined in the paper, which is\n",
        "    essential for ensuring the positivity of the query (Q) and key (K) vectors\n",
        "    in the linear attention mechanism. This positivity is critical for the\n",
        "    stability of the rational attention denominator.\n",
        "\n",
        "    The function is defined by the piecewise formula from Equation (7):\n",
        "    φ(x) = x + 1,      if x > 0\n",
        "    φ(x) = α * x + 1,  if x <= 0\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor):\n",
        "            The input tensor of pre-activation values. Can be of any shape.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration dictionary, from which the `alpha`\n",
        "            parameter for the Leaky ReLU is sourced.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor:\n",
        "            A new tensor of the same shape as the input, with the shifted\n",
        "            Leaky ReLU activation applied element-wise.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input `x` is not a torch.Tensor.\n",
        "        KeyError: If 'leaky_relu_alpha' is not found in the configuration.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the primary input is a PyTorch tensor.\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "        raise TypeError(f\"Input 'x' must be a torch.Tensor, but got {type(x).__name__}.\")\n",
        "\n",
        "    # Retrieve the alpha parameter from the validated configuration.\n",
        "    try:\n",
        "        alpha = model_config['architecture']['leaky_relu_alpha']\n",
        "    except KeyError:\n",
        "        raise KeyError(\"Configuration missing 'leaky_relu_alpha' in 'architecture' section.\")\n",
        "\n",
        "    # --- Implementation of the Shifted Leaky ReLU ---\n",
        "    # This implementation uses torch.where for a fully vectorized and\n",
        "    # computationally efficient application of the piecewise formula.\n",
        "    # The autograd engine in PyTorch will correctly handle the gradient\n",
        "    # computation for this conditional operation.\n",
        "\n",
        "    # Condition: Check where elements of the input tensor x are greater than 0.\n",
        "    condition = x > 0\n",
        "\n",
        "    # Value if True: For x > 0, the function is x + 1.\n",
        "    value_if_true = x + 1.0\n",
        "\n",
        "    # Value if False: For x <= 0, the function is α * x + 1.\n",
        "    value_if_false = alpha * x + 1.0\n",
        "\n",
        "    # Apply the condition element-wise to construct the output tensor.\n",
        "    output = torch.where(condition, value_if_true, value_if_false)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "j_E-Lveco2zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9 — Compute Q_t, K_t, V_t projections for each time step\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Compute Q_t, K_t, V_t projections for each time step\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_qkv_projections(\n",
        "    multivector_matrix: np.ndarray,\n",
        "    parameters: Dict[str, nn.Parameter],\n",
        "    model_config: Dict[str, Any]\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Orchestrates the projection of multivector embeddings into Q, K, V spaces.\n",
        "\n",
        "    This function performs the linear transformations that map the 11D geometric\n",
        "    multivector embeddings into the lower-dimensional query, key, and value\n",
        "    spaces required by the attention mechanism. It applies the custom shifted\n",
        "    Leaky ReLU activation function to the query and key projections, while the\n",
        "    value projection remains linear, as specified by the model architecture.\n",
        "\n",
        "    Args:\n",
        "        multivector_matrix (np.ndarray):\n",
        "            The (T, 11) NumPy array of multivector embeddings from Task 6.\n",
        "            T is the total number of time steps.\n",
        "        parameters (Dict[str, nn.Parameter]):\n",
        "            A dictionary containing the initialized model parameters, including\n",
        "            the projection matrices 'W_Q', 'W_K', and 'W_V'.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration dictionary, needed for the\n",
        "            activation function.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, torch.Tensor]:\n",
        "            A dictionary containing the resulting 'queries', 'keys', and 'values'\n",
        "            tensors, each of shape (T, d_h), where d_h is the hidden dimension.\n",
        "            Rows corresponding to warm-up periods will contain NaNs.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(multivector_matrix, np.ndarray):\n",
        "        raise TypeError(\"Input 'multivector_matrix' must be a NumPy array.\")\n",
        "\n",
        "    required_params = {'W_Q', 'W_K', 'W_V'}\n",
        "    if not required_params.issubset(parameters.keys()):\n",
        "        missing = required_params - set(parameters.keys())\n",
        "        raise KeyError(f\"Missing required projection matrices in 'parameters': {missing}\")\n",
        "\n",
        "    # Convert the NumPy multivector matrix to a PyTorch tensor.\n",
        "    # This is necessary for performing tensor operations and enabling gradient tracking.\n",
        "    # We set the dtype to float32, the standard for deep learning models.\n",
        "    M = torch.tensor(multivector_matrix, dtype=torch.float32)\n",
        "\n",
        "    # Retrieve the projection matrices from the parameters dictionary.\n",
        "    W_Q = parameters['W_Q']\n",
        "    W_K = parameters['W_K']\n",
        "    W_V = parameters['W_V']\n",
        "\n",
        "    # --- Step 1: Project M_t through W_Q and W_K with activation φ ---\n",
        "    # This is a single, batched matrix multiplication for all time steps.\n",
        "    # The operation is M @ W.T, with shapes (T, d_m) @ (d_m, d_h) -> (T, d_h).\n",
        "\n",
        "    # Equation: Q_t = φ(W_Q * M_t)\n",
        "    # Compute pre-activations for Queries.\n",
        "    pre_activation_Q = torch.matmul(M, W_Q.t())\n",
        "    # Apply the custom shifted Leaky ReLU activation function.\n",
        "    queries = shifted_leaky_relu(pre_activation_Q, model_config)\n",
        "\n",
        "    # Equation: K_t = φ(W_K * M_t)\n",
        "    # Compute pre-activations for Keys.\n",
        "    pre_activation_K = torch.matmul(M, W_K.t())\n",
        "    # Apply the custom shifted Leaky ReLU activation function.\n",
        "    keys = shifted_leaky_relu(pre_activation_K, model_config)\n",
        "\n",
        "    # --- Step 2: Project M_t through W_V without activation ---\n",
        "    # The value projection is a pure linear transformation.\n",
        "\n",
        "    # Equation: V_t = W_V * M_t\n",
        "    # Compute the Values tensor directly via matrix multiplication.\n",
        "    values = torch.matmul(M, W_V.t())\n",
        "\n",
        "    # --- Step 3: Store Q, K, V sequences for attention computation ---\n",
        "    # Package the results into a dictionary for clear, structured output.\n",
        "    qkv_projections = {\n",
        "        'queries': queries,\n",
        "        'keys': keys,\n",
        "        'values': values\n",
        "    }\n",
        "\n",
        "    return qkv_projections\n"
      ],
      "metadata": {
        "id": "_d8FesWOpWuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 — Compute linear attention sufficient statistics S_t and Z_t\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Compute linear attention sufficient statistics S_t and Z_t\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Helper: Efficient Causal Rolling Sum for Tensors\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _efficient_rolling_sum(\n",
        "    tensor: torch.Tensor,\n",
        "    window_size: int\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes a standard rolling sum on a tensor with O(T) complexity.\n",
        "\n",
        "    This function calculates the sum over a sliding window of a fixed size.\n",
        "    The value at index `t` is the sum of the tensor's values over the\n",
        "    inclusive window [t - window_size + 1, t]. It is implemented efficiently\n",
        "    using the cumulative sum method.\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor):\n",
        "            The input tensor. The rolling sum is applied along the first\n",
        "            dimension (dim=0), which is assumed to represent time.\n",
        "        window_size (int):\n",
        "            The size of the rolling window (L).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor:\n",
        "            A tensor of the same shape as the input, containing the rolling sums.\n",
        "            The first `window_size - 1` elements are effectively warm-up values\n",
        "            and will be handled by the `valid_mask` in the calling function.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(tensor, torch.Tensor):\n",
        "        raise TypeError(\"Input 'tensor' must be a torch.Tensor.\")\n",
        "    if not isinstance(window_size, int) or window_size <= 0:\n",
        "        raise ValueError(\"'window_size' must be a positive integer.\")\n",
        "\n",
        "    # Pad the tensor at the beginning with zeros. This simplifies the cumsum logic\n",
        "    # by ensuring the subtraction window is always valid.\n",
        "    # The padding is applied only along the time dimension (the first dimension).\n",
        "    padding_dims = [0, 0] * (tensor.dim() - 1) + [window_size - 1, 0]\n",
        "    padded_tensor = F.pad(tensor, padding_dims)\n",
        "\n",
        "    # Compute the cumulative sum along the time dimension.\n",
        "    cumsum = torch.cumsum(padded_tensor, dim=0)\n",
        "\n",
        "    # Compute the rolling sum by subtracting the lagged cumulative sum.\n",
        "    # The sum over [t-L+1, t] is cumsum[t] - cumsum[t-L].\n",
        "    # Due to padding, this is equivalent to cumsum[t+L-1] - cumsum[t-1] on the padded tensor.\n",
        "    rolling_sum = cumsum[window_size:] - cumsum[:-window_size]\n",
        "\n",
        "    return rolling_sum\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_attention_statistics(\n",
        "    qkv_projections: Dict[str, torch.Tensor],\n",
        "    valid_mask: torch.Tensor,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of causal attention statistics.\n",
        "\n",
        "    This function calculates the sufficient statistics S_t and Z_t for the\n",
        "    linear attention mechanism. This corrected implementation ensures causality\n",
        "    by explicitly lagging the key and value tensors before applying an efficient\n",
        "    rolling sum. This approach is transparent, correct, and robust.\n",
        "\n",
        "    - S_t = Σ_{τ=t-L to t-1} K_τ V_τ^T\n",
        "    - Z_t = Σ_{τ=t-L to t-1} K_τ\n",
        "\n",
        "    Args:\n",
        "        qkv_projections (Dict[str, torch.Tensor]):\n",
        "            Dictionary with 'keys' and 'values' tensors, shape (T, d_h).\n",
        "        valid_mask (torch.Tensor):\n",
        "            A boolean tensor of shape (T,) indicating valid time steps.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration with 'lookback_horizon_L'.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, torch.Tensor]:\n",
        "            A dictionary containing the sufficient statistics:\n",
        "            - 'S_statistics' (torch.Tensor): Shape (T, d_h, d_h).\n",
        "            - 'Z_statistics' (torch.Tensor): Shape (T, d_h).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_keys = {'keys', 'values'}\n",
        "    if not required_keys.issubset(qkv_projections.keys()):\n",
        "        raise KeyError(f\"Input 'qkv_projections' is missing keys: {required_keys - set(qkv_projections.keys())}\")\n",
        "\n",
        "    keys = qkv_projections['keys']\n",
        "    values = qkv_projections['values']\n",
        "\n",
        "    if keys.shape != values.shape:\n",
        "        raise ValueError(\"'keys' and 'values' tensors must have the same shape.\")\n",
        "\n",
        "    # Retrieve the lookback horizon (L).\n",
        "    lookback_horizon = model_config['data_processing']['lookback_horizon_L']\n",
        "\n",
        "    # --- Step 1: Enforce Causality by Lagging Tensors ---\n",
        "    # To compute statistics for time `t` using data up to `t-1`, we first\n",
        "    # lag the key and value tensors by one step. We pad with a zero vector at\n",
        "    # the start and remove the last element.\n",
        "    # `lagged_keys[t]` will now contain the original `keys[t-1]`.\n",
        "    keys_lagged = F.pad(keys, (0, 0, 1, 0))[:-1]\n",
        "    values_lagged = F.pad(values, (0, 0, 1, 0))[:-1]\n",
        "\n",
        "    # --- Step 2: Compute Outer Products on Lagged Data ---\n",
        "    # The outer product K_τ V_τ^T is computed for each time step τ using the\n",
        "    # lagged tensors. This is vectorized using batch matrix multiplication.\n",
        "    # Shapes: (T, d_h, 1) @ (T, 1, d_h) -> (T, d_h, d_h).\n",
        "    outer_products_lagged = torch.bmm(keys_lagged.unsqueeze(2), values_lagged.unsqueeze(1))\n",
        "\n",
        "    # --- Step 3: Compute S_t and Z_t with Rolling Sum on Lagged Data ---\n",
        "    # Now we apply a standard rolling sum of size L to the lagged data.\n",
        "    # The sum at time `t` covers the window [t-L+1, t] of the lagged data,\n",
        "    # which corresponds to the window [t-L, t-1] of the original data.\n",
        "    # This correctly and transparently implements the causal summation.\n",
        "\n",
        "    # Equation: S_t = Σ_{τ=t-L to t-1} K_τ V_τ^T\n",
        "    S_statistics = _efficient_rolling_sum(outer_products_lagged, lookback_horizon)\n",
        "\n",
        "    # Equation: Z_t = Σ_{τ=t-L to t-1} K_τ\n",
        "    Z_statistics = _efficient_rolling_sum(keys_lagged, lookback_horizon)\n",
        "\n",
        "    # --- Final Masking ---\n",
        "    # Zero out the statistics for any time steps that are invalid according\n",
        "    # to the mask. This is a safeguard.\n",
        "    S_statistics[~valid_mask] = 0.0\n",
        "    Z_statistics[~valid_mask] = 0.0\n",
        "\n",
        "    return {\n",
        "        'S_statistics': S_statistics,\n",
        "        'Z_statistics': Z_statistics\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ZI3_jWgNqF-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11 — Compute the attended context O_t via rational linear attention\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Compute the attended context O_t via rational linear attention\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_attended_context(\n",
        "    qkv_projections: Dict[str, torch.Tensor],\n",
        "    attention_statistics: Dict[str, torch.Tensor],\n",
        "    valid_mask: torch.Tensor,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the attended context vector O_t using the rational linear attention formula.\n",
        "\n",
        "    This function is the core of the attention mechanism, combining the queries (Q)\n",
        "    with the pre-computed sufficient statistics (S and Z) to produce a context\n",
        "    vector for each time step. The implementation is fully vectorized to process\n",
        "    the entire time series simultaneously.\n",
        "\n",
        "    The function implements Equation (10) from the paper:\n",
        "    O_t = (Q_t^T * S_t) / (Q_t^T * Z_t + ε)\n",
        "\n",
        "    Args:\n",
        "        qkv_projections (Dict[str, torch.Tensor]):\n",
        "            Dictionary containing the 'queries' tensor of shape (T, d_h).\n",
        "        attention_statistics (Dict[str, torch.Tensor]):\n",
        "            Dictionary containing the 'S_statistics' (T, d_h, d_h) and\n",
        "            'Z_statistics' (T, d_h) tensors.\n",
        "        valid_mask (torch.Tensor):\n",
        "            A boolean tensor of shape (T,) indicating which time steps are\n",
        "            valid for computation.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration, used to retrieve the\n",
        "            'attention_stability_epsilon'.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor:\n",
        "            The attended context tensor of shape (T, d_h). Rows corresponding\n",
        "            to invalid time steps (as per the mask) are zeroed out.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If required keys are missing from the input dictionaries.\n",
        "        ValueError: If tensor shapes are inconsistent.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if 'queries' not in qkv_projections:\n",
        "        raise KeyError(\"Input 'qkv_projections' is missing the 'queries' tensor.\")\n",
        "    if not {'S_statistics', 'Z_statistics'}.issubset(attention_statistics.keys()):\n",
        "        raise KeyError(\"Input 'attention_statistics' is missing required statistic tensors.\")\n",
        "\n",
        "    queries = qkv_projections['queries']\n",
        "    S_stats = attention_statistics['S_statistics']\n",
        "    Z_stats = attention_statistics['Z_statistics']\n",
        "\n",
        "    # Verify shape consistency.\n",
        "    T, d_h = queries.shape\n",
        "    if S_stats.shape != (T, d_h, d_h) or Z_stats.shape != (T, d_h):\n",
        "        raise ValueError(\"Inconsistent tensor shapes for queries and statistics.\")\n",
        "\n",
        "    # Retrieve the stability epsilon from the configuration.\n",
        "    epsilon = model_config['architecture']['attention_stability_epsilon']\n",
        "\n",
        "    # --- Step 1: Compute the Numerator (Q_t^T * S_t) ---\n",
        "    # This is a batch matrix-vector product. We reshape Q to (T, 1, d_h)\n",
        "    # to use torch.bmm with S of shape (T, d_h, d_h).\n",
        "    # The result has shape (T, 1, d_h), which we squeeze back to (T, d_h).\n",
        "    queries_reshaped = queries.unsqueeze(1)\n",
        "    numerator = torch.bmm(queries_reshaped, S_stats).squeeze(1)\n",
        "\n",
        "    # --- Step 2: Compute the Denominator (Q_t^T * Z_t + ε) ---\n",
        "    # This is a batch dot product. We compute it via element-wise\n",
        "    # multiplication and summing over the hidden dimension.\n",
        "    # The result is a vector of shape (T,).\n",
        "    dot_product = torch.sum(queries * Z_stats, dim=1)\n",
        "\n",
        "    # Add the stability constant epsilon.\n",
        "    denominator = dot_product + epsilon\n",
        "\n",
        "    # --- Defensive Check for Stability ---\n",
        "    # In a well-behaved model, the denominator should be positive due to the\n",
        "    # shifted Leaky ReLU on Q and K. A non-positive value indicates a problem.\n",
        "    if torch.any(denominator <= 0):\n",
        "        # This should not happen in production but is a critical debug check.\n",
        "        # We log a warning instead of raising an error to allow training to continue,\n",
        "        # but this signals a potential instability.\n",
        "        warnings.warn(\"Non-positive denominator detected in attention mechanism.\")\n",
        "\n",
        "    # --- Step 3: Compute the Attended Context O_t ---\n",
        "    # We divide the numerator (T, d_h) by the denominator (T,).\n",
        "    # To enable broadcasting, we unsqueeze the denominator to (T, 1).\n",
        "    attended_context = numerator / denominator.unsqueeze(1)\n",
        "\n",
        "    # --- Final Masking ---\n",
        "    # Ensure that context vectors for invalid time steps are zeroed out.\n",
        "    # This prevents any data leakage from the warm-up period into the loss.\n",
        "    # We unsqueeze the mask for broadcasting across the hidden dimension.\n",
        "    attended_context[~valid_mask] = 0.0\n",
        "\n",
        "    return attended_context\n"
      ],
      "metadata": {
        "id": "KbcxT3fMqt7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12 — Forward pass through the MLP prediction head to obtain ŷ_t\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Forward pass through the MLP prediction head to obtain ŷ_t\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def mlp_forward_pass(\n",
        "    attended_context: torch.Tensor,\n",
        "    parameters: Dict[str, nn.Parameter]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Performs a forward pass through the two-layer MLP prediction head.\n",
        "\n",
        "    This function takes the attended context vectors and maps them to final\n",
        "    scalar predictions (ŷ_t) for the charge-off rate. The MLP introduces a\n",
        "    controlled non-linearity, which, as noted in the paper, can help in\n",
        "    capturing sharp turning points in the credit cycle.\n",
        "\n",
        "    The function implements the full MLP equation from the paper (Equation 12):\n",
        "    ŷ_t = W_2 * σ(W_1 * O_t + b_1) + b_2\n",
        "    where σ is the ReLU activation function.\n",
        "\n",
        "    Args:\n",
        "        attended_context (torch.Tensor):\n",
        "            The (T, d_h) tensor of context vectors from the attention mechanism.\n",
        "            T is the number of time steps, d_h is the hidden dimension.\n",
        "        parameters (Dict[str, nn.Parameter]):\n",
        "            A dictionary containing the initialized model parameters, including\n",
        "            the MLP weights and biases 'W1', 'b1', 'W2', 'b2'.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor:\n",
        "            A 1D tensor of shape (T,) containing the scalar prediction for each\n",
        "            time step. Predictions for invalid time steps (e.g., warm-up)\n",
        "            will be zero if their corresponding context vectors were zero.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If required MLP parameters are missing from the dictionary.\n",
        "        ValueError: If tensor shapes are inconsistent for matrix multiplication.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_params = {'W1', 'b1', 'W2', 'b2'}\n",
        "    if not required_params.issubset(parameters.keys()):\n",
        "        missing = required_params - set(parameters.keys())\n",
        "        raise KeyError(f\"Missing required MLP parameters: {missing}\")\n",
        "\n",
        "    W1, b1, W2, b2 = (\n",
        "        parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
        "    )\n",
        "\n",
        "    # Verify shape consistency for the first layer.\n",
        "    # W1 shape: (h, d_h), attended_context shape: (T, d_h)\n",
        "    if W1.shape[1] != attended_context.shape[1]:\n",
        "        raise ValueError(\n",
        "            f\"Shape mismatch for first MLP layer: W1 columns ({W1.shape[1]}) \"\n",
        "            f\"must match attended_context columns ({attended_context.shape[1]}).\"\n",
        "        )\n",
        "\n",
        "    # --- Step 1: Compute the hidden layer activation ---\n",
        "    # This is a single batched operation for all time steps T.\n",
        "\n",
        "    # Equation: h_t = σ(W_1 * O_t + b_1)\n",
        "    # Linear transformation: O @ W1.T\n",
        "    # Shapes: (T, d_h) @ (d_h, h) -> (T, h)\n",
        "    hidden_pre_activation = torch.matmul(attended_context, W1.t()) + b1\n",
        "\n",
        "    # Apply ReLU activation function element-wise.\n",
        "    hidden_activation = F.relu(hidden_pre_activation)\n",
        "\n",
        "    # --- Step 2: Compute the scalar prediction ŷ_t ---\n",
        "    # This is the final linear output layer.\n",
        "\n",
        "    # Verify shape consistency for the second layer.\n",
        "    # W2 shape: (1, h), hidden_activation shape: (T, h)\n",
        "    if W2.shape[1] != hidden_activation.shape[1]:\n",
        "        raise ValueError(\n",
        "            f\"Shape mismatch for second MLP layer: W2 columns ({W2.shape[1]}) \"\n",
        "            f\"must match hidden_activation columns ({hidden_activation.shape[1]}).\"\n",
        "        )\n",
        "\n",
        "    # Equation: ŷ_t = W_2 * h_t + b_2\n",
        "    # Linear transformation: h @ W2.T\n",
        "    # Shapes: (T, h) @ (h, 1) -> (T, 1)\n",
        "    output = torch.matmul(hidden_activation, W2.t()) + b2\n",
        "\n",
        "    # --- Step 3: Store predictions and align with ground truth ---\n",
        "    # Squeeze the last dimension to get a 1D tensor of shape (T,).\n",
        "    # This is a convenient format for the loss function.\n",
        "    predictions = output.squeeze(-1)\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "aXogUSswreg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13 — Compute the prediction loss L_pred with warm-up masking\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Compute the prediction loss L_pred with warm-up masking\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_prediction_loss(\n",
        "    predictions: torch.Tensor,\n",
        "    ground_truth: torch.Tensor,\n",
        "    valid_mask: Union[torch.Tensor, np.ndarray]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the masked Mean Squared Error (MSE) prediction loss.\n",
        "\n",
        "    This function calculates the prediction loss, L_pred, by comparing the\n",
        "    model's predictions against the ground truth values. Crucially, it only\n",
        "    considers the time steps marked as valid by the `valid_mask`, effectively\n",
        "    ignoring the warm-up period where predictions are based on insufficient\n",
        "    historical data.\n",
        "\n",
        "    The loss is calculated as:\n",
        "    L_pred = (1 / |T_valid|) * Σ_{t ∈ T_valid} (ŷ_t - y_t)^2\n",
        "    where T_valid is the set of time steps where valid_mask is True.\n",
        "\n",
        "    Args:\n",
        "        predictions (torch.Tensor):\n",
        "            A 1D tensor of shape (T,) containing the model's scalar prediction\n",
        "            for each time step.\n",
        "        ground_truth (torch.Tensor):\n",
        "            A 1D tensor of shape (T,) containing the actual target values\n",
        "            (e.g., charge-off rates).\n",
        "        valid_mask (Union[torch.Tensor, np.ndarray]):\n",
        "            A 1D boolean tensor or array of shape (T,) where `True` indicates\n",
        "            a valid time step to be included in the loss calculation.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor:\n",
        "            A scalar tensor containing the computed MSE loss. If no samples\n",
        "            are valid, it returns a scalar zero tensor with requires_grad=False.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input tensors and mask have inconsistent lengths.\n",
        "        TypeError: If inputs are not of the expected tensor/array types.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(predictions, torch.Tensor) or not isinstance(ground_truth, torch.Tensor):\n",
        "        raise TypeError(\"Inputs 'predictions' and 'ground_truth' must be torch.Tensors.\")\n",
        "    if not isinstance(valid_mask, (torch.Tensor, np.ndarray)):\n",
        "        raise TypeError(\"Input 'valid_mask' must be a torch.Tensor or np.ndarray.\")\n",
        "\n",
        "    if not (predictions.shape == ground_truth.shape and predictions.shape[0] == len(valid_mask)):\n",
        "        raise ValueError(\n",
        "            \"Shape mismatch: 'predictions', 'ground_truth', and 'valid_mask' must \"\n",
        "            f\"have the same length. Got shapes {predictions.shape}, \"\n",
        "            f\"{ground_truth.shape}, and ({len(valid_mask)},).\"\n",
        "        )\n",
        "\n",
        "    # Ensure the mask is a boolean PyTorch tensor for indexing.\n",
        "    if isinstance(valid_mask, np.ndarray):\n",
        "        mask_tensor = torch.from_numpy(valid_mask).bool()\n",
        "    else:\n",
        "        mask_tensor = valid_mask.bool()\n",
        "\n",
        "    # --- Step 1: Identify the set of quarters eligible for loss computation ---\n",
        "\n",
        "    # Check if there are any valid samples to compute loss on.\n",
        "    num_valid_samples = mask_tensor.sum()\n",
        "    if num_valid_samples == 0:\n",
        "        # If no samples are valid, the loss is undefined. Returning 0.0 is a\n",
        "        # safe, neutral action that results in no gradient update.\n",
        "        return torch.tensor(0.0)\n",
        "\n",
        "    # Apply the boolean mask to select only the valid predictions and targets.\n",
        "    # This is the most critical step for ensuring the warm-up period is ignored.\n",
        "    predictions_valid = predictions[mask_tensor]\n",
        "    ground_truth_valid = ground_truth[mask_tensor]\n",
        "\n",
        "    # --- Step 2: Compute Mean Squared Error over unmasked quarters ---\n",
        "\n",
        "    # Use PyTorch's built-in MSE loss function for efficiency and numerical stability.\n",
        "    # It correctly computes the mean of the squared errors over the valid samples.\n",
        "    loss = F.mse_loss(\n",
        "        input=predictions_valid,\n",
        "        target=ground_truth_valid,\n",
        "        reduction='mean'\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Validate that the loss is finite and non-negative ---\n",
        "    # This is a defensive check against numerical issues like inf or nan loss.\n",
        "    if not torch.isfinite(loss) or loss < 0:\n",
        "        # This should not happen in normal operation but is a critical safeguard.\n",
        "        warnings.warn(f\"Computed prediction loss is not a finite, non-negative number: {loss.item()}\")\n",
        "        # In a production system, one might return 0 or raise an error.\n",
        "        # For training, we allow it to proceed but with a warning.\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "QF--heoAsdBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 — Compute the structured L2 regularization loss L_reg\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Compute the structured L2 regularization loss L_reg\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_regularization_loss(\n",
        "    parameters: Dict[str, nn.Parameter],\n",
        "    model_config: Dict[str, Any]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the structured L2 regularization loss (L_reg).\n",
        "\n",
        "    This function implements the regularization penalty specified in the paper,\n",
        "    which applies different weights to different sets of parameters. Specifically,\n",
        "    it penalizes the query (W_Q) and key (W_K) projection matrices more heavily\n",
        "    than the value (W_V) matrix to promote stability in the attention similarity\n",
        "    kernels, as justified in Section 5.3 of the paper.\n",
        "\n",
        "    The function implements Equation (35):\n",
        "    L_reg = λ_QK * (||W_Q||_F^2 + ||W_K||_F^2) + λ_V * ||W_V||_F^2\n",
        "    where ||.||_F^2 is the squared Frobenius norm.\n",
        "\n",
        "    Args:\n",
        "        parameters (Dict[str, nn.Parameter]):\n",
        "            A dictionary containing the initialized model parameters, including\n",
        "            the projection matrices 'W_Q', 'W_K', and 'W_V'.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration, used to retrieve the regularization\n",
        "            hyperparameters 'lambda_qk' and 'lambda_v'.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor:\n",
        "            A scalar tensor containing the computed regularization loss.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If required parameters or configuration keys are missing.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    required_params = {'W_Q', 'W_K', 'W_V'}\n",
        "    if not required_params.issubset(parameters.keys()):\n",
        "        missing = required_params - set(parameters.keys())\n",
        "        raise KeyError(f\"Missing required projection matrices in 'parameters': {missing}\")\n",
        "\n",
        "    try:\n",
        "        train_config = model_config['training']\n",
        "        lambda_qk = train_config['regularization_lambda_qk']\n",
        "        lambda_v = train_config['regularization_lambda_v']\n",
        "    except KeyError as e:\n",
        "        raise KeyError(f\"Could not find required regularization key in model_config: {e}\")\n",
        "\n",
        "    # Retrieve the relevant parameter tensors.\n",
        "    W_Q = parameters['W_Q']\n",
        "    W_K = parameters['W_K']\n",
        "    W_V = parameters['W_V']\n",
        "\n",
        "    # --- Step 1: Compute Frobenius norms of W_Q and W_K ---\n",
        "    # The squared Frobenius norm is the sum of the squares of all elements.\n",
        "    # W.pow(2).sum() is a direct and efficient way to compute this.\n",
        "\n",
        "    # Equation: ||W_Q||_F^2 = Σ_{i,j} (W_Q)_{ij}^2\n",
        "    norm_W_Q_sq = torch.sum(W_Q.pow(2))\n",
        "\n",
        "    # Equation: ||W_K||_F^2 = Σ_{i,j} (W_K)_{ij}^2\n",
        "    norm_W_K_sq = torch.sum(W_K.pow(2))\n",
        "\n",
        "    # --- Step 2: Compute Frobenius norm of W_V ---\n",
        "\n",
        "    # Equation: ||W_V||_F^2 = Σ_{i,j} (W_V)_{ij}^2\n",
        "    norm_W_V_sq = torch.sum(W_V.pow(2))\n",
        "\n",
        "    # --- Step 3: Assemble the structured regularization loss ---\n",
        "    # Apply the different lambda weights to the corresponding squared norms.\n",
        "\n",
        "    # Term 1: Penalty on Query and Key matrices.\n",
        "    loss_qk = lambda_qk * (norm_W_Q_sq + norm_W_K_sq)\n",
        "\n",
        "    # Term 2: Penalty on the Value matrix.\n",
        "    loss_v = lambda_v * norm_W_V_sq\n",
        "\n",
        "    # The total regularization loss is the sum of the two terms.\n",
        "    # L_reg = λ_QK * (||W_Q||_F^2 + ||W_K||_F^2) + λ_V * ||W_V||_F^2\n",
        "    total_reg_loss = loss_qk + loss_v\n",
        "\n",
        "    return total_reg_loss\n"
      ],
      "metadata": {
        "id": "4nkKqa8UzXXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 — Compute the total loss and perform backpropagation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Compute the total loss and perform backpropagation\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def perform_training_step(\n",
        "    prediction_loss: torch.Tensor,\n",
        "    regularization_loss: torch.Tensor,\n",
        "    optimizer: torch.optim.Optimizer\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Performs a single training step: loss computation, backpropagation, and update.\n",
        "\n",
        "    This function encapsulates the core logic of a single iteration of model\n",
        "    training. It combines the prediction loss with the regularization loss to\n",
        "    form the final objective function, computes the gradients of this total loss\n",
        "    with respect to all model parameters, and then updates the parameters using\n",
        "    the provided optimizer.\n",
        "\n",
        "    Args:\n",
        "        prediction_loss (torch.Tensor):\n",
        "            The scalar tensor for the prediction loss (e.g., MSE), computed\n",
        "            from Task 13.\n",
        "        regularization_loss (torch.Tensor):\n",
        "            The scalar tensor for the structured L2 regularization loss,\n",
        "            computed from Task 14.\n",
        "        optimizer (torch.optim.Optimizer):\n",
        "            The optimizer instance (e.g., Adam) that holds the model's\n",
        "            parameters and will perform the update step.\n",
        "\n",
        "    Returns:\n",
        "        float:\n",
        "            The scalar value of the total loss for this training step,\n",
        "            detached from the computation graph, suitable for logging.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the inputs are not of the expected types.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(prediction_loss, torch.Tensor) or not isinstance(regularization_loss, torch.Tensor):\n",
        "        raise TypeError(\"Loss components must be torch.Tensors.\")\n",
        "    if not isinstance(optimizer, torch.optim.Optimizer):\n",
        "        raise TypeError(\"Input 'optimizer' must be a torch.optim.Optimizer instance.\")\n",
        "\n",
        "    # --- Step 1: Compute total loss ---\n",
        "    # The total loss is the objective function to be minimized. It combines the\n",
        "    # goal of fitting the data (prediction_loss) with the goal of controlling\n",
        "    # model complexity (regularization_loss).\n",
        "    total_loss = prediction_loss + regularization_loss\n",
        "\n",
        "    # --- Step 2: Compute gradients via backpropagation ---\n",
        "\n",
        "    # Before computing new gradients, it is essential to zero out any gradients\n",
        "    # that may have accumulated from a previous step. PyTorch accumulates\n",
        "    # gradients by default on subsequent .backward() calls.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # This is the core of automatic differentiation. It computes the gradient of\n",
        "    # `total_loss` with respect to every parameter in the model that has\n",
        "    # `requires_grad=True` and was involved in the loss computation.\n",
        "    # The computed gradients are stored in the `.grad` attribute of each parameter.\n",
        "    total_loss.backward()\n",
        "\n",
        "    # --- Step 3: Update parameters using the Adam optimizer ---\n",
        "\n",
        "    # The optimizer.step() function updates the value of each parameter based on\n",
        "    # its `.grad` attribute and the optimizer's internal logic (e.g., the\n",
        "    # Adam update rule, including momentum and adaptive learning rates).\n",
        "    optimizer.step()\n",
        "\n",
        "    # Return the scalar value of the total loss for logging.\n",
        "    # .item() detaches the tensor from the computation graph and returns a\n",
        "    # standard Python float.\n",
        "    return total_loss.item()\n"
      ],
      "metadata": {
        "id": "7wDJ2sQvz-Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16 — Implement the chronological training loop over epochs\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Implement the chronological training loop over epochs\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Implementation of the Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def train_model(\n",
        "    multivector_matrix: np.ndarray,\n",
        "    ground_truth: np.ndarray,\n",
        "    valid_mask: np.ndarray,\n",
        "    parameters: Dict[str, nn.Parameter],\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> Tuple[Dict[str, nn.Parameter], List[Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the chronological training loop over epochs.\n",
        "\n",
        "    This function implements the full training process as specified in the paper,\n",
        "    adhering to a strict chronological, single-step update policy. This corrected\n",
        "    version ensures that for each valid time step `t`, the forward pass uses\n",
        "    parameters updated from step `t-1`, and the subsequent backward pass\n",
        "    updates the parameters before proceeding to step `t+1`.\n",
        "\n",
        "    This is achieved by iterating through each time step sequentially within each\n",
        "    epoch and performing a full forward/backward pass for each one. Historical\n",
        "    key/value projections are stored and managed within each epoch to correctly\n",
        "    construct the attention statistics causally.\n",
        "\n",
        "    Args:\n",
        "        multivector_matrix (np.ndarray):\n",
        "            The (T, 11) NumPy array of multivector embeddings.\n",
        "        ground_truth (np.ndarray):\n",
        "            A 1D NumPy array of shape (T,) with the target charge-off rates.\n",
        "        valid_mask (np.ndarray):\n",
        "            A 1D boolean NumPy array of shape (T,) indicating valid time steps.\n",
        "        parameters (Dict[str, nn.Parameter]):\n",
        "            Dictionary of all learnable model parameters.\n",
        "        optimizer (torch.optim.Optimizer):\n",
        "            The configured optimizer instance.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, nn.Parameter], List[Dict[str, float]]]:\n",
        "            - The dictionary of trained model parameters.\n",
        "            - A list of dictionaries, where each dictionary contains the\n",
        "              logged metrics (e.g., average loss) for an epoch.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    # Retrieve training parameters from the configuration.\n",
        "    train_config = model_config['training']\n",
        "    num_epochs = train_config['num_epochs']\n",
        "    lookback_horizon = model_config['data_processing']['lookback_horizon_L']\n",
        "    epsilon = model_config['architecture']['attention_stability_epsilon']\n",
        "\n",
        "    # Convert NumPy inputs to PyTorch tensors.\n",
        "    M = torch.tensor(multivector_matrix, dtype=torch.float32)\n",
        "    y = torch.tensor(ground_truth, dtype=torch.float32)\n",
        "    mask = torch.from_numpy(valid_mask).bool()\n",
        "\n",
        "    T = M.shape[0]\n",
        "    training_history = []\n",
        "\n",
        "    # --- Outer Loop: Epochs ---\n",
        "    epoch_pbar = tqdm(range(num_epochs), desc=\"Training Epochs\")\n",
        "    for epoch in epoch_pbar:\n",
        "        # Set model parameters to training mode.\n",
        "        for param in parameters.values():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Initialize epoch-specific state.\n",
        "        # These lists will store the history of K and V projections within this epoch.\n",
        "        k_history, v_history = [], []\n",
        "        epoch_total_loss, epoch_pred_loss, epoch_reg_loss = 0.0, 0.0, 0.0\n",
        "        num_valid_steps = 0\n",
        "\n",
        "        # --- Inner Loop: Chronological Time Steps ---\n",
        "        for t in range(T):\n",
        "            # --- Per-Step Forward Pass (Part 1: Projections) ---\n",
        "            # Extract the multivector for the current time step.\n",
        "            m_t = M[t]\n",
        "\n",
        "            # Project M_t to get the current q_t, k_t, v_t using current parameters.\n",
        "            # This must be done inside the loop as parameters change at every step.\n",
        "            q_t = shifted_leaky_relu(torch.matmul(m_t, parameters['W_Q'].t()), model_config)\n",
        "            k_t = shifted_leaky_relu(torch.matmul(m_t, parameters['W_K'].t()), model_config)\n",
        "            v_t = torch.matmul(m_t, parameters['W_V'].t())\n",
        "\n",
        "            # Store the historical projections.\n",
        "            # CRITICAL: .detach() is used to cut off the gradient history. We only\n",
        "            # need the *values* of past keys/values, not the graph that created them.\n",
        "            # Failing to do this would exhaust memory over long sequences.\n",
        "            k_history.append(k_t.detach())\n",
        "            v_history.append(v_t.detach())\n",
        "\n",
        "            # --- Masking ---\n",
        "            # If the current step is invalid (e.g., warm-up), we still perform the\n",
        "            # projection to populate the history, but we skip the loss and update.\n",
        "            if not mask[t]:\n",
        "                continue\n",
        "\n",
        "            num_valid_steps += 1\n",
        "\n",
        "            # --- Per-Step Forward Pass (Part 2: Attention) ---\n",
        "            # Define the lookback window for the current step `t`.\n",
        "            start_idx = max(0, t - lookback_horizon)\n",
        "            end_idx = t\n",
        "\n",
        "            # Slice the history to get the keys and values for the window [t-L, t-1].\n",
        "            k_window = torch.stack(k_history[start_idx:end_idx])\n",
        "            v_window = torch.stack(v_history[start_idx:end_idx])\n",
        "\n",
        "            # Compute S_t = Σ K_τ V_τ^T for the window.\n",
        "            s_t = torch.sum(torch.bmm(k_window.unsqueeze(2), v_window.unsqueeze(1)), dim=0)\n",
        "\n",
        "            # Compute Z_t = Σ K_τ for the window.\n",
        "            z_t = torch.sum(k_window, dim=0)\n",
        "\n",
        "            # Compute attended context O_t.\n",
        "            numerator = torch.matmul(q_t.unsqueeze(0), s_t).squeeze(0)\n",
        "            denominator = torch.dot(q_t, z_t) + epsilon\n",
        "            o_t = numerator / denominator\n",
        "\n",
        "            # Compute prediction ŷ_t.\n",
        "            y_hat_t = mlp_forward_pass(o_t.unsqueeze(0), parameters).squeeze(0)\n",
        "\n",
        "            # --- Per-Step Loss and Backward Pass ---\n",
        "            # Compute prediction loss for the single time step.\n",
        "            pred_loss = F.mse_loss(y_hat_t, y[t])\n",
        "\n",
        "            # Compute regularization loss (independent of t).\n",
        "            reg_loss = compute_regularization_loss(parameters, model_config)\n",
        "\n",
        "            # Perform the backward pass and parameter update.\n",
        "            total_loss_item = perform_training_step(pred_loss, reg_loss, optimizer)\n",
        "\n",
        "            # Accumulate losses for epoch-level logging.\n",
        "            epoch_total_loss += total_loss_item\n",
        "            epoch_pred_loss += pred_loss.item()\n",
        "            epoch_reg_loss += reg_loss.item()\n",
        "\n",
        "        # --- End of Epoch Logging ---\n",
        "        avg_total_loss = epoch_total_loss / num_valid_steps if num_valid_steps > 0 else 0.0\n",
        "        avg_pred_loss = epoch_pred_loss / num_valid_steps if num_valid_steps > 0 else 0.0\n",
        "        avg_reg_loss = epoch_reg_loss / num_valid_steps if num_valid_steps > 0 else 0.0\n",
        "\n",
        "        epoch_log = {\n",
        "            'epoch': epoch + 1,\n",
        "            'avg_total_loss': avg_total_loss,\n",
        "            'avg_prediction_loss': avg_pred_loss,\n",
        "            'avg_regularization_loss': avg_reg_loss,\n",
        "        }\n",
        "        training_history.append(epoch_log)\n",
        "        epoch_pbar.set_postfix(loss=f\"{avg_total_loss:.6f}\")\n",
        "\n",
        "    return parameters, training_history\n"
      ],
      "metadata": {
        "id": "PxR2kUOo06El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17 — Compute temporal attribution (normalized attention weights w_τ,T)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Compute temporal attribution (normalized attention weights w_τ,T)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_temporal_attribution(\n",
        "    multivector_matrix: np.ndarray,\n",
        "    parameters: Dict[str, nn.Parameter],\n",
        "    valid_mask: np.ndarray,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the temporal attribution as normalized attention weights.\n",
        "\n",
        "    This function is a post-training analysis tool that calculates which\n",
        "    historical time steps the trained model pays attention to when processing\n",
        "    each point in the time series. It produces a matrix of weights `w_{τ,T}`\n",
        "    where `T` is the current time step and `τ` indexes the lookback window.\n",
        "\n",
        "    The process involves:\n",
        "    1. A full forward pass to get the final Query and Key projections.\n",
        "    2. Vectorized computation of raw attention scores (Q_T^T * K_τ).\n",
        "    3. Stable normalization of scores using the Softmax function.\n",
        "\n",
        "    Args:\n",
        "        multivector_matrix (np.ndarray):\n",
        "            The (T, 11) NumPy array of multivector embeddings.\n",
        "        parameters (Dict[str, nn.Parameter]):\n",
        "            The dictionary of trained model parameters.\n",
        "        valid_mask (np.ndarray):\n",
        "            A 1D boolean NumPy array of shape (T,) indicating valid time steps.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray:\n",
        "            A NumPy array of shape (T, L) containing the normalized attention\n",
        "            weights, where L is the lookback horizon. `attention[T, l]` is\n",
        "            the weight on the `l`-th lag for time step `T`.\n",
        "    \"\"\"\n",
        "    # --- Setup and Input Validation ---\n",
        "    if not isinstance(multivector_matrix, np.ndarray):\n",
        "        raise TypeError(\"Input 'multivector_matrix' must be a NumPy array.\")\n",
        "\n",
        "    # Set model to evaluation mode (important if dropout/batchnorm were used).\n",
        "    for param in parameters.values():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Retrieve necessary configuration.\n",
        "    lookback_horizon = model_config['data_processing']['lookback_horizon_L']\n",
        "    T = multivector_matrix.shape[0]\n",
        "\n",
        "    # --- Step 1: Full Forward Pass for Q and K Projections ---\n",
        "    # Re-compute the Q, K, V projections using the final trained parameters.\n",
        "    # We do this once for the entire dataset.\n",
        "    qkv_projections = compute_qkv_projections(\n",
        "        multivector_matrix, parameters, model_config\n",
        "    )\n",
        "    queries = qkv_projections['queries']\n",
        "    keys = qkv_projections['keys']\n",
        "\n",
        "    # --- Step 2: Prepare Lookback Windows for Keys ---\n",
        "    # To compute attention scores for each time `T` over its window `[T-L, T-1]`,\n",
        "    # we first create a lagged version of the keys tensor.\n",
        "    keys_lagged = F.pad(keys, (0, 0, 1, 0))[:-1]\n",
        "\n",
        "    # Use `unfold` to create a sliding window view of the lagged keys.\n",
        "    # This is a highly efficient, memory-friendly way to create the lookback windows.\n",
        "    # The result is a tensor of shape (T - L + 1, d_h, L).\n",
        "    key_windows_unfolded = keys_lagged.unfold(\n",
        "        dimension=0, size=lookback_horizon, step=1\n",
        "    )\n",
        "    # We need to permute the dimensions to get (T - L + 1, L, d_h) for einsum.\n",
        "    key_windows = key_windows_unfolded.permute(0, 2, 1)\n",
        "\n",
        "    # The unfold operation shortens the time series. We need to align it with\n",
        "    # the queries, which start from the first valid time step.\n",
        "    # We pad the start to align `key_windows[t]` with `queries[t]`.\n",
        "    num_pad_timesteps = T - key_windows.shape[0]\n",
        "    padding = torch.zeros(num_pad_timesteps, lookback_horizon, keys.shape[1])\n",
        "    key_windows_aligned = torch.cat([padding, key_windows], dim=0)\n",
        "\n",
        "    # --- Step 3: Compute Raw Attention Scores via Einsum ---\n",
        "    # Equation: a_{T,τ} = Q_T^T * K_τ\n",
        "    # We use Einstein summation for a clean and efficient batch dot product.\n",
        "    # 't h, t l h -> t l' means: for each time `t`, multiply the query vector `Q[t]`\n",
        "    # of shape (h,) with each of the `L` key vectors in the window `K_window[t]`\n",
        "    # of shape (L, h), resulting in a vector of `L` raw scores.\n",
        "    raw_scores = torch.einsum('th,tlh->tl', queries, key_windows_aligned)\n",
        "\n",
        "    # --- Step 4: Normalize Scores using Softmax ---\n",
        "    # Softmax provides a numerically stable way to convert raw scores into a\n",
        "    # probability distribution that sums to 1, which is ideal for interpretation.\n",
        "    # w_{τ,T} = exp(a_{T,τ}) / Σ_τ exp(a_{T,τ})\n",
        "    attention_weights = F.softmax(raw_scores, dim=-1)\n",
        "\n",
        "    # --- Final Masking and Output ---\n",
        "    # Ensure weights for invalid time steps are zeroed out.\n",
        "    mask_tensor = torch.from_numpy(valid_mask).bool()\n",
        "    attention_weights[~mask_tensor] = 0.0\n",
        "\n",
        "    # Detach from the graph and convert to NumPy for analysis.\n",
        "    return attention_weights.numpy()\n"
      ],
      "metadata": {
        "id": "frNyJFuv11z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18 — Compute geometric occlusion attribution Δ^B ŷ_T\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Compute geometric occlusion attribution Δ^B ŷ_T\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Helper: Full Forward Pass\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_full_forward_pass(\n",
        "    multivector_matrix: np.ndarray,\n",
        "    parameters: Dict[str, nn.Parameter],\n",
        "    valid_mask: np.ndarray,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Executes a complete forward pass of the model from embeddings to predictions.\n",
        "\n",
        "    This helper function chains together all the necessary forward pass components\n",
        "    to generate predictions from a given multivector matrix. It is used to get\n",
        "    both the baseline predictions and the occluded predictions for attribution.\n",
        "\n",
        "    Args:\n",
        "        multivector_matrix (np.ndarray): The (T, 11) input multivector embeddings.\n",
        "        parameters (Dict[str, nn.Parameter]): The trained model parameters.\n",
        "        valid_mask (np.ndarray): The boolean validity mask.\n",
        "        model_config (Dict[str, Any]): The validated model configuration.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A 1D tensor of shape (T,) containing the predictions.\n",
        "    \"\"\"\n",
        "    # 1. Compute Q, K, V projections.\n",
        "    qkv = compute_qkv_projections(multivector_matrix, parameters, model_config)\n",
        "\n",
        "    # 2. Compute attention statistics S and Z.\n",
        "    stats = compute_attention_statistics(qkv, torch.from_numpy(valid_mask), model_config)\n",
        "\n",
        "    # 3. Compute the attended context O.\n",
        "    context = compute_attended_context(qkv, stats, torch.from_numpy(valid_mask), model_config)\n",
        "\n",
        "    # 4. Compute final predictions ŷ.\n",
        "    predictions = mlp_forward_pass(context, parameters)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_geometric_attribution(\n",
        "    multivector_matrix: np.ndarray,\n",
        "    parameters: Dict[str, nn.Parameter],\n",
        "    valid_mask: np.ndarray,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes geometric attribution via controlled occlusion.\n",
        "\n",
        "    This function quantifies the contribution of each geometric component\n",
        "    (scalar, vector, bivector) to the final prediction. It does this by\n",
        "    systematically zeroing out each component in the multivector embedding\n",
        "    and measuring the resulting change in the model's output.\n",
        "\n",
        "    The attribution for a block B is calculated as:\n",
        "    Δ^B ŷ_T = ŷ_T (baseline) - ŷ_T (with block B occluded)\n",
        "\n",
        "    Args:\n",
        "        multivector_matrix (np.ndarray):\n",
        "            The (T, 11) NumPy array of multivector embeddings.\n",
        "        parameters (Dict[str, nn.Parameter]):\n",
        "            The dictionary of trained model parameters.\n",
        "        valid_mask (np.ndarray):\n",
        "            A 1D boolean NumPy array of shape (T,) indicating valid time steps.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]:\n",
        "            A dictionary where keys are the block names ('scalar', 'vector',\n",
        "            'bivector') and values are 1D NumPy arrays of shape (T,)\n",
        "            containing the attribution scores for each time step.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    # Set model to evaluation mode.\n",
        "    for param in parameters.values():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # --- Step 1: Define Geometric Blocks in M_t ---\n",
        "    # Programmatically determine the column indices for each block from the config.\n",
        "    component_order = model_config['architecture']['component_order']\n",
        "    block_indices = {\n",
        "        'scalar': [i for i, name in enumerate(component_order) if name == 'scalar'],\n",
        "        'vector': [i for i, name in enumerate(component_order) if '(' in name and '^' not in name],\n",
        "        'bivector': [i for i, name in enumerate(component_order) if '^' in name]\n",
        "    }\n",
        "\n",
        "    # --- Baseline Prediction ---\n",
        "    # First, compute the baseline predictions with the original, unoccluded data.\n",
        "    baseline_predictions = _run_full_forward_pass(\n",
        "        multivector_matrix, parameters, valid_mask, model_config\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Perform Occlusion for Each Block ---\n",
        "    attributions = {}\n",
        "    for block_name, indices in block_indices.items():\n",
        "        # Create a copy of the multivector matrix for occlusion.\n",
        "        occluded_matrix = multivector_matrix.copy()\n",
        "\n",
        "        # Zero out the columns corresponding to the current geometric block.\n",
        "        occluded_matrix[:, indices] = 0.0\n",
        "\n",
        "        # Run a full forward pass with the occluded data.\n",
        "        occluded_predictions = _run_full_forward_pass(\n",
        "            occluded_matrix, parameters, valid_mask, model_config\n",
        "        )\n",
        "\n",
        "        # --- Step 3: Compute the Contribution of Block B ---\n",
        "        # Equation: Δ^B ŷ_T = ŷ_T - ŷ_T_occluded\n",
        "        # The attribution is the change in prediction caused by removing the block.\n",
        "        block_attribution = baseline_predictions - occluded_predictions\n",
        "\n",
        "        # Store the results, detaching from the graph and converting to NumPy.\n",
        "        attributions[block_name] = block_attribution.detach().numpy()\n",
        "\n",
        "    return attributions\n"
      ],
      "metadata": {
        "id": "meXZlM563mcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19 — Compute component magnitudes of O_t for regime diagnostics\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Compute component magnitudes of O_t for regime diagnostics\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Helper: Forward Pass to Attended Context\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _run_forward_pass_to_context(\n",
        "    multivector_matrix: np.ndarray,\n",
        "    parameters: Dict[str, nn.Parameter],\n",
        "    valid_mask: np.ndarray,\n",
        "    model_config: Dict[str, Any]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Executes a forward pass up to the attended context vector `O_t`.\n",
        "\n",
        "    This helper function is an efficient version of the full forward pass,\n",
        "    stopping before the MLP head. It is used to get the baseline and occluded\n",
        "    context vectors for attribution and magnitude analysis.\n",
        "\n",
        "    Args:\n",
        "        multivector_matrix (np.ndarray): The (T, 11) input multivector embeddings.\n",
        "        parameters (Dict[str, nn.Parameter]): The trained model parameters.\n",
        "        valid_mask (np.ndarray): The boolean validity mask.\n",
        "        model_config (Dict[str, Any]): The validated model configuration.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The (T, d_h) tensor of attended context vectors.\n",
        "    \"\"\"\n",
        "    # 1. Compute Q, K, V projections.\n",
        "    qkv = compute_qkv_projections(multivector_matrix, parameters, model_config)\n",
        "\n",
        "    # 2. Compute attention statistics S and Z.\n",
        "    stats = compute_attention_statistics(qkv, torch.from_numpy(valid_mask), model_config)\n",
        "\n",
        "    # 3. Compute the attended context O.\n",
        "    context = compute_attended_context(qkv, stats, torch.from_numpy(valid_mask), model_config)\n",
        "\n",
        "    return context\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_component_magnitudes(\n",
        "    multivector_matrix: np.ndarray,\n",
        "    parameters: Dict[str, nn.Parameter],\n",
        "    valid_mask: np.ndarray,\n",
        "    model_config: Dict[str, Any],\n",
        "    date_index: pd.DatetimeIndex\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the magnitude of influence of each geometric component on the context vector.\n",
        "\n",
        "    This diagnostic function quantifies how much the scalar, vector, and bivector\n",
        "    components of the input multivector `M_t` each contribute to the final\n",
        "    attended context vector `O_t`. It uses controlled occlusion, measuring the\n",
        "    L2 norm of the change in `O_t` when each component block is zeroed out.\n",
        "\n",
        "    Magnitude(B) = || O_t (baseline) - O_t (with block B occluded) ||_2\n",
        "\n",
        "    Args:\n",
        "        multivector_matrix (np.ndarray):\n",
        "            The (T, 11) NumPy array of multivector embeddings.\n",
        "        parameters (Dict[str, nn.Parameter]):\n",
        "            The dictionary of trained model parameters.\n",
        "        valid_mask (np.ndarray):\n",
        "            A 1D boolean NumPy array of shape (T,) indicating valid time steps.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The validated model configuration dictionary.\n",
        "        date_index (pd.DatetimeIndex):\n",
        "            The DatetimeIndex from the cleansed DataFrame, for the output.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A DataFrame with the provided `date_index` and columns\n",
        "            ['scalar_magnitude', 'vector_magnitude', 'bivector_magnitude'],\n",
        "            containing the time series of component influence magnitudes.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    # Set model to evaluation mode.\n",
        "    for param in parameters.values():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # --- Step 1: Define Geometric Blocks in M_t ---\n",
        "    component_order = model_config['architecture']['component_order']\n",
        "    block_indices = {\n",
        "        'scalar': [i for i, name in enumerate(component_order) if name == 'scalar'],\n",
        "        'vector': [i for i, name in enumerate(component_order) if '(' in name and '^' not in name],\n",
        "        'bivector': [i for i, name in enumerate(component_order) if '^' in name]\n",
        "    }\n",
        "\n",
        "    # --- Baseline Context Vector ---\n",
        "    # Compute the baseline context vectors with the original, unoccluded data.\n",
        "    baseline_context = _run_forward_pass_to_context(\n",
        "        multivector_matrix, parameters, valid_mask, model_config\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Perform Occlusion and Compute Magnitudes ---\n",
        "    magnitudes = {}\n",
        "    for block_name, indices in block_indices.items():\n",
        "        # Create a copy of the multivector matrix for occlusion.\n",
        "        occluded_matrix = multivector_matrix.copy()\n",
        "\n",
        "        # Zero out the columns corresponding to the current geometric block.\n",
        "        occluded_matrix[:, indices] = 0.0\n",
        "\n",
        "        # Run a forward pass with the occluded data to get the occluded context.\n",
        "        occluded_context = _run_forward_pass_to_context(\n",
        "            occluded_matrix, parameters, valid_mask, model_config\n",
        "        )\n",
        "\n",
        "        # --- Step 3: Compute the Magnitude of the Change ---\n",
        "        # The contribution is the difference between the baseline and occluded context.\n",
        "        context_difference = baseline_context - occluded_context\n",
        "\n",
        "        # The magnitude is the L2 norm of this difference vector, computed for each time step.\n",
        "        # torch.linalg.norm(..., dim=1) computes the vector norm for each row.\n",
        "        block_magnitude = torch.linalg.norm(context_difference, ord=2, dim=1)\n",
        "\n",
        "        # Store the results, detaching from the graph and converting to NumPy.\n",
        "        magnitudes[f\"{block_name}_magnitude\"] = block_magnitude.detach().numpy()\n",
        "\n",
        "    # --- Final Output Assembly ---\n",
        "    # Create a pandas DataFrame for easy plotting and analysis.\n",
        "    magnitudes_df = pd.DataFrame(magnitudes, index=date_index)\n",
        "\n",
        "    # Ensure magnitudes for invalid time steps are set to NaN for clarity in plots.\n",
        "    magnitudes_df.loc[~valid_mask] = np.nan\n",
        "\n",
        "    return magnitudes_df\n"
      ],
      "metadata": {
        "id": "TzthHe8J4cG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20 — Perform PCA on attended context sequence {O_t} for trajectory analysis\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Perform PCA on attended context sequence {O_t} for trajectory analysis\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def perform_pca_on_context(\n",
        "    attended_context: torch.Tensor,\n",
        "    ground_truth: np.ndarray,\n",
        "    valid_mask: np.ndarray,\n",
        "    date_index: pd.DatetimeIndex\n",
        ") -> Tuple[pd.DataFrame, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Performs PCA on the attended context vectors to visualize the system's trajectory.\n",
        "\n",
        "    This function takes the time series of high-dimensional context vectors (`O_t`),\n",
        "    applies Principal Component Analysis (PCA) to reduce them to two dimensions,\n",
        "    and prepares the results for plotting. This is a key diagnostic for\n",
        "    visualizing economic regimes, cycles, and hysteresis, as shown in Figure 2\n",
        "    of the paper.\n",
        "\n",
        "    Args:\n",
        "        attended_context (torch.Tensor):\n",
        "            The (T, d_h) tensor of context vectors from a full forward pass.\n",
        "        ground_truth (np.ndarray):\n",
        "            A 1D NumPy array of shape (T,) with the target charge-off rates.\n",
        "        valid_mask (np.ndarray):\n",
        "            A 1D boolean NumPy array of shape (T,) indicating valid time steps.\n",
        "        date_index (pd.DatetimeIndex):\n",
        "            The DatetimeIndex corresponding to the full time series.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, np.ndarray]:\n",
        "            - A pandas DataFrame with columns ['PC1', 'PC2', 'charge_off_rate'],\n",
        "              indexed by the dates of the valid time steps, ready for plotting.\n",
        "            - A 1D NumPy array containing the explained variance ratio for\n",
        "              the first two principal components.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(attended_context, torch.Tensor):\n",
        "        raise TypeError(\"Input 'attended_context' must be a torch.Tensor.\")\n",
        "    if not (len(attended_context) == len(ground_truth) == len(valid_mask) == len(date_index)):\n",
        "        raise ValueError(\"All inputs must have the same length (T).\")\n",
        "\n",
        "    # --- Step 1: Assemble and Center the Attended Context Matrix ---\n",
        "\n",
        "    # Filter the data to include only the valid time steps.\n",
        "    # This is critical to ensure the PCA is not distorted by warm-up periods.\n",
        "    valid_context = attended_context[valid_mask].detach().numpy()\n",
        "    valid_dates = date_index[valid_mask]\n",
        "    valid_ground_truth = ground_truth[valid_mask]\n",
        "\n",
        "    # Handle the edge case where there are not enough samples for PCA.\n",
        "    if valid_context.shape[0] < 2:\n",
        "        # Return empty results if there are fewer than 2 data points.\n",
        "        empty_df = pd.DataFrame(columns=['PC1', 'PC2', 'charge_off_rate'])\n",
        "        return empty_df, np.array([0.0, 0.0])\n",
        "\n",
        "    # Center the data by subtracting the mean of each feature (column).\n",
        "    # This is a standard and necessary preprocessing step for PCA.\n",
        "    mean_vec = np.mean(valid_context, axis=0)\n",
        "    centered_context = valid_context - mean_vec\n",
        "\n",
        "    # --- Step 2: Compute PCA via Singular Value Decomposition (SVD) ---\n",
        "\n",
        "    # Use SVD to decompose the centered data matrix.\n",
        "    # U, S, Vh = svd(X_centered), where Vh contains the principal components as rows.\n",
        "    # `full_matrices=False` is more efficient.\n",
        "    try:\n",
        "        U, S, Vh = np.linalg.svd(centered_context, full_matrices=False)\n",
        "    except np.linalg.LinAlgError as e:\n",
        "        raise RuntimeError(f\"SVD computation failed, which may indicate a problem with the input data. Original error: {e}\")\n",
        "\n",
        "    # The principal components (eigenvectors of the covariance matrix) are the rows of Vh.\n",
        "    # We select the first two components.\n",
        "    principal_components = Vh[:2, :]\n",
        "\n",
        "    # --- Enforce a deterministic sign convention for reproducibility ---\n",
        "    # The sign of eigenvectors is arbitrary. We enforce a rule to make the\n",
        "    # output consistent across runs: flip the sign of a component if the element\n",
        "    # with the largest absolute value is negative.\n",
        "    for i in range(principal_components.shape[0]):\n",
        "        max_abs_idx = np.argmax(np.abs(principal_components[i, :]))\n",
        "        if principal_components[i, max_abs_idx] < 0:\n",
        "            principal_components[i, :] *= -1\n",
        "\n",
        "    # --- Step 3: Project Data and Prepare for Visualization ---\n",
        "\n",
        "    # Project the centered data onto the first two principal components.\n",
        "    # This is a matrix multiplication: X_centered @ Vh.T\n",
        "    projected_data = centered_context @ principal_components.T\n",
        "\n",
        "    # Calculate the explained variance ratio for the first two components.\n",
        "    explained_variance = S**2 / np.sum(S**2)\n",
        "    explained_variance_ratio = explained_variance[:2]\n",
        "\n",
        "    # Assemble the final DataFrame for easy plotting.\n",
        "    pca_df = pd.DataFrame({\n",
        "        'PC1': projected_data[:, 0],\n",
        "        'PC2': projected_data[:, 1],\n",
        "        'charge_off_rate': valid_ground_truth\n",
        "    }, index=valid_dates)\n",
        "\n",
        "    return pca_df, explained_variance_ratio\n"
      ],
      "metadata": {
        "id": "Sj-bdj2E5jT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21 — Create an end-to-end orchestrator function for the full pipeline\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Create an end-to-end orchestrator function for the full pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Helper: Robust Recursive Configuration Override\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _apply_config_overrides(\n",
        "    base_config: Dict[str, Any],\n",
        "    overrides: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Safely and recursively applies overrides to a deep copy of the base configuration.\n",
        "\n",
        "    This function creates a deep copy of the base configuration dictionary and\n",
        "    updates its values with any provided in the `overrides` dictionary. The update\n",
        "    is recursive, meaning it can update values in nested dictionaries, which is\n",
        "    essential for modifying specific hyperparameters (e.g., learning_rate_eta\n",
        "    within the 'training' section).\n",
        "\n",
        "    Args:\n",
        "        base_config (Dict[str, Any]):\n",
        "            The original, validated model configuration dictionary.\n",
        "        overrides (Dict[str, Any]):\n",
        "            A dictionary containing key-value pairs to override in the base\n",
        "            configuration. Keys can correspond to nested parameters.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A new configuration dictionary with the overrides applied.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If an override key does not exist in any nested dictionary\n",
        "                  within the base configuration.\n",
        "    \"\"\"\n",
        "    # Create a deep copy to ensure the original configuration object is not mutated.\n",
        "    config = copy.deepcopy(base_config)\n",
        "\n",
        "    # Iterate through each key-value pair in the provided overrides.\n",
        "    for key, value in overrides.items():\n",
        "        # Flag to track if the override key was found and applied.\n",
        "        key_found = False\n",
        "        # Iterate through the top-level sections of the configuration (e.g., 'training').\n",
        "        for section in config.values():\n",
        "            # Check if the section is a dictionary and contains the override key.\n",
        "            if isinstance(section, dict) and key in section:\n",
        "                # Apply the override.\n",
        "                section[key] = value\n",
        "                # Mark the key as found.\n",
        "                key_found = True\n",
        "                # Break the inner loop once the key is found and updated.\n",
        "                break\n",
        "        # If the key was not found in any section, it's an invalid override.\n",
        "        if not key_found:\n",
        "            raise KeyError(f\"Override key '{key}' not found in any section of the model configuration.\")\n",
        "\n",
        "    # Return the new configuration with overrides applied.\n",
        "    return config\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Implementation of the Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_full_analysis_pipeline(\n",
        "    consolidated_df_raw: pd.DataFrame,\n",
        "    model_config: Dict[str, Any],\n",
        "    random_seed: int = 42,\n",
        "    verbose: bool = True,\n",
        "    **kwargs: Any\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end research pipeline.\n",
        "\n",
        "    This master function is the single entry point for a complete, reproducible\n",
        "    analysis. It executes the entire workflow with escalated rigor:\n",
        "    1. Applies configuration overrides and sets a random seed for reproducibility.\n",
        "    2. Validates all inputs (raw data and final configuration).\n",
        "    3. Runs the full data preprocessing pipeline from cleansing to embedding.\n",
        "    4. Initializes model parameters and trains the model using the corrected\n",
        "       chronological loop.\n",
        "    5. Performs an efficient, single forward pass to generate all tensors\n",
        "       needed for post-hoc analysis.\n",
        "    6. Runs all diagnostic and attribution analyses on the trained model.\n",
        "    7. Compiles a comprehensive, structured dictionary of all results and artifacts.\n",
        "\n",
        "    Args:\n",
        "        consolidated_df_raw (pd.DataFrame):\n",
        "            The raw, unprocessed quarterly economic data, conforming to the\n",
        "            required structure (DatetimeIndex, specific columns).\n",
        "        model_config (Dict[str, Any]):\n",
        "            The base model configuration dictionary.\n",
        "        random_seed (int, optional):\n",
        "            A seed for the PyTorch random number generator to ensure\n",
        "            reproducible model initialization and training. Defaults to 42.\n",
        "        verbose (bool, optional):\n",
        "            If True, prints progress updates for each major stage of the\n",
        "            pipeline to the console. Defaults to True.\n",
        "        **kwargs (Any):\n",
        "            Optional keyword arguments to override parameters in `model_config`.\n",
        "            For example, `learning_rate_eta=1e-5`.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A comprehensive dictionary containing all artifacts of the analysis,\n",
        "            structured into metadata, configuration, data, model, and analysis\n",
        "            results for easy access and archival.\n",
        "    \"\"\"\n",
        "    # --- Stage 1: Setup and Validation ---\n",
        "    if verbose:\n",
        "        print(\"Stage 1: Configuring environment and validating inputs...\")\n",
        "\n",
        "    # Apply any user-provided keyword argument overrides to a deep copy of the configuration.\n",
        "    final_config = _apply_config_overrides(model_config, kwargs)\n",
        "\n",
        "    # Set the global random seed for PyTorch to ensure that all random operations\n",
        "    # (e.g., parameter initialization) are deterministic.\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    # Validate the structure and content of the raw data DataFrame.\n",
        "    validate_raw_data(consolidated_df_raw)\n",
        "    # Validate the final, potentially modified, configuration dictionary.\n",
        "    validate_model_config(final_config)\n",
        "\n",
        "    if verbose:\n",
        "        print(\" -> Inputs validated successfully.\")\n",
        "\n",
        "    # --- Stage 2: Data Preprocessing Pipeline ---\n",
        "    if verbose:\n",
        "        print(\"Stage 2: Executing data preprocessing pipeline...\")\n",
        "\n",
        "    # Task 3: Cleanse data (handle NaNs, duplicates, etc.) and create the initial validity mask.\n",
        "    df_clean, mask, report = cleanse_and_prepare_data(consolidated_df_raw, final_config)\n",
        "\n",
        "    # Task 4: Apply log-difference transformations to create growth rate features.\n",
        "    features_pre_std, mask = apply_growth_transformations(df_clean, mask)\n",
        "\n",
        "    # Task 5: Apply rolling-window standardization to all features.\n",
        "    features_std, mask = apply_rolling_standardization(features_pre_std, mask, final_config)\n",
        "\n",
        "    # Task 6: Construct the geometric algebra multivector embedding from standardized features.\n",
        "    multivector_embedding = construct_multivector_embedding(features_std, final_config)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\" -> Preprocessing complete. Final embedding shape: {multivector_embedding.shape}\")\n",
        "\n",
        "    # --- Stage 3: Model Initialization and Training ---\n",
        "    if verbose:\n",
        "        print(\"Stage 3: Initializing and training the model...\")\n",
        "\n",
        "    # Task 7: Initialize all learnable parameters and the Adam optimizer.\n",
        "    params, optimizer = initialize_model_and_optimizer(final_config, random_seed)\n",
        "\n",
        "    # Extract the ground truth target variable for training.\n",
        "    ground_truth_np = df_clean['CORCACBS'].values\n",
        "\n",
        "    # Task 16: Run the full chronological training loop using the corrected function.\n",
        "    trained_params, history = train_model(\n",
        "        multivector_embedding, ground_truth_np, mask, params, optimizer, final_config\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(\" -> Training complete.\")\n",
        "\n",
        "    # --- Stage 4: Post-Hoc Analysis ---\n",
        "    if verbose:\n",
        "        print(\"Stage 4: Generating post-hoc analysis and diagnostics...\")\n",
        "\n",
        "    # Perform ONE efficient, full forward pass using the final trained parameters.\n",
        "    # This avoids redundant computation in the subsequent analysis functions.\n",
        "    with torch.no_grad():  # Disable gradient computation for inference.\n",
        "        # Re-compute all intermediate tensors required for analysis.\n",
        "        qkv = compute_qkv_projections(multivector_embedding, trained_params, final_config)\n",
        "        stats = compute_attention_statistics(qkv, torch.from_numpy(mask), final_config)\n",
        "        context = compute_attended_context(qkv, stats, torch.from_numpy(mask), final_config)\n",
        "        predictions = mlp_forward_pass(context, trained_params)\n",
        "\n",
        "    # Task 17: Compute temporal attribution weights.\n",
        "    temporal_attributions = compute_temporal_attribution(\n",
        "        multivector_embedding, trained_params, mask, final_config\n",
        "    )\n",
        "\n",
        "    # Task 18: Compute geometric occlusion attribution.\n",
        "    geometric_attributions = compute_geometric_attribution(\n",
        "        multivector_embedding, trained_params, mask, final_config\n",
        "    )\n",
        "\n",
        "    # Task 19: Compute component magnitudes for regime diagnostics.\n",
        "    component_magnitudes_df = compute_component_magnitudes(\n",
        "        multivector_embedding, trained_params, mask, final_config, df_clean.index\n",
        "    )\n",
        "\n",
        "    # Task 20: Perform PCA on the attended context sequence.\n",
        "    pca_trajectory_df, explained_variance = perform_pca_on_context(\n",
        "        context, ground_truth_np, mask, df_clean.index\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(\" -> Analysis complete.\")\n",
        "\n",
        "    # --- Stage 5: Compile Final Results ---\n",
        "    if verbose:\n",
        "        print(\"Stage 5: Compiling final results package...\")\n",
        "\n",
        "    # Assemble all artifacts into a single, comprehensive, and well-structured dictionary.\n",
        "    results = {\n",
        "        'metadata': {\n",
        "            'random_seed': random_seed,\n",
        "            'cleaning_report': report,\n",
        "            'pca_explained_variance_ratio': explained_variance,\n",
        "        },\n",
        "        'config_final': final_config,\n",
        "        'data_artifacts': {\n",
        "            'df_clean': df_clean,\n",
        "            'valid_mask': mask,\n",
        "            'multivector_embedding': multivector_embedding,\n",
        "        },\n",
        "        'model_artifacts': {\n",
        "            'trained_parameters': trained_params,\n",
        "            'training_history': history,\n",
        "            'final_predictions_np': predictions.numpy(),\n",
        "        },\n",
        "        'analysis_artifacts': {\n",
        "            'qkv_projections': {k: v.numpy() for k, v in qkv.items()},\n",
        "            'attention_statistics': {k: v.numpy() for k, v in stats.items()},\n",
        "            'attended_context_np': context.numpy(),\n",
        "        },\n",
        "        'interpretability_results': {\n",
        "            'temporal_attributions': temporal_attributions,\n",
        "            'geometric_attributions': geometric_attributions,\n",
        "            'component_magnitudes_df': component_magnitudes_df,\n",
        "            'pca_trajectory_df': pca_trajectory_df,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Pipeline finished successfully.\")\n",
        "\n",
        "    # Return the complete results package.\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "6nAooLIV7JpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22 — Conduct robustness analysis via the orchestrator (hyperparameter sweeps)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Conduct robustness analysis via the orchestrator (hyperparameter sweeps)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def conduct_robustness_analysis(\n",
        "    consolidated_df_raw: pd.DataFrame,\n",
        "    model_config: Dict[str, Any],\n",
        "    hyperparameter_grid: Dict[str, List[Any]],\n",
        "    base_random_seed: int = 42\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Conducts a robustness analysis by running the full pipeline across a grid of hyperparameters.\n",
        "\n",
        "    This function systematically explores the model's sensitivity to different\n",
        "    hyperparameter choices. It iterates through every combination of parameters\n",
        "    defined in the `hyperparameter_grid`, running the complete end-to-end\n",
        "    analysis pipeline for each combination. The results from every run are\n",
        "    collected, providing a comprehensive dataset for evaluating the robustness\n",
        "    of the model's findings.\n",
        "\n",
        "    Args:\n",
        "        consolidated_df_raw (pd.DataFrame):\n",
        "            The raw, unprocessed quarterly economic data.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The base model configuration dictionary.\n",
        "        hyperparameter_grid (Dict[str, List[Any]]):\n",
        "            A dictionary where keys are hyperparameter names (e.g.,\n",
        "            'hidden_dimension_dh') and values are lists of the values to test.\n",
        "        base_random_seed (int, optional):\n",
        "            A base seed for the random number generator. Each run in the sweep\n",
        "            will use a deterministically derived seed (`base_seed + run_index`)\n",
        "            to ensure reproducibility. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]:\n",
        "            A list of the comprehensive results dictionaries. Each dictionary\n",
        "            corresponds to one hyperparameter combination and is augmented with\n",
        "            a 'hyperparameters' key detailing the configuration for that run.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Define Hyperparameter Grid for Sensitivity Analysis ---\n",
        "\n",
        "    # Get the names of the hyperparameters to be varied.\n",
        "    param_names = list(hyperparameter_grid.keys())\n",
        "    # Get the lists of values for each hyperparameter.\n",
        "    param_values = list(hyperparameter_grid.values())\n",
        "\n",
        "    # Use itertools.product to create the Cartesian product of all parameter values.\n",
        "    # This generates all unique combinations of hyperparameters.\n",
        "    run_configurations = list(product(*param_values))\n",
        "\n",
        "    # Initialize a list to store the results from all runs.\n",
        "    all_results = []\n",
        "\n",
        "    # --- Step 2: Run the Orchestrator for Each Configuration ---\n",
        "\n",
        "    # Use tqdm to create a progress bar for the entire hyperparameter sweep.\n",
        "    sweep_pbar = tqdm(\n",
        "        enumerate(run_configurations),\n",
        "        total=len(run_configurations),\n",
        "        desc=\"Hyperparameter Sweep\"\n",
        "    )\n",
        "\n",
        "    # Loop through each unique combination of hyperparameters.\n",
        "    for i, combo in sweep_pbar:\n",
        "        # Create a dictionary of the specific overrides for this run.\n",
        "        overrides = dict(zip(param_names, combo))\n",
        "\n",
        "        # Update the progress bar with the current set of parameters.\n",
        "        sweep_pbar.set_postfix(overrides)\n",
        "\n",
        "        # Generate a unique, deterministic random seed for this specific run.\n",
        "        # This ensures that the only difference between runs is the hyperparameters,\n",
        "        # not random initialization.\n",
        "        run_seed = base_random_seed + i\n",
        "\n",
        "        try:\n",
        "            # Execute the entire analysis pipeline with the specified overrides.\n",
        "            # The `**overrides` syntax unpacks the dictionary into keyword arguments\n",
        "            # for the pipeline function.\n",
        "            result_package = run_full_analysis_pipeline(\n",
        "                consolidated_df_raw=consolidated_df_raw,\n",
        "                model_config=model_config,\n",
        "                random_seed=run_seed,\n",
        "                verbose=False,  # Disable verbose logging for individual runs in a sweep.\n",
        "                **overrides\n",
        "            )\n",
        "\n",
        "            # Augment the results package with the hyperparameters used for this run.\n",
        "            # This makes each result self-documenting.\n",
        "            result_package['hyperparameters'] = overrides\n",
        "\n",
        "            # Append the complete, augmented result to the list of all results.\n",
        "            all_results.append(result_package)\n",
        "\n",
        "        except Exception as e:\n",
        "            # If a run fails for any reason, log the error and continue.\n",
        "            # This makes the sweep robust to failures in individual configurations.\n",
        "            print(f\"\\n--- Run failed for hyperparameters: {overrides} ---\")\n",
        "            print(f\"Error: {e}\")\n",
        "            # Optionally, store failure information.\n",
        "            all_results.append({\n",
        "                'hyperparameters': overrides,\n",
        "                'status': 'FAILED',\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "    # --- Step 3: Return the collection of results ---\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "C3_2dVsvCBwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23 — Verify reproduction fidelity and compile final deliverables\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Verify reproduction fidelity and compile final deliverables\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Step 1: Generate Historical Fit Plot\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def plot_historical_fit(\n",
        "    results: Dict[str, Any],\n",
        "    ax: plt.Axes\n",
        ") -> plt.Axes:\n",
        "    \"\"\"\n",
        "    Generates a plot of the model's historical fit against the actual data.\n",
        "\n",
        "    This function visualizes the model's performance by plotting the predicted\n",
        "    charge-off rates against the true values over the entire sample period. It\n",
        "    also shades major crisis periods for contextual analysis.\n",
        "\n",
        "    Args:\n",
        "        results (Dict[str, Any]):\n",
        "            The comprehensive results dictionary from the main pipeline.\n",
        "        ax (plt.Axes):\n",
        "            The matplotlib Axes object on which to draw the plot.\n",
        "\n",
        "    Returns:\n",
        "        plt.Axes:\n",
        "            The Axes object with the plot drawn on it.\n",
        "    \"\"\"\n",
        "    # Extract necessary data from the results package.\n",
        "    df_clean = results['data_artifacts']['df_clean']\n",
        "    predictions = results['model_artifacts']['final_predictions_np']\n",
        "    valid_mask = results['data_artifacts']['valid_mask']\n",
        "\n",
        "    # Create a DataFrame for plotting, aligning predictions with the ground truth.\n",
        "    plot_df = pd.DataFrame({\n",
        "        'Actual': df_clean['CORCACBS'],\n",
        "        'Predicted': predictions\n",
        "    }, index=df_clean.index)\n",
        "    # Set predictions for invalid periods to NaN so they are not plotted.\n",
        "    plot_df.loc[~valid_mask, 'Predicted'] = np.nan\n",
        "\n",
        "    # Plot the actual and predicted time series.\n",
        "    sns.lineplot(data=plot_df, ax=ax, linewidth=2)\n",
        "\n",
        "    # Define crisis periods for shading.\n",
        "    crisis_periods = {\n",
        "        '1990-91 Recession': ('1990-07-01', '1991-03-31'),\n",
        "        '2001 Recession': ('2001-03-01', '2001-11-30'),\n",
        "        'Global Financial Crisis': ('2007-12-01', '2009-06-30'),\n",
        "        'COVID-19 Recession': ('2020-02-01', '2020-04-30'),\n",
        "    }\n",
        "\n",
        "    # Add shaded regions for each crisis period.\n",
        "    for name, (start, end) in crisis_periods.items():\n",
        "        ax.axvspan(start, end, color='red', alpha=0.15, label='_nolegend_')\n",
        "\n",
        "    # Finalize plot aesthetics.\n",
        "    ax.set_title('Model Historical Fit: Actual vs. Predicted Charge-Off Rates', fontsize=14)\n",
        "    ax.set_ylabel('Charge-Off Rate (%)')\n",
        "    ax.set_xlabel('Year')\n",
        "    ax.legend()\n",
        "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "    return ax\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Step 2: Compile Diagnostic Visuals\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def plot_diagnostic_visuals(\n",
        "    results: Dict[str, Any]\n",
        ") -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Creates a dashboard of key diagnostic plots to interpret the model's behavior.\n",
        "\n",
        "    This function generates a 2x2 figure containing:\n",
        "    1. The PCA trajectory of the attended context space.\n",
        "    2. A heatmap of the geometric component magnitudes over time.\n",
        "    3. A heatmap of the temporal attention weights over time.\n",
        "\n",
        "    Args:\n",
        "        results (Dict[str, Any]):\n",
        "            The comprehensive results dictionary from the main pipeline.\n",
        "\n",
        "    Returns:\n",
        "        plt.Figure:\n",
        "            The matplotlib Figure object containing the dashboard of plots.\n",
        "    \"\"\"\n",
        "    # Create a 2x2 figure for the dashboard.\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "    fig.suptitle('Model Diagnostic Dashboard', fontsize=20)\n",
        "\n",
        "    # --- 1. PCA Trajectory Plot ---\n",
        "    ax1 = axes[0, 0]\n",
        "    pca_df = results['interpretability_results']['pca_trajectory_df']\n",
        "    sns.scatterplot(\n",
        "        data=pca_df,\n",
        "        x='PC1',\n",
        "        y='PC2',\n",
        "        hue='charge_off_rate',\n",
        "        palette='viridis',\n",
        "        ax=ax1,\n",
        "        linewidth=0\n",
        "    )\n",
        "    ax1.set_title('PCA Trajectory of Attended Context (Colored by Charge-Off Rate)', fontsize=14)\n",
        "    ax1.set_xlabel('Principal Component 1')\n",
        "    ax1.set_ylabel('Principal Component 2')\n",
        "    ax1.grid(True, linestyle='--', linewidth=0.5)\n",
        "\n",
        "    # --- 2. Component Magnitude Heatmap ---\n",
        "    ax2 = axes[0, 1]\n",
        "    magnitudes_df = results['interpretability_results']['component_magnitudes_df']\n",
        "    sns.heatmap(\n",
        "        magnitudes_df.T,\n",
        "        ax=ax2,\n",
        "        cmap='plasma',\n",
        "        norm=mcolors.LogNorm(), # Use log scale to see variations\n",
        "        cbar_kws={'label': 'Magnitude (Log Scale)'}\n",
        "    )\n",
        "    ax2.set_title('Geometric Component Influence Magnitude Over Time', fontsize=14)\n",
        "    ax2.set_xlabel('Year')\n",
        "    ax2.set_ylabel('Geometric Component')\n",
        "\n",
        "    # --- 3. Attention Heatmap ---\n",
        "    ax3 = axes[1, 0]\n",
        "    attributions = results['interpretability_results']['temporal_attributions']\n",
        "    lookback = attributions.shape[1]\n",
        "    # Create a DataFrame for better labeling.\n",
        "    attn_df = pd.DataFrame(\n",
        "        attributions,\n",
        "        index=results['data_artifacts']['df_clean'].index\n",
        "    ).T\n",
        "    attn_df.index = [-(lookback - i) for i in range(lookback)] # Lags from -L to -1\n",
        "\n",
        "    sns.heatmap(\n",
        "        attn_df,\n",
        "        ax=ax3,\n",
        "        cmap='cividis',\n",
        "        cbar_kws={'label': 'Attention Weight'}\n",
        "    )\n",
        "    ax3.set_title('Temporal Attention Weights Over Time', fontsize=14)\n",
        "    ax3.set_xlabel('Year')\n",
        "    ax3.set_ylabel('Lookback Lag (Quarters)')\n",
        "\n",
        "    # Hide the last subplot as it's unused.\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    return fig\n",
        "\n",
        "\n",
        "def _create_regime_summary_table(\n",
        "    results: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a summary table characterizing different economic regimes.\n",
        "\n",
        "    This function synthesizes data from the analysis results to provide a\n",
        "    quantitative summary of key economic periods, as described in the paper.\n",
        "    It calculates metrics for charge-off levels, dominant geometric influence,\n",
        "    and attention patterns for each predefined regime.\n",
        "\n",
        "    Args:\n",
        "        results (Dict[str, Any]):\n",
        "            The comprehensive results dictionary from the main pipeline.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A formatted DataFrame summarizing the characteristics of each regime.\n",
        "    \"\"\"\n",
        "    # Define the date ranges for the economic regimes of interest.\n",
        "    regime_dates = {\n",
        "        \"Normal (Mid-2010s)\": ('2014-01-01', '2018-12-31'),\n",
        "        \"1990-91 Recession\": ('1990-07-01', '1991-03-31'),\n",
        "        \"2001 Downturn\": ('2001-03-01', '2001-11-30'),\n",
        "        \"GFC 2008\": ('2007-12-01', '2009-06-30'),\n",
        "        \"COVID-19 Shock\": ('2020-02-01', '2020-12-31'),\n",
        "        \"Recent Period (2022-24)\": ('2022-01-01', '2024-12-31'),\n",
        "    }\n",
        "\n",
        "    # Extract necessary DataFrames from the results.\n",
        "    df_clean = results['data_artifacts']['df_clean']\n",
        "    magnitudes_df = results['interpretability_results']['component_magnitudes_df']\n",
        "    attributions = results['interpretability_results']['temporal_attributions']\n",
        "\n",
        "    # Combine magnitudes with charge-off rates for easier slicing.\n",
        "    analysis_df = magnitudes_df.copy()\n",
        "    analysis_df['charge_off_rate'] = df_clean['CORCACBS']\n",
        "\n",
        "    # Calculate attention dispersion (entropy) for each time step.\n",
        "    # Add a small epsilon to avoid log(0).\n",
        "    entropy = -np.sum(attributions * np.log(attributions + 1e-9), axis=1)\n",
        "    analysis_df['attention_entropy'] = entropy\n",
        "\n",
        "    # Initialize a list to store the summary data for each regime.\n",
        "    summary_data = []\n",
        "\n",
        "    # Iterate through each defined regime to calculate summary statistics.\n",
        "    for name, (start, end) in regime_dates.items():\n",
        "        # Slice the analysis DataFrame to get the data for the current regime.\n",
        "        regime_df = analysis_df[start:end]\n",
        "\n",
        "        if regime_df.empty:\n",
        "            continue\n",
        "\n",
        "        # Calculate summary metrics for the period.\n",
        "        mean_charge_off = regime_df['charge_off_rate'].mean()\n",
        "\n",
        "        # Calculate the ratio of bivector to vector influence.\n",
        "        mean_bivector_mag = regime_df['bivector_magnitude'].mean()\n",
        "        mean_vector_mag = regime_df['vector_magnitude'].mean()\n",
        "        bivector_ratio = mean_bivector_mag / mean_vector_mag if mean_vector_mag > 0 else np.inf\n",
        "\n",
        "        # Determine the dominant geometry based on the ratio.\n",
        "        if bivector_ratio > 1.1:\n",
        "            dominant_geometry = \"Bivector-Dominant\"\n",
        "        elif bivector_ratio < 0.9:\n",
        "            dominant_geometry = \"Vector-Dominant\"\n",
        "        else:\n",
        "            dominant_geometry = \"Mixed\"\n",
        "\n",
        "        # Calculate the average attention dispersion.\n",
        "        mean_attention_entropy = regime_df['attention_entropy'].mean()\n",
        "\n",
        "        # Append the summarized data to our list.\n",
        "        summary_data.append({\n",
        "            \"Regime\": name,\n",
        "            \"Mean Charge-Off (%)\": f\"{mean_charge_off:.2f}\",\n",
        "            \"Dominant Geometry\": dominant_geometry,\n",
        "            \"Bivector/Vector Ratio\": f\"{bivector_ratio:.2f}\",\n",
        "            \"Attention Entropy\": f\"{mean_attention_entropy:.3f}\",\n",
        "        })\n",
        "\n",
        "    # Create the final summary DataFrame.\n",
        "    summary_df = pd.DataFrame(summary_data).set_index(\"Regime\")\n",
        "    return summary_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Step 3: Archive Reproducibility Artifacts\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def archive_analysis_results(\n",
        "    results: Dict[str, Any],\n",
        "    save_dir: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Archives all artifacts from an analysis run for full reproducibility.\n",
        "\n",
        "    This function saves the complete results dictionary, including data, trained\n",
        "    parameters, and analysis outputs, to a specified directory. It also saves\n",
        "    key environment metadata to ensure that the computational environment can\n",
        "    be replicated.\n",
        "\n",
        "    Args:\n",
        "        results (Dict[str, Any]):\n",
        "            The comprehensive results dictionary from the main pipeline.\n",
        "        save_dir (str):\n",
        "            The path to the directory where the artifacts will be saved.\n",
        "            The directory will be created if it does not exist.\n",
        "    \"\"\"\n",
        "    # Create the save directory if it doesn't exist.\n",
        "    output_path = Path(save_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # --- Save the main results object ---\n",
        "    # Pickle is used to save the entire dictionary containing various object types.\n",
        "    with open(output_path / \"full_results.pkl\", \"wb\") as f:\n",
        "        pickle.dump(results, f)\n",
        "\n",
        "    # --- Save trained parameters separately for easier access ---\n",
        "    # This is the standard way to save PyTorch model parameters.\n",
        "    torch.save(\n",
        "        results['model_artifacts']['trained_parameters'],\n",
        "        output_path / \"trained_parameters.pth\"\n",
        "    )\n",
        "\n",
        "    # --- Save environment metadata for reproducibility ---\n",
        "    # Capture key library versions and system info.\n",
        "    environment_meta = {\n",
        "        \"python_version\": sys.version,\n",
        "        \"torch_version\": torch.__version__,\n",
        "        \"numpy_version\": np.__version__,\n",
        "        \"pandas_version\": pd.__version__,\n",
        "        \"seaborn_version\": sns.__version__,\n",
        "    }\n",
        "    with open(output_path / \"environment.json\", \"w\") as f:\n",
        "        json.dump(environment_meta, f, indent=4)\n",
        "\n",
        "    print(f\"All analysis artifacts successfully archived to: {output_path}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_final_deliverables(\n",
        "    results: Dict[str, Any],\n",
        "    save_dir: str,\n",
        "    show_plots: bool = True\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation and archival of all final analysis deliverables.\n",
        "\n",
        "    This function serves as the final reporting step. It takes the complete\n",
        "    results from a pipeline run and produces all specified outputs:\n",
        "    1. A set of diagnostic plots saved to a file.\n",
        "    2. A summary table of economic regimes saved to a CSV file.\n",
        "    3. A complete archive of all data, parameters, and results for full\n",
        "       reproducibility.\n",
        "\n",
        "    Args:\n",
        "        results (Dict[str, Any]):\n",
        "            The comprehensive results dictionary from `run_full_analysis_pipeline`.\n",
        "        save_dir (str):\n",
        "            The path to a directory where all deliverables will be saved. A\n",
        "            timestamped subdirectory will be created here.\n",
        "        show_plots (bool, optional):\n",
        "            If True, displays the generated plots. Defaults to True.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    # Create a unique, timestamped subdirectory for this set of deliverables.\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_path = Path(save_dir) / f\"analysis_run_{timestamp}\"\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Generating deliverables in: {output_path}\")\n",
        "\n",
        "    # --- Step 1: Generate Historical Fit Plot ---\n",
        "    fig_fit, ax_fit = plt.subplots(1, 1, figsize=(16, 8))\n",
        "    plot_historical_fit(results, ax=ax_fit)\n",
        "    fit_plot_path = output_path / \"historical_fit.png\"\n",
        "    fig_fit.savefig(fit_plot_path, dpi=300)\n",
        "    print(f\" -> Saved historical fit plot to {fit_plot_path}\")\n",
        "\n",
        "    # --- Step 2: Compile Diagnostic Visuals and Regime Summary Table ---\n",
        "    # Generate the main diagnostic dashboard.\n",
        "    fig_diag = plot_diagnostic_visuals(results)\n",
        "    diag_plot_path = output_path / \"diagnostic_dashboard.png\"\n",
        "    fig_diag.savefig(diag_plot_path, dpi=300)\n",
        "    print(f\" -> Saved diagnostic dashboard to {diag_plot_path}\")\n",
        "\n",
        "    # Generate the regime summary table.\n",
        "    summary_table = _create_regime_summary_table(results)\n",
        "    table_path = output_path / \"regime_summary_table.csv\"\n",
        "    summary_table.to_csv(table_path)\n",
        "    print(f\" -> Saved regime summary table to {table_path}\")\n",
        "\n",
        "    # Display the table in the console for immediate review.\n",
        "    print(\"\\n--- Regime Summary Table ---\")\n",
        "    print(summary_table)\n",
        "    print(\"--------------------------\\n\")\n",
        "\n",
        "    # --- Step 3: Archive Reproducibility Artifacts ---\n",
        "    # Call the dedicated archiving function.\n",
        "    archive_analysis_results(results, str(output_path))\n",
        "\n",
        "    # Optionally display the plots.\n",
        "    if show_plots:\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "bPwRwVY0IPk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# FINAL ORCHESTRATOR: Master Controller for the Entire Research Workflow\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Final Master Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def execute_geometric_credit_cycle_research(\n",
        "    consolidated_df_raw: pd.DataFrame,\n",
        "    model_config: Dict[str, Any],\n",
        "    hyperparameter_grid: Dict[str, List[Any]],\n",
        "    save_dir: str,\n",
        "    base_random_seed: int = 42,\n",
        "    run_robustness_analysis: bool = True,\n",
        "    show_plots: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete research workflow for the geometric credit cycle model.\n",
        "\n",
        "    This top-level master orchestrator serves as the single entry point for\n",
        "    running the entire project. It performs the following sequence:\n",
        "    1. Runs the primary analysis using the base model configuration.\n",
        "    2. Generates and archives all final deliverables (plots, tables, etc.)\n",
        "       for the primary analysis.\n",
        "    3. Optionally, if `run_robustness_analysis` is True, it conducts a full\n",
        "       hyperparameter sweep as defined by the `hyperparameter_grid`.\n",
        "\n",
        "    Args:\n",
        "        consolidated_df_raw (pd.DataFrame):\n",
        "            The raw, unprocessed quarterly economic data.\n",
        "        model_config (Dict[str, Any]):\n",
        "            The base model configuration dictionary.\n",
        "        hyperparameter_grid (Dict[str, List[Any]]):\n",
        "            A dictionary defining the hyperparameter grid for the robustness\n",
        "            analysis.\n",
        "        save_dir (str):\n",
        "            The root directory where all deliverables and archives will be saved.\n",
        "        base_random_seed (int, optional):\n",
        "            A base seed for all random operations to ensure reproducibility.\n",
        "            Defaults to 42.\n",
        "        run_robustness_analysis (bool, optional):\n",
        "            If True, the full hyperparameter sweep will be executed after the\n",
        "            primary analysis. Defaults to True.\n",
        "        show_plots (bool, optional):\n",
        "            If True, displays the generated plots after they are saved.\n",
        "            Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A dictionary containing the key results from the workflow, including\n",
        "            the full results from the primary run and, if executed, the list\n",
        "            of results from the robustness analysis.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(consolidated_df_raw, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'consolidated_df_raw' must be a pandas DataFrame.\")\n",
        "    if not isinstance(model_config, dict) or not isinstance(hyperparameter_grid, dict):\n",
        "        raise TypeError(\"Inputs 'model_config' and 'hyperparameter_grid' must be dictionaries.\")\n",
        "\n",
        "    # --- Step 1: Execute the Primary Analysis Pipeline ---\n",
        "    print(\"=\"*80)\n",
        "    print(\"Executing Primary Analysis Run...\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Call the main orchestrator from Task 21 to run the analysis with base config.\n",
        "    primary_results = run_full_analysis_pipeline(\n",
        "        consolidated_df_raw=consolidated_df_raw,\n",
        "        model_config=model_config,\n",
        "        random_seed=base_random_seed,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Generate and Archive Deliverables for the Primary Run ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Generating Deliverables for Primary Analysis Run...\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Call the orchestrator from Task 23 to create plots, tables, and archives.\n",
        "    generate_final_deliverables(\n",
        "        results=primary_results,\n",
        "        save_dir=save_dir,\n",
        "        show_plots=show_plots\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Optionally Conduct Robustness Analysis ---\n",
        "    robustness_results = None\n",
        "    if run_robustness_analysis:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Executing Robustness Analysis (Hyperparameter Sweep)...\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Call the orchestrator from Task 22 to run the hyperparameter sweep.\n",
        "        robustness_results = conduct_robustness_analysis(\n",
        "            consolidated_df_raw=consolidated_df_raw,\n",
        "            model_config=model_config,\n",
        "            hyperparameter_grid=hyperparameter_grid,\n",
        "            base_random_seed=base_random_seed\n",
        "        )\n",
        "\n",
        "        # Save the full list of robustness results to the main save directory.\n",
        "        robustness_archive_path = Path(save_dir) / \"robustness_analysis_full_results.pkl\"\n",
        "        with open(robustness_archive_path, \"wb\") as f:\n",
        "            pickle.dump(robustness_results, f)\n",
        "        print(f\"\\nRobustness analysis complete. Full results archived to: {robustness_archive_path}\")\n",
        "\n",
        "    # --- Step 4: Compile and Return Final Workflow Outputs ---\n",
        "    # Return a dictionary containing the key outputs of the entire workflow.\n",
        "    final_workflow_output = {\n",
        "        \"primary_run_results\": primary_results,\n",
        "        \"robustness_analysis_results\": robustness_results\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Master Orchestrator Finished.\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return final_workflow_output\n"
      ],
      "metadata": {
        "id": "t35H96eiL_Yu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}